{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Managing complex configurations any other way would be highly illogical... About spock is a framework that helps manage complex parameter configurations during research and development of Python applications. spock lets you focus on the code you need to write instead of re-implementing boilerplate code like creating ArgParsers, reading configuration files, implementing traceability etc. In short, spock configurations are defined by simple and familiar class-based structures. This allows spock to support inheritance, read from multiple markdown formats, automatically generate cmd-line arguments, and allow hierarchical configuration by composition. Key Features Simple Declaration : Type checked parameters are defined within a @spock decorated class. Supports required/optional and automatic defaults. Easily Managed Parameter Groups: Each class automatically generates its own object within a single namespace. Parameter Inheritance : Classes support inheritance allowing for complex configurations derived from a common base set of parameters. Complex Types : Nested Lists/Tuples, List/Tuples of Enum of @spock classes, List of repeated @spock classes Multiple Configuration File Types: Configurations are specified from YAML, TOML, or JSON files. Hierarchical Configuration : Compose from multiple configuration files via simple include statements. Command-Line Overrides : Quickly experiment by overriding a value with automatically generated command line arguments. Immutable: All classes are frozen preventing any misuse or accidental overwrites (to the extent they can be in Python). Tractability and Reproducibility : Save runtime parameter configuration to YAML, TOML, or JSON with a single chained command (with extra runtime info such as Git info, Python version, machine FQDN, etc). The saved markdown file can be used as the configuration input to reproduce prior runtime configurations. Hyper-Parameter Tuner Addon : Provides a unified interface for defining hyper-parameters (via @spockTuner decorator) that supports various tuning/algorithm backends (currently: Optuna, Ax) S3 Addon : Automatically detects s3:// URI(s) and handles loading and saving spock configuration files when an active boto3.Session is passed in (plus any additional S3Transfer configurations) Quick Install The basic install and [s3] extension require Python 3.6+ while the [tune] extension requires Python 3.7+ Base w/ S3 Extension w/ Hyper-Parameter Tuner pip install spock-config pip install spock-config[s3] pip install spock-config[tune] Quick Start & Documentation Refer to the quick-start guide here . Current documentation and more information can be found here . Example spock usage is located here . News/Releases See Releases for more information. August 17, 2021 Added hyper-parameter tuning backend support for Ax via Service API July 21, 2021 Added hyper-parameter tuning support with pip install spock-config[tune] Hyper-parameter tuning backend support for Optuna define-and-run API (WIP for Ax) May 6th, 2021 Added S3 support with pip install spock-config[s3] S3 addon supports automatically handling loading/saving from paths defined with s3:// URI(s) by passing in an active boto3.Session Original Implementation Nicholas Cilfone , Siddharth Narayanan spock is developed and maintained by the Artificial Intelligence Center of Excellence at Fidelity Investments .","title":"Home"},{"location":"#about","text":"spock is a framework that helps manage complex parameter configurations during research and development of Python applications. spock lets you focus on the code you need to write instead of re-implementing boilerplate code like creating ArgParsers, reading configuration files, implementing traceability etc. In short, spock configurations are defined by simple and familiar class-based structures. This allows spock to support inheritance, read from multiple markdown formats, automatically generate cmd-line arguments, and allow hierarchical configuration by composition.","title":"About"},{"location":"#key-features","text":"Simple Declaration : Type checked parameters are defined within a @spock decorated class. Supports required/optional and automatic defaults. Easily Managed Parameter Groups: Each class automatically generates its own object within a single namespace. Parameter Inheritance : Classes support inheritance allowing for complex configurations derived from a common base set of parameters. Complex Types : Nested Lists/Tuples, List/Tuples of Enum of @spock classes, List of repeated @spock classes Multiple Configuration File Types: Configurations are specified from YAML, TOML, or JSON files. Hierarchical Configuration : Compose from multiple configuration files via simple include statements. Command-Line Overrides : Quickly experiment by overriding a value with automatically generated command line arguments. Immutable: All classes are frozen preventing any misuse or accidental overwrites (to the extent they can be in Python). Tractability and Reproducibility : Save runtime parameter configuration to YAML, TOML, or JSON with a single chained command (with extra runtime info such as Git info, Python version, machine FQDN, etc). The saved markdown file can be used as the configuration input to reproduce prior runtime configurations. Hyper-Parameter Tuner Addon : Provides a unified interface for defining hyper-parameters (via @spockTuner decorator) that supports various tuning/algorithm backends (currently: Optuna, Ax) S3 Addon : Automatically detects s3:// URI(s) and handles loading and saving spock configuration files when an active boto3.Session is passed in (plus any additional S3Transfer configurations)","title":"Key Features"},{"location":"#quick-install","text":"The basic install and [s3] extension require Python 3.6+ while the [tune] extension requires Python 3.7+ Base w/ S3 Extension w/ Hyper-Parameter Tuner pip install spock-config pip install spock-config[s3] pip install spock-config[tune]","title":"Quick Install"},{"location":"#quick-start-documentation","text":"Refer to the quick-start guide here . Current documentation and more information can be found here . Example spock usage is located here .","title":"Quick Start &amp; Documentation"},{"location":"#newsreleases","text":"See Releases for more information.","title":"News/Releases"},{"location":"#august-17-2021","text":"Added hyper-parameter tuning backend support for Ax via Service API","title":"August 17, 2021"},{"location":"#july-21-2021","text":"Added hyper-parameter tuning support with pip install spock-config[tune] Hyper-parameter tuning backend support for Optuna define-and-run API (WIP for Ax)","title":"July 21, 2021"},{"location":"#may-6th-2021","text":"Added S3 support with pip install spock-config[s3] S3 addon supports automatically handling loading/saving from paths defined with s3:// URI(s) by passing in an active boto3.Session","title":"May 6th, 2021"},{"location":"#original-implementation","text":"Nicholas Cilfone , Siddharth Narayanan spock is developed and maintained by the Artificial Intelligence Center of Excellence at Fidelity Investments .","title":"Original Implementation"},{"location":"CONTRIBUTING/","text":"Contributing We welcome all contributions from the community! Any contributions to spock should come through valid Pull/Merge Requests in the public repository. Contribution Guidelines Adhere to PEP-8 standards. Run black and isort linters before creating a PR. Any changes to core functionality must pass all existing unit tests. Additional functionality should have associated unit tests. Provide documentation ( Numpy Docstring format ) whenever possible, even for simple functions or classes.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"We welcome all contributions from the community! Any contributions to spock should come through valid Pull/Merge Requests in the public repository.","title":"Contributing"},{"location":"CONTRIBUTING/#contribution-guidelines","text":"Adhere to PEP-8 standards. Run black and isort linters before creating a PR. Any changes to core functionality must pass all existing unit tests. Additional functionality should have associated unit tests. Provide documentation ( Numpy Docstring format ) whenever possible, even for simple functions or classes.","title":"Contribution Guidelines"},{"location":"docs/ArgParser-Replacement/","text":"Drop In Replacement For Argparser spock can easily be used as a drop in for argparser. This means that all parameter definitions as required to come in from the command line or from setting defaults within the @spock decorated classes. Automatic Command-Line Argument Generation spock will automatically generate command line arguments for each parameter, unless the no_cmd_line=True flag is passed to the ConfigArgBuilder . Let's create a simple example to demonstrate: from spock.config import spock from typing import Optional @spock class ExampleConfig : read_path : str = '/tmp' date : int cache_path : Optional [ str ] Given these definitions, spock will automatically generate a command-line argument (via an internally maintained argparser) for each parameter within each @spock decorated class. The syntax follows simple dot notation of --classname.parameter . Thus, for our sample classes above, spock will automatically generate the following valid command-line arguments: --ExampleConfig.read_path *value* --ExampleConfig.date *value* --ExampleConfig.cache_path *value* Use Spock via the Command-Line Simply do not pass a -c or --config argument at the command line and instead pass in all values to the automatically generated cmd-line arguments. $ python simple.py --ExampleConfig.read_path /my/file/path --ExampleConfig.date 1292838124 \\ --ExampleConfig.cache_path /path/to/cache/dir","title":"argparse Replacement"},{"location":"docs/ArgParser-Replacement/#drop-in-replacement-for-argparser","text":"spock can easily be used as a drop in for argparser. This means that all parameter definitions as required to come in from the command line or from setting defaults within the @spock decorated classes.","title":"Drop In Replacement For Argparser"},{"location":"docs/ArgParser-Replacement/#automatic-command-line-argument-generation","text":"spock will automatically generate command line arguments for each parameter, unless the no_cmd_line=True flag is passed to the ConfigArgBuilder . Let's create a simple example to demonstrate: from spock.config import spock from typing import Optional @spock class ExampleConfig : read_path : str = '/tmp' date : int cache_path : Optional [ str ] Given these definitions, spock will automatically generate a command-line argument (via an internally maintained argparser) for each parameter within each @spock decorated class. The syntax follows simple dot notation of --classname.parameter . Thus, for our sample classes above, spock will automatically generate the following valid command-line arguments: --ExampleConfig.read_path *value* --ExampleConfig.date *value* --ExampleConfig.cache_path *value*","title":"Automatic Command-Line Argument Generation"},{"location":"docs/ArgParser-Replacement/#use-spock-via-the-command-line","text":"Simply do not pass a -c or --config argument at the command line and instead pass in all values to the automatically generated cmd-line arguments. $ python simple.py --ExampleConfig.read_path /my/file/path --ExampleConfig.date 1292838124 \\ --ExampleConfig.cache_path /path/to/cache/dir","title":"Use Spock via the Command-Line"},{"location":"docs/Installation/","text":"Installation Requirements Python: 3.6+ ( [tune] extension requires 3.7+) Base Dependencies: attrs, GitPython, PyYAML, toml Tested OS: Ubuntu (16.04, 18.04), OSX (10.14.6, 11.3.1) Install/Upgrade PyPi pip install spock-config w/ S3 Extension Extra Dependencies: boto3, botocore, hurry.filesize, s3transfer pip install spock-config [ s3 ] w/ Hyper-Parameter Tuner Extension Requires Python 3.7+ Extra Dependencies: optuna, ax-platform, torch, torchvision, mypy_extensions (Python < 3.8) pip install spock-config [ tune ] Pip From Source pip install git+https://github.com/fidelity/spock Build From Source git clone https://github.com/fidelity/spock cd spock pip install setuptools wheel python setup.py bdist_wheel pip install /dist/spock-config-X.X.XxX-py3-none-any.whl","title":"Installation"},{"location":"docs/Installation/#installation","text":"","title":"Installation"},{"location":"docs/Installation/#requirements","text":"Python: 3.6+ ( [tune] extension requires 3.7+) Base Dependencies: attrs, GitPython, PyYAML, toml Tested OS: Ubuntu (16.04, 18.04), OSX (10.14.6, 11.3.1)","title":"Requirements"},{"location":"docs/Installation/#installupgrade","text":"","title":"Install/Upgrade"},{"location":"docs/Installation/#pypi","text":"pip install spock-config","title":"PyPi"},{"location":"docs/Installation/#w-s3-extension","text":"Extra Dependencies: boto3, botocore, hurry.filesize, s3transfer pip install spock-config [ s3 ]","title":"w/ S3 Extension"},{"location":"docs/Installation/#w-hyper-parameter-tuner-extension","text":"Requires Python 3.7+ Extra Dependencies: optuna, ax-platform, torch, torchvision, mypy_extensions (Python < 3.8) pip install spock-config [ tune ]","title":"w/ Hyper-Parameter Tuner Extension"},{"location":"docs/Installation/#pip-from-source","text":"pip install git+https://github.com/fidelity/spock","title":"Pip From Source"},{"location":"docs/Installation/#build-from-source","text":"git clone https://github.com/fidelity/spock cd spock pip install setuptools wheel python setup.py bdist_wheel pip install /dist/spock-config-X.X.XxX-py3-none-any.whl","title":"Build From Source"},{"location":"docs/Motivation/","text":"Motivation Why Spock? spock arose out of a few observations within the Artificial Intelligence Center of Excellence at Fidelity. Modern ML Models == Spaghetti Parameters During research and development of machine learning (ML) models (especially deep learning models) the total number of configuration parameters within a codebase can quickly spiral out of control: data-related parameters, model hyper-parameters, logging parameters, i/o parameters, etc. After writing parser.add_argument() for the 1000th time we figured there had to be a better way to manage the complex configurations needed for modern ML models. In addition, we found a lot of open source ML/DL models (e.g. NVIDIA OpenSeq2Seq ) that had mind-boggling spaghetti parameter definitions. Just figuring out the configurations in order to run, modify, or re-implement an open-source model/library was becoming a minefield in and of itself. Finding a Consistent Solution Looking across an fast-moving enterprise scale AI organization, we noticed a pretty fractured set of configuration management practices across the organization (and even within groups). We saw some pretty bad practices like the hard-coding of parameters within scripts, functions, modules, ... e.g.: def my_function ( args ): my_parameter = 10 # do something with my_parameter ... Not only is this just bad practice but it is also not reproducible (your parameters become dependent on branch and commit) and really isn't helpful for a collaborative codebase. Additionally, it and makes monitoring, re-training, and/or deployment a nightmare. We saw a lot of colleagues taking an easy way out e.g.: ### in config.py ### PARAMETER_1 = 10 PARAMETER_2 = 'relu' ### in function.py ### from config import * def my_function ( arg_1 = PARAMETER_1 ): # do something with arg_1 ... Almost just as bad of practice as above but at least self-contained. It still is hard to reproduce and code and and parameters are still just as intertwined. Those who tried some configuration/parameter management resorted to re-implementing the same boilerplate code e.g.: import argparse parser = argparse . ArgumentParser () parser . add_argument ( '--foo' , help = 'foo help' ) args = parser . parse_args () Better practice to bring parameters in from the command line (or even an external INI file using configparser) but the amount of boilerplate is high and it's pretty tedious to manage more than 10+ parameters. On top of all of this, most parameter definitions were mutable meaning that they can be changed within code blocks (on purpose or by accident) leading to dangerous and unexpected behavior. What Does spock Do To Resolve These Observations? Thus, the idea of of spock was born... A simple, understandable, and lightweight library to manage complex parameter configurations during Python development. Anyone familiar with Python and knows how to define a basic class object can use spock (or if they don't there is plenty of documentation and tutorials on classes). Other Libraries There are other open source complex configuration libraries available for Python. We think that spock fits within this space by providing a simple and lightweight configuration management library in comparison to other libraries. The main two that fill a similar role as spock are: gin-config Provides a lightweight configuration framework for Python, based on dependency injection gin-config does a lot of things and does them pretty well. However, we found that gin-config 's dependency injection and heavy use of decorators just didn't fit what we wanted. Dynamic injection provides a less verbose configuration solution, but can lack reproducibility depending on where and how parameters are defined. In addition, the pretty large 'kitchen sink' of different ways to manage configurations isn't simple nor lightweight enough of a solution. Hydra A framework for elegantly configuring complex applications At the time we started building spock Hydra wasn't available. Hydra is similar to spock but is more restrictive in its functionality and syntax. Similar to gin-config parameters are dynamically injected via a decorator which limits type checking, reproducibility, and traceability. In addition, Hydra doesn't support inheritance which was a big motivator for creating a new solution.","title":"Motivation"},{"location":"docs/Motivation/#motivation","text":"","title":"Motivation"},{"location":"docs/Motivation/#why-spock","text":"spock arose out of a few observations within the Artificial Intelligence Center of Excellence at Fidelity.","title":"Why Spock?"},{"location":"docs/Motivation/#modern-ml-models-spaghetti-parameters","text":"During research and development of machine learning (ML) models (especially deep learning models) the total number of configuration parameters within a codebase can quickly spiral out of control: data-related parameters, model hyper-parameters, logging parameters, i/o parameters, etc. After writing parser.add_argument() for the 1000th time we figured there had to be a better way to manage the complex configurations needed for modern ML models. In addition, we found a lot of open source ML/DL models (e.g. NVIDIA OpenSeq2Seq ) that had mind-boggling spaghetti parameter definitions. Just figuring out the configurations in order to run, modify, or re-implement an open-source model/library was becoming a minefield in and of itself.","title":"Modern ML Models == Spaghetti Parameters"},{"location":"docs/Motivation/#finding-a-consistent-solution","text":"Looking across an fast-moving enterprise scale AI organization, we noticed a pretty fractured set of configuration management practices across the organization (and even within groups). We saw some pretty bad practices like the hard-coding of parameters within scripts, functions, modules, ... e.g.: def my_function ( args ): my_parameter = 10 # do something with my_parameter ... Not only is this just bad practice but it is also not reproducible (your parameters become dependent on branch and commit) and really isn't helpful for a collaborative codebase. Additionally, it and makes monitoring, re-training, and/or deployment a nightmare. We saw a lot of colleagues taking an easy way out e.g.: ### in config.py ### PARAMETER_1 = 10 PARAMETER_2 = 'relu' ### in function.py ### from config import * def my_function ( arg_1 = PARAMETER_1 ): # do something with arg_1 ... Almost just as bad of practice as above but at least self-contained. It still is hard to reproduce and code and and parameters are still just as intertwined. Those who tried some configuration/parameter management resorted to re-implementing the same boilerplate code e.g.: import argparse parser = argparse . ArgumentParser () parser . add_argument ( '--foo' , help = 'foo help' ) args = parser . parse_args () Better practice to bring parameters in from the command line (or even an external INI file using configparser) but the amount of boilerplate is high and it's pretty tedious to manage more than 10+ parameters. On top of all of this, most parameter definitions were mutable meaning that they can be changed within code blocks (on purpose or by accident) leading to dangerous and unexpected behavior.","title":"Finding a Consistent Solution"},{"location":"docs/Motivation/#what-does-spock-do-to-resolve-these-observations","text":"Thus, the idea of of spock was born... A simple, understandable, and lightweight library to manage complex parameter configurations during Python development. Anyone familiar with Python and knows how to define a basic class object can use spock (or if they don't there is plenty of documentation and tutorials on classes).","title":"What Does spock Do To Resolve These Observations?"},{"location":"docs/Motivation/#other-libraries","text":"There are other open source complex configuration libraries available for Python. We think that spock fits within this space by providing a simple and lightweight configuration management library in comparison to other libraries. The main two that fill a similar role as spock are:","title":"Other Libraries"},{"location":"docs/Motivation/#gin-config","text":"Provides a lightweight configuration framework for Python, based on dependency injection gin-config does a lot of things and does them pretty well. However, we found that gin-config 's dependency injection and heavy use of decorators just didn't fit what we wanted. Dynamic injection provides a less verbose configuration solution, but can lack reproducibility depending on where and how parameters are defined. In addition, the pretty large 'kitchen sink' of different ways to manage configurations isn't simple nor lightweight enough of a solution.","title":"gin-config"},{"location":"docs/Motivation/#hydra","text":"A framework for elegantly configuring complex applications At the time we started building spock Hydra wasn't available. Hydra is similar to spock but is more restrictive in its functionality and syntax. Similar to gin-config parameters are dynamically injected via a decorator which limits type checking, reproducibility, and traceability. In addition, Hydra doesn't support inheritance which was a big motivator for creating a new solution.","title":"Hydra"},{"location":"docs/Quick-Start/","text":"Quick Start This is a quick and dirty guide to getting up and running with spock . Read the Basic Tutorial as a simple guide and then explore more Advanced Features for in-depth usage. All examples can be found here . TL;DR Import the necessary components from spock Create a basic Python class, decorate it with @spock Define your parameters in the class (using the typing module if needed) Use the defined parameters in your code Create a configuration file Run your code with --config /path/to/config Simple Example A basic python script, simple.py . First we import the necessary functionality from spock . We define our class using the @spock decorator and our parameters with supported argument types from the typing library. Lastly, we write simple Google style docstrings to provide command line --help information. from spock.builder import ConfigArgBuilder from spock.config import spock from typing import List @spock class BasicConfig : \"\"\"Basic spock configuration for example purposes Attributes: parameter: simple boolean that flags rounding fancy_parameter: parameter that multiplies a value fancier_parameter: parameter that gets added to product of val and fancy_parameter most_fancy_parameter: values to apply basic algebra to \"\"\" parameter : bool fancy_parameter : float fancier_parameter : float most_fancy_parameter : List [ int ] Next let's add two simple function(s) to our script. They both so the same thing but use our parameters in two different ways. def add_namespace ( config ): # Lets just do some basic algebra here val_sum = sum ([( config . fancy_parameter * val ) + config . fancier_parameter for val in config . most_fancy_parameter ]) # If the boolean is true let's round if config . parameter : val_sum = round ( val_sum ) return val_sum def add_by_parameter ( multiply_param , list_vals , add_param , tf_round ): # Lets just do some basic algebra here val_sum = sum ([( multiply_param * val ) + add_param for val in list_vals ]) # If the boolean is true let's round if tf_round : val_sum = round ( val_sum ) return val_sum Now, we build out the parameter objects by passing in the spock objects (as *args ) to the ConfigArgBuilder and chain call the generate method. The returned namespace object contains the defined classes named with the given spock class name. We then can pass the whole object to our first function or specific parameters to our second function. def main (): # Chain the generate function to the class call config = ConfigArgBuilder ( BasicConfig , desc = 'Quick start example' ) . generate () # One can now access the Spock config object by class name with the returned namespace print ( config . BasicConfig . parameter ) # And pass the namespace to our first function val_sum_namespace = add_namespace ( config . BasicConfig ) print ( val_sum_namespace ) # Or pass by parameter val_sum_parameter = add_by_parameter ( config . BasicConfig . fancy_parameter , config . BasicConfig . most_fancy_parameter , config . BasicConfig . fancier_parameter , config . BasicConfig . parameter ) print ( val_sum_parameter ) if __name__ == '__main__' : main () Next let's create a simple configuration file that sets the values of our parameters. Let's make a YAML file (you can also use TOML or JSON), simple.yaml : # Parameters parameter : true fancy_parameter : 8.8 fancier_parameter : 64.64 most_fancy_parameter : [ 768 , 768 , 512 , 128 ] Finally, we would run our script and pass the path to the configuration file to the command line ( -c or --config ): $ python simple.py -c simple.yaml To get help for our spock class and defined parameters: $ python simple.py --help usage: /Users/a635179/Documents/git_repos/open_source/spock/examples/quick-start/simple.py -c [ --config ] config1 [ config2, config3, ... ] Quick start example configuration ( s ) : BasicConfig ( Basic spock configuration for example purposes ) parameter bool simple boolean that flags rounding ( default: False ) fancy_parameter float parameter that multiplies a value fancier_parameter float parameter that gets added to product of val and fancy_parameter most_fancy_parameter List [ int ] values to apply basic algebra to Spock As a Drop In Replacement For Argparser spock can easily be used as a drop in replacement for argparser. See the docs/example here .","title":"Quick Start"},{"location":"docs/Quick-Start/#quick-start","text":"This is a quick and dirty guide to getting up and running with spock . Read the Basic Tutorial as a simple guide and then explore more Advanced Features for in-depth usage. All examples can be found here .","title":"Quick Start"},{"location":"docs/Quick-Start/#tldr","text":"Import the necessary components from spock Create a basic Python class, decorate it with @spock Define your parameters in the class (using the typing module if needed) Use the defined parameters in your code Create a configuration file Run your code with --config /path/to/config","title":"TL;DR"},{"location":"docs/Quick-Start/#simple-example","text":"A basic python script, simple.py . First we import the necessary functionality from spock . We define our class using the @spock decorator and our parameters with supported argument types from the typing library. Lastly, we write simple Google style docstrings to provide command line --help information. from spock.builder import ConfigArgBuilder from spock.config import spock from typing import List @spock class BasicConfig : \"\"\"Basic spock configuration for example purposes Attributes: parameter: simple boolean that flags rounding fancy_parameter: parameter that multiplies a value fancier_parameter: parameter that gets added to product of val and fancy_parameter most_fancy_parameter: values to apply basic algebra to \"\"\" parameter : bool fancy_parameter : float fancier_parameter : float most_fancy_parameter : List [ int ] Next let's add two simple function(s) to our script. They both so the same thing but use our parameters in two different ways. def add_namespace ( config ): # Lets just do some basic algebra here val_sum = sum ([( config . fancy_parameter * val ) + config . fancier_parameter for val in config . most_fancy_parameter ]) # If the boolean is true let's round if config . parameter : val_sum = round ( val_sum ) return val_sum def add_by_parameter ( multiply_param , list_vals , add_param , tf_round ): # Lets just do some basic algebra here val_sum = sum ([( multiply_param * val ) + add_param for val in list_vals ]) # If the boolean is true let's round if tf_round : val_sum = round ( val_sum ) return val_sum Now, we build out the parameter objects by passing in the spock objects (as *args ) to the ConfigArgBuilder and chain call the generate method. The returned namespace object contains the defined classes named with the given spock class name. We then can pass the whole object to our first function or specific parameters to our second function. def main (): # Chain the generate function to the class call config = ConfigArgBuilder ( BasicConfig , desc = 'Quick start example' ) . generate () # One can now access the Spock config object by class name with the returned namespace print ( config . BasicConfig . parameter ) # And pass the namespace to our first function val_sum_namespace = add_namespace ( config . BasicConfig ) print ( val_sum_namespace ) # Or pass by parameter val_sum_parameter = add_by_parameter ( config . BasicConfig . fancy_parameter , config . BasicConfig . most_fancy_parameter , config . BasicConfig . fancier_parameter , config . BasicConfig . parameter ) print ( val_sum_parameter ) if __name__ == '__main__' : main () Next let's create a simple configuration file that sets the values of our parameters. Let's make a YAML file (you can also use TOML or JSON), simple.yaml : # Parameters parameter : true fancy_parameter : 8.8 fancier_parameter : 64.64 most_fancy_parameter : [ 768 , 768 , 512 , 128 ] Finally, we would run our script and pass the path to the configuration file to the command line ( -c or --config ): $ python simple.py -c simple.yaml To get help for our spock class and defined parameters: $ python simple.py --help usage: /Users/a635179/Documents/git_repos/open_source/spock/examples/quick-start/simple.py -c [ --config ] config1 [ config2, config3, ... ] Quick start example configuration ( s ) : BasicConfig ( Basic spock configuration for example purposes ) parameter bool simple boolean that flags rounding ( default: False ) fancy_parameter float parameter that multiplies a value fancier_parameter float parameter that gets added to product of val and fancy_parameter most_fancy_parameter List [ int ] values to apply basic algebra to","title":"Simple Example"},{"location":"docs/Quick-Start/#spock-as-a-drop-in-replacement-for-argparser","text":"spock can easily be used as a drop in replacement for argparser. See the docs/example here .","title":"Spock As a Drop In Replacement For Argparser"},{"location":"docs/addons/S3/","text":"S3 Support When installed with the S3 addon spock will attempt to identify S3 URI(s) (e.g. s3://<bucket-name>/<key> ) and handle them automatically. The user only needs to provide an active boto3.session.Session to an S3Config object and pass it to the ConfigArgBuilder . Installing Install spock with the extra s3 related dependencies. pip install spock-config [ s3 ] Creating a boto3 Session The user must provide an active boto3.session.Session object to spock in order for the library to automatically handle S3 URI(s). Configuration is highly dependent upon your current AWS setup/security. Please refer to the boto3 docs for session and credentials for help on how to correctly configure your boto3.session.Session . For instance, let's just suppose we are going to get our tokens via SAML authorization where we already have the SAMLAssertion, RoleArn, and PrincipalArn stored as env variables: import boto3 import os client = boto3 . client ( 'sts' ) token = client . assume_role_with_saml ( RoleArn = os . environ . get ( \"RoleArn\" ), PrincipalArn = os . environ . get ( \"PrincipalArn\" ), SAMLAssertion = os . environ . get ( \"SamlString\" ) ) credentials = token [ 'Credentials' ] session = boto3 . Session ( aws_access_key_id = credentials [ 'AccessKeyId' ], aws_secret_access_key = credentials [ 'SecretAccessKey' ], aws_session_token = credentials [ 'SessionToken' ], region_name = os . environ . get ( 'AWS_REGION' )) Using the S3Config Object As an example let's create a basic @spock decorated class, instantiate a S3Config object from spock.addons.s3 with the boto3.session.Session we created above, and pass it to the ConfigArgBuilder . from spock.addons.s3 import S3Config from spock.builder import ConfigArgBuilder from spock.config import spock from typing import List @spock class BasicConfig : \"\"\"Basic spock configuration for example purposes Attributes: parameter: simple boolean that flags rounding fancy_parameter: parameter that multiplies a value fancier_parameter: parameter that gets added to product of val and fancy_parameter most_fancy_parameter: values to apply basic algebra to \"\"\" parameter : bool fancy_parameter : float fancier_parameter : float most_fancy_parameter : List [ int ] def main (): # Create an S3Config object and pass in the boto3 session s3_config = S3Config ( session = session ) # Chain the generate function to the ConfigArgBuilder call # Pass in the S3Config object config = ConfigArgBuilder ( BasicConfig , desc = 'S3 example' , s3_config = s3_config ) . generate () Defining the configuration file with a S3 URI Usually we pass a relative or absolute system path as the configuration file command line argument. Here we pass in a S3 URI instead: $ python simple.py -c s3://my-bucket/path/to/file/config.yaml With a S3Config object passed into the ConfigArgBuilder the S3 URI will automatically be handled by spock . Saving to a S3 URI Similarly, we usually pass a relative or absolute system path to the SavePath special argument type or to the user_specified_path kwarg. Again, instead we give a S3 URI: def main (): # Create an S3Config object and pass in the boto3 session s3_config = S3Config ( session = session ) # Chain the generate function to the ConfigArgBuilder call # Pass in the S3Config object config = ConfigArgBuilder ( BasicConfig , desc = 'S3 example' , s3_config = s3_config ) . save ( user_specified_path = \"s3://my-bucket/path/to/file/\" ) . generate () With a S3Config object passed into the ConfigArgBuilder the S3 URI will automatically be handled by spock . S3Transfer ExtraArgs If you require any other settings for uploading or downloading files from S3 the S3Config class has two extra attributes: download_config which takes a S3DownloadConfig object from spock.addons.s3 which supports all ExtraArgs from S3Transfer.ALLOWED_DOWNLOAD_ARGS upload_config which takes a S3UploadConfig object from spock.addons.s3 which supports all ExtraArgs from S3Transfer.ALLOWED_UPLOAD_ARGS","title":"S3"},{"location":"docs/addons/S3/#s3-support","text":"When installed with the S3 addon spock will attempt to identify S3 URI(s) (e.g. s3://<bucket-name>/<key> ) and handle them automatically. The user only needs to provide an active boto3.session.Session to an S3Config object and pass it to the ConfigArgBuilder .","title":"S3 Support"},{"location":"docs/addons/S3/#installing","text":"Install spock with the extra s3 related dependencies. pip install spock-config [ s3 ]","title":"Installing"},{"location":"docs/addons/S3/#creating-a-boto3-session","text":"The user must provide an active boto3.session.Session object to spock in order for the library to automatically handle S3 URI(s). Configuration is highly dependent upon your current AWS setup/security. Please refer to the boto3 docs for session and credentials for help on how to correctly configure your boto3.session.Session . For instance, let's just suppose we are going to get our tokens via SAML authorization where we already have the SAMLAssertion, RoleArn, and PrincipalArn stored as env variables: import boto3 import os client = boto3 . client ( 'sts' ) token = client . assume_role_with_saml ( RoleArn = os . environ . get ( \"RoleArn\" ), PrincipalArn = os . environ . get ( \"PrincipalArn\" ), SAMLAssertion = os . environ . get ( \"SamlString\" ) ) credentials = token [ 'Credentials' ] session = boto3 . Session ( aws_access_key_id = credentials [ 'AccessKeyId' ], aws_secret_access_key = credentials [ 'SecretAccessKey' ], aws_session_token = credentials [ 'SessionToken' ], region_name = os . environ . get ( 'AWS_REGION' ))","title":"Creating a boto3 Session"},{"location":"docs/addons/S3/#using-the-s3config-object","text":"As an example let's create a basic @spock decorated class, instantiate a S3Config object from spock.addons.s3 with the boto3.session.Session we created above, and pass it to the ConfigArgBuilder . from spock.addons.s3 import S3Config from spock.builder import ConfigArgBuilder from spock.config import spock from typing import List @spock class BasicConfig : \"\"\"Basic spock configuration for example purposes Attributes: parameter: simple boolean that flags rounding fancy_parameter: parameter that multiplies a value fancier_parameter: parameter that gets added to product of val and fancy_parameter most_fancy_parameter: values to apply basic algebra to \"\"\" parameter : bool fancy_parameter : float fancier_parameter : float most_fancy_parameter : List [ int ] def main (): # Create an S3Config object and pass in the boto3 session s3_config = S3Config ( session = session ) # Chain the generate function to the ConfigArgBuilder call # Pass in the S3Config object config = ConfigArgBuilder ( BasicConfig , desc = 'S3 example' , s3_config = s3_config ) . generate ()","title":"Using the S3Config Object"},{"location":"docs/addons/S3/#defining-the-configuration-file-with-a-s3-uri","text":"Usually we pass a relative or absolute system path as the configuration file command line argument. Here we pass in a S3 URI instead: $ python simple.py -c s3://my-bucket/path/to/file/config.yaml With a S3Config object passed into the ConfigArgBuilder the S3 URI will automatically be handled by spock .","title":"Defining the configuration file with a S3 URI"},{"location":"docs/addons/S3/#saving-to-a-s3-uri","text":"Similarly, we usually pass a relative or absolute system path to the SavePath special argument type or to the user_specified_path kwarg. Again, instead we give a S3 URI: def main (): # Create an S3Config object and pass in the boto3 session s3_config = S3Config ( session = session ) # Chain the generate function to the ConfigArgBuilder call # Pass in the S3Config object config = ConfigArgBuilder ( BasicConfig , desc = 'S3 example' , s3_config = s3_config ) . save ( user_specified_path = \"s3://my-bucket/path/to/file/\" ) . generate () With a S3Config object passed into the ConfigArgBuilder the S3 URI will automatically be handled by spock .","title":"Saving to a S3 URI"},{"location":"docs/addons/S3/#s3transfer-extraargs","text":"If you require any other settings for uploading or downloading files from S3 the S3Config class has two extra attributes: download_config which takes a S3DownloadConfig object from spock.addons.s3 which supports all ExtraArgs from S3Transfer.ALLOWED_DOWNLOAD_ARGS upload_config which takes a S3UploadConfig object from spock.addons.s3 which supports all ExtraArgs from S3Transfer.ALLOWED_UPLOAD_ARGS","title":"S3Transfer ExtraArgs"},{"location":"docs/addons/tuner/About/","text":"Hyper-Parameter Tuning Support This series of docs will describe the basics of hyper-parameter support within spock . spock tries to be as hands-off as possible with the underlying backends that support hyper-parameter tuning and only provide a common and simplified interface to define hyper-parameter tuning runs. The rest is left up to the user to define and handle, thus to not handcuff the user into too simplified functionality. All examples can be found here . Installing Install spock with the extra hyper-parameter tuning related dependencies. Requires Python 3.7+ due to ax-platform pip install spock-config [ tune ] Supported Backends Optuna Ax","title":"About"},{"location":"docs/addons/tuner/About/#hyper-parameter-tuning-support","text":"This series of docs will describe the basics of hyper-parameter support within spock . spock tries to be as hands-off as possible with the underlying backends that support hyper-parameter tuning and only provide a common and simplified interface to define hyper-parameter tuning runs. The rest is left up to the user to define and handle, thus to not handcuff the user into too simplified functionality. All examples can be found here .","title":"Hyper-Parameter Tuning Support"},{"location":"docs/addons/tuner/About/#installing","text":"Install spock with the extra hyper-parameter tuning related dependencies. Requires Python 3.7+ due to ax-platform pip install spock-config [ tune ]","title":"Installing"},{"location":"docs/addons/tuner/About/#supported-backends","text":"Optuna Ax","title":"Supported Backends"},{"location":"docs/addons/tuner/Ax/","text":"Ax Support spock integrates with the Ax optimization framework through the provided Service API. See docs for AxClient info. All examples can be found here . Defining the Backend So let's continue with our Ax specific version of tune.py : It's important to note that you can still use the @spock decorator to define any non hyper-parameters! For posterity let's add some fixed parameters (those that are not part of hyper-parameter tuning) that we will use elsewhere in our code. from spock.config import spock @spock class BasicParams : n_trials : int max_iter : int Now we need to tell spock that we intend on doing hyper-parameter tuning and which backend we would like to use. We do this by calling the tuner method on the ConfigArgBuilder object passing in a configuration object for the backend of choice (just like in basic functionality this is a chained command, thus the builder object will still be returned). For Ax one uses AxTunerConfig . This config mirrors all options that would be passed into the AxClient constructor and the AxClient.create_experiment function call so that spock can setup the Service API. (Note: The @spockTuner decorated classes are passed to the ConfigArgBuilder in the exact same way as basic @spock decorated classes.) from spock.addons.tune import AxTunerConfig # Ax config -- this will internally spawn the AxClient service API style which will be returned # by accessing the tuner_status property on the ConfigArgBuilder object -- note here that we need to define the # objective name that the client will expect to be within the data dictionary when completing trials ax_config = AxTunerConfig ( objective_name = \"accuracy\" , minimize = False ) # Use the builder to setup # Call tuner to indicate that we are going to do some HP tuning -- passing in an ax study object attrs_obj = ConfigArgBuilder ( LogisticRegressionHP , BasicParams , desc = \"Example Logistic Regression Hyper-Parameter Tuning -- Ax Backend\" , ) . tuner ( tuner_config = ax_config ) Generate Functionality Still Exists To get the set of fixed parameters (those that are not hyper-parameters) one simply calls the generate() function just like they would for normal spock usage to get the fixed parameter spockspace . Continuing in tune.py : # Here we need some of the fixed parameters first so we can just call the generate fnc to grab all the fixed params # prior to starting the sampling process fixed_params = attrs_obj . generate () Sample as an Alternative to Generate The sample() call is the crux of spock hyper-parameter tuning support. It draws a hyper-parameter sample from the underlying backend sampler and combines it with fixed parameters and returns a single Spockspace with all useable parameters (defined with dot notation). For Ax -- Under the hood spock uses the Service API (with an AxClient ) -- thus it handles the underlying call to get the next trial. The spock builder object has a @property called tuner_status that returns any necessary backend objects in a dictionary that the user needs to interface with. In the case of Ax, this contains both the AxClient and trial_index (as dictionary keys). We use the return of tuner_status to handle trial completion via the complete_trial call based on the metric of interested (here just the simple validation accuracy -- remember during AxTunerConfig instantiation we set the objective_name to 'accuracy' -- we also set the SEM to 0.0 since we are not using it for this example) See here for Ax documentation on completing trials. Continuing in tune.py : # Iterate through a bunch of ax trials for _ in range ( fixed_params . BasicParams . n_trials ): # Call sample on the spock object hp_attrs = attrs_obj . sample () # Use the currently sampled parameters in a simple LogisticRegression from sklearn clf = LogisticRegression ( C = hp_attrs . LogisticRegressionHP . c , solver = hp_attrs . LogisticRegressionHP . solver , max_iter = hp_attrs . BasicParams . max_iter ) clf . fit ( X_train , y_train ) val_acc = clf . score ( X_valid , y_valid ) # Get the status of the tuner -- this dict will contain all the objects needed to update tuner_status = attrs_obj . tuner_status # Pull the AxClient object and trial index out of the return dictionary and call 'complete_trial' on the # AxClient object with the correct raw_data that contains the objective name tuner_status [ \"client\" ] . complete_trial ( trial_index = tuner_status [ \"trial_index\" ], raw_data = { \"accuracy\" : ( val_acc , 0.0 )}, )","title":"Ax"},{"location":"docs/addons/tuner/Ax/#ax-support","text":"spock integrates with the Ax optimization framework through the provided Service API. See docs for AxClient info. All examples can be found here .","title":"Ax Support"},{"location":"docs/addons/tuner/Ax/#defining-the-backend","text":"So let's continue with our Ax specific version of tune.py : It's important to note that you can still use the @spock decorator to define any non hyper-parameters! For posterity let's add some fixed parameters (those that are not part of hyper-parameter tuning) that we will use elsewhere in our code. from spock.config import spock @spock class BasicParams : n_trials : int max_iter : int Now we need to tell spock that we intend on doing hyper-parameter tuning and which backend we would like to use. We do this by calling the tuner method on the ConfigArgBuilder object passing in a configuration object for the backend of choice (just like in basic functionality this is a chained command, thus the builder object will still be returned). For Ax one uses AxTunerConfig . This config mirrors all options that would be passed into the AxClient constructor and the AxClient.create_experiment function call so that spock can setup the Service API. (Note: The @spockTuner decorated classes are passed to the ConfigArgBuilder in the exact same way as basic @spock decorated classes.) from spock.addons.tune import AxTunerConfig # Ax config -- this will internally spawn the AxClient service API style which will be returned # by accessing the tuner_status property on the ConfigArgBuilder object -- note here that we need to define the # objective name that the client will expect to be within the data dictionary when completing trials ax_config = AxTunerConfig ( objective_name = \"accuracy\" , minimize = False ) # Use the builder to setup # Call tuner to indicate that we are going to do some HP tuning -- passing in an ax study object attrs_obj = ConfigArgBuilder ( LogisticRegressionHP , BasicParams , desc = \"Example Logistic Regression Hyper-Parameter Tuning -- Ax Backend\" , ) . tuner ( tuner_config = ax_config )","title":"Defining the Backend"},{"location":"docs/addons/tuner/Ax/#generate-functionality-still-exists","text":"To get the set of fixed parameters (those that are not hyper-parameters) one simply calls the generate() function just like they would for normal spock usage to get the fixed parameter spockspace . Continuing in tune.py : # Here we need some of the fixed parameters first so we can just call the generate fnc to grab all the fixed params # prior to starting the sampling process fixed_params = attrs_obj . generate ()","title":"Generate Functionality Still Exists"},{"location":"docs/addons/tuner/Ax/#sample-as-an-alternative-to-generate","text":"The sample() call is the crux of spock hyper-parameter tuning support. It draws a hyper-parameter sample from the underlying backend sampler and combines it with fixed parameters and returns a single Spockspace with all useable parameters (defined with dot notation). For Ax -- Under the hood spock uses the Service API (with an AxClient ) -- thus it handles the underlying call to get the next trial. The spock builder object has a @property called tuner_status that returns any necessary backend objects in a dictionary that the user needs to interface with. In the case of Ax, this contains both the AxClient and trial_index (as dictionary keys). We use the return of tuner_status to handle trial completion via the complete_trial call based on the metric of interested (here just the simple validation accuracy -- remember during AxTunerConfig instantiation we set the objective_name to 'accuracy' -- we also set the SEM to 0.0 since we are not using it for this example) See here for Ax documentation on completing trials. Continuing in tune.py : # Iterate through a bunch of ax trials for _ in range ( fixed_params . BasicParams . n_trials ): # Call sample on the spock object hp_attrs = attrs_obj . sample () # Use the currently sampled parameters in a simple LogisticRegression from sklearn clf = LogisticRegression ( C = hp_attrs . LogisticRegressionHP . c , solver = hp_attrs . LogisticRegressionHP . solver , max_iter = hp_attrs . BasicParams . max_iter ) clf . fit ( X_train , y_train ) val_acc = clf . score ( X_valid , y_valid ) # Get the status of the tuner -- this dict will contain all the objects needed to update tuner_status = attrs_obj . tuner_status # Pull the AxClient object and trial index out of the return dictionary and call 'complete_trial' on the # AxClient object with the correct raw_data that contains the objective name tuner_status [ \"client\" ] . complete_trial ( trial_index = tuner_status [ \"trial_index\" ], raw_data = { \"accuracy\" : ( val_acc , 0.0 )}, )","title":"Sample as an Alternative to Generate"},{"location":"docs/addons/tuner/Basics/","text":"Tune Basics Just like the basic spock functionality, hyper-parameters are defined via a class based solution. All parameters must be defined in a class or multiple classes by decorating with the @spockTuner decorator. Parameters are defined as one of the two basic types, RangeHyperParameter or ChoiceHyperParameter . Once built (with a specific backend), all parameters can be found within an automatically generated namespace object that contains both the fixed and sampled parameters that can be accessed with the given @spock or @spockTuner class names. Supported Hyper-Parameter Types spock supports the two following types for hyper-parameters, RangeHyperParameter or ChoiceHyperParameter . The RangeHyperParameter type is used for hyper-parameters that are to be drawn from a sampled range of int or float while the ChoiceHyperParameter type is used for hyper-parameters that are to be sampled from a discrete set of values that can be of base type int , float , bool , or str . RangeHyperParameter requires the following inputs: type: string of either int or float depending on the needed type bounds: a tuple of two values that define the lower and upper bound of the range (int or float) log_scale: boolean to activate log scaling of the range ChoiceHyperParameter requires the following inputs: type: string of either int, float, bool, str depending on the needed type choices: a list of any length that contains the discrete values to sample from Defining a spockTuner Class Let's start building out a very simple example (logistic regression of iris w/ sklearn) that we will continue to use within the tutorial: tune.py Tune functions exactly the same as base spock functionality. We import the basic units of functionality from spock.addons.tune , define our class using the @spockTuner decorator, and define our parameters with supported argument types. We also pull in the sample iris data from sklearn. from spock.addons.tune import ChoiceHyperParameter from spock.addons.tune import RangeHyperParameter from spock.addons.tune import spockTuner from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split @spockTuner class LogisticRegressionHP : c : RangeHyperParameter solver : ChoiceHyperParameter # Load the iris data X , y = load_iris ( return_X_y = True ) # Split the Iris data X_train , X_valid , y_train , y_valid = train_test_split ( X , y ) The @spockTuner decorated classes are passed to the ConfigArgBuilder in the exact same way as basic @spock decorated classes. This returns a spock builder object which can be used to call different methods. attrs_obj = ConfigArgBuilder ( LogisticRegressionHP , desc = \"Example Logistic Regression Hyper-Parameter Tuning\" , ) Creating a Configuration File Just like basic spock functionality, values in spock are set primarily using external configuration files. For our hyper-parameters we just defined above our tune.yaml file might look something like this (remember each class requires specific inputs): # Hyper-parameter config LogisticRegressionHP : c : bounds : - 0.01 - 10.0 log_scale : true type : float solver : choices : - lbfgs - saga type : str Continuing The rest of the docs are backend specific so refer to the correct backend specific documentation.","title":"Basics"},{"location":"docs/addons/tuner/Basics/#tune-basics","text":"Just like the basic spock functionality, hyper-parameters are defined via a class based solution. All parameters must be defined in a class or multiple classes by decorating with the @spockTuner decorator. Parameters are defined as one of the two basic types, RangeHyperParameter or ChoiceHyperParameter . Once built (with a specific backend), all parameters can be found within an automatically generated namespace object that contains both the fixed and sampled parameters that can be accessed with the given @spock or @spockTuner class names.","title":"Tune Basics"},{"location":"docs/addons/tuner/Basics/#supported-hyper-parameter-types","text":"spock supports the two following types for hyper-parameters, RangeHyperParameter or ChoiceHyperParameter . The RangeHyperParameter type is used for hyper-parameters that are to be drawn from a sampled range of int or float while the ChoiceHyperParameter type is used for hyper-parameters that are to be sampled from a discrete set of values that can be of base type int , float , bool , or str . RangeHyperParameter requires the following inputs: type: string of either int or float depending on the needed type bounds: a tuple of two values that define the lower and upper bound of the range (int or float) log_scale: boolean to activate log scaling of the range ChoiceHyperParameter requires the following inputs: type: string of either int, float, bool, str depending on the needed type choices: a list of any length that contains the discrete values to sample from","title":"Supported Hyper-Parameter Types"},{"location":"docs/addons/tuner/Basics/#defining-a-spocktuner-class","text":"Let's start building out a very simple example (logistic regression of iris w/ sklearn) that we will continue to use within the tutorial: tune.py Tune functions exactly the same as base spock functionality. We import the basic units of functionality from spock.addons.tune , define our class using the @spockTuner decorator, and define our parameters with supported argument types. We also pull in the sample iris data from sklearn. from spock.addons.tune import ChoiceHyperParameter from spock.addons.tune import RangeHyperParameter from spock.addons.tune import spockTuner from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split @spockTuner class LogisticRegressionHP : c : RangeHyperParameter solver : ChoiceHyperParameter # Load the iris data X , y = load_iris ( return_X_y = True ) # Split the Iris data X_train , X_valid , y_train , y_valid = train_test_split ( X , y ) The @spockTuner decorated classes are passed to the ConfigArgBuilder in the exact same way as basic @spock decorated classes. This returns a spock builder object which can be used to call different methods. attrs_obj = ConfigArgBuilder ( LogisticRegressionHP , desc = \"Example Logistic Regression Hyper-Parameter Tuning\" , )","title":"Defining a spockTuner Class"},{"location":"docs/addons/tuner/Basics/#creating-a-configuration-file","text":"Just like basic spock functionality, values in spock are set primarily using external configuration files. For our hyper-parameters we just defined above our tune.yaml file might look something like this (remember each class requires specific inputs): # Hyper-parameter config LogisticRegressionHP : c : bounds : - 0.01 - 10.0 log_scale : true type : float solver : choices : - lbfgs - saga type : str","title":"Creating a Configuration File"},{"location":"docs/addons/tuner/Basics/#continuing","text":"The rest of the docs are backend specific so refer to the correct backend specific documentation.","title":"Continuing"},{"location":"docs/addons/tuner/Optuna/","text":"Optuna Support spock integrates with the Optuna hyper-parameter optimization framework through the provided ask-and-run interface and the define-and-run API. See docs . All examples can be found here . Defining the Backend So let's continue in our Optuna specific version of tune.py : It's important to note that you can still use the @spock decorator to define any non hyper-parameters! For posterity let's add some fixed parameters (those that are not part of hyper-parameter tuning) that we will use elsewhere in our code. from spock.config import spock @spock class BasicParams : n_trials : int max_iter : int Now we need to tell spock that we intend on doing hyper-parameter tuning and which backend we would like to use. We do this by calling the tuner method on the ConfigArgBuilder object passing in a configuration object for the backend of choice (just like in basic functionality this is a chained command, thus the builder object will still be returned). For Optuna one uses OptunaTunerConfig . This config mirrors all options that would be passed into the optuna.study.create_study function call so that spock can setup the define-and-run API. (Note: The @spockTuner decorated classes are passed to the ConfigArgBuilder in the exact same way as basic @spock decorated classes.) from spock.addons.tune import OptunaTunerConfig # Optuna config -- this will internally configure the study object for the define-and-run style which will be returned # by accessing the tuner_status property on the ConfigArgBuilder object optuna_config = OptunaTunerConfig ( study_name = \"Iris Logistic Regression\" , direction = \"maximize\" ) # Use the builder to setup # Call tuner to indicate that we are going to do some HP tuning -- passing in an optuna study object attrs_obj = ConfigArgBuilder ( LogisticRegressionHP , BasicParams , desc = \"Example Logistic Regression Hyper-Parameter Tuning -- Optuna Backend\" , ) . tuner ( tuner_config = optuna_config ) Generate Functionality Still Exists To get the set of fixed parameters (those that are not hyper-parameters) one simply calls the generate() function just like they would for normal spock usage to get the fixed parameter spockspace . Continuing in tune.py : # Here we need some of the fixed parameters first so we can just call the generate fnc to grab all the fixed params # prior to starting the sampling process fixed_params = attrs_obj . generate () Sample as an Alternative to Generate The sample() call is the crux of spock hyper-parameter tuning support. It draws a hyper-parameter sample from the underlying backend sampler and combines it with fixed parameters and returns a single Spockspace with all useable parameters (defined with dot notation). For Optuna -- Under the hood spock uses the define-and-run Optuna interface -- thus it handles the underlying 'ask' call. The spock builder object has a @property called tuner_status that returns any necessary backend objects in a dictionary that the user needs to interface with. In the case of Optuna, this contains both the Optuna study and trial (as dictionary keys). We use the return of tuner_status to handle the 'tell' call based on the metric of interested (here just simple validation accuracy) Continuing in tune.py : # Iterate through a bunch of optuna trials for _ in range ( fixed_params . BasicParams . n_trials ): # Call sample on the spock object hp_attrs = attrs_obj . sample () # Use the currently sampled parameters in a simple LogisticRegression from sklearn clf = LogisticRegression ( C = hp_attrs . LogisticRegressionHP . c , solver = hp_attrs . LogisticRegressionHP . solver , max_iter = hp_attrs . BasicParams . max_iter ) clf . fit ( X_train , y_train ) val_acc = clf . score ( X_valid , y_valid ) # Get the status of the tuner -- this dict will contain all the objects needed to update tuner_status = attrs_obj . tuner_status # Pull the study and trials object out of the return dictionary and pass it to the tell call using the study # object tuner_status [ \"study\" ] . tell ( tuner_status [ \"trial\" ], val_acc )","title":"Optuna"},{"location":"docs/addons/tuner/Optuna/#optuna-support","text":"spock integrates with the Optuna hyper-parameter optimization framework through the provided ask-and-run interface and the define-and-run API. See docs . All examples can be found here .","title":"Optuna Support"},{"location":"docs/addons/tuner/Optuna/#defining-the-backend","text":"So let's continue in our Optuna specific version of tune.py : It's important to note that you can still use the @spock decorator to define any non hyper-parameters! For posterity let's add some fixed parameters (those that are not part of hyper-parameter tuning) that we will use elsewhere in our code. from spock.config import spock @spock class BasicParams : n_trials : int max_iter : int Now we need to tell spock that we intend on doing hyper-parameter tuning and which backend we would like to use. We do this by calling the tuner method on the ConfigArgBuilder object passing in a configuration object for the backend of choice (just like in basic functionality this is a chained command, thus the builder object will still be returned). For Optuna one uses OptunaTunerConfig . This config mirrors all options that would be passed into the optuna.study.create_study function call so that spock can setup the define-and-run API. (Note: The @spockTuner decorated classes are passed to the ConfigArgBuilder in the exact same way as basic @spock decorated classes.) from spock.addons.tune import OptunaTunerConfig # Optuna config -- this will internally configure the study object for the define-and-run style which will be returned # by accessing the tuner_status property on the ConfigArgBuilder object optuna_config = OptunaTunerConfig ( study_name = \"Iris Logistic Regression\" , direction = \"maximize\" ) # Use the builder to setup # Call tuner to indicate that we are going to do some HP tuning -- passing in an optuna study object attrs_obj = ConfigArgBuilder ( LogisticRegressionHP , BasicParams , desc = \"Example Logistic Regression Hyper-Parameter Tuning -- Optuna Backend\" , ) . tuner ( tuner_config = optuna_config )","title":"Defining the Backend"},{"location":"docs/addons/tuner/Optuna/#generate-functionality-still-exists","text":"To get the set of fixed parameters (those that are not hyper-parameters) one simply calls the generate() function just like they would for normal spock usage to get the fixed parameter spockspace . Continuing in tune.py : # Here we need some of the fixed parameters first so we can just call the generate fnc to grab all the fixed params # prior to starting the sampling process fixed_params = attrs_obj . generate ()","title":"Generate Functionality Still Exists"},{"location":"docs/addons/tuner/Optuna/#sample-as-an-alternative-to-generate","text":"The sample() call is the crux of spock hyper-parameter tuning support. It draws a hyper-parameter sample from the underlying backend sampler and combines it with fixed parameters and returns a single Spockspace with all useable parameters (defined with dot notation). For Optuna -- Under the hood spock uses the define-and-run Optuna interface -- thus it handles the underlying 'ask' call. The spock builder object has a @property called tuner_status that returns any necessary backend objects in a dictionary that the user needs to interface with. In the case of Optuna, this contains both the Optuna study and trial (as dictionary keys). We use the return of tuner_status to handle the 'tell' call based on the metric of interested (here just simple validation accuracy) Continuing in tune.py : # Iterate through a bunch of optuna trials for _ in range ( fixed_params . BasicParams . n_trials ): # Call sample on the spock object hp_attrs = attrs_obj . sample () # Use the currently sampled parameters in a simple LogisticRegression from sklearn clf = LogisticRegression ( C = hp_attrs . LogisticRegressionHP . c , solver = hp_attrs . LogisticRegressionHP . solver , max_iter = hp_attrs . BasicParams . max_iter ) clf . fit ( X_train , y_train ) val_acc = clf . score ( X_valid , y_valid ) # Get the status of the tuner -- this dict will contain all the objects needed to update tuner_status = attrs_obj . tuner_status # Pull the study and trials object out of the return dictionary and pass it to the tell call using the study # object tuner_status [ \"study\" ] . tell ( tuner_status [ \"trial\" ], val_acc )","title":"Sample as an Alternative to Generate"},{"location":"docs/addons/tuner/Saving/","text":"Saving Hyper-Parameter Configs -- Base, Samples, and Best spock provides the capability to save the configuration for each stage of hyper-parameter tuning. Saving Base Hyper-Parameter Definitions First, if we wanted to save the configuration state of the defined hyper-parameter ranges (i.e. the definitions of the parameters that are not sampled) we simply chain the save() call post tuner() call just like we did with basic spock usage. If there are defined hyper-parameters from @spockTuner these will automatically get written into the markdown file along with the fixed parameters. For instance in tune.py : # Chain the .save call which will dump the hyper-parameter definitions to the configuration file attrs_obj = ConfigArgBuilder ( LogisticRegressionHP , BasicParams , desc = \"Example Logistic Regression Hyper-Parameter Tuning\" , ) . tuner ( tuner_config = optuna_config ) . save ( user_specified_path = '/tmp' ) Would result in the following YAML file: BasicParams : max_iter : 150 n_trials : 10 LogisticRegressionHP : c : bounds : - 0.01 - 10.0 log_scale : true type : float solver : choices : - lbfgs - saga type : str Saving Individual Hyper-Parameter Samples If we want to save each individual hyper-parameter sample we again use the save() call with the addition of the add_tuner_sample=True keyword arg and chain it before the sample() call. The order might be slightly confusing but this is to allow all methods to return the builder object except for hte sample() and generate() calls which returns a Spockspace . The saver will append hp.sample.[0-9+] to the filename to identify each sample configuration. For instance in tune.py : # Now we iterate through a bunch of optuna trials for _ in range ( fixed_params . BasicParams . n_trials ): hp_attrs = attrs_obj . save ( add_tuner_sample = True , user_specified_path = \"/tmp\" ) . sample () Would result in n_trials files named hp.sample.[0-9]+.{uuid}.spock.cfg . For instance opening a file named hp.sample.1.d1cc7a30-10f0-4d2c-b076-513fe3494566.spock.cfg.yaml we would see the first sample set of the hyper-parameters: BasicParams : max_iter : 150 n_trials : 10 LogisticRegressionHP : c : 0.21495978453310358 solver : lbfgs Saving the Best Hyper-Parameter Samples If we want to keep track of the current/final best hyper-parameter set based on the optimization metric we use the save_best() call on the builder object. This function takes all the same arguments as the save() method but maintains only a single configuration file that is the current/final best hyper-parameter configuration. The saver will append hp.best. to the filename to identify the best configuration. Note: Make sure this function is only called post all backend handling (in the case of Optuna -- the 'tell' call) for the sample or else an exception will be raised as the best configuration will not yet be registered. For instance in tune.py : # Now we iterate through a bunch of optuna trials for _ in range ( fixed_params . BasicParams . n_trials ): hp_attrs = attrs_obj . sample () # Use the currently sampled parameters in a simple LogisticRegression from sklearn clf = LogisticRegression ( C = hp_attrs . LogisticRegressionHP . c , solver = hp_attrs . LogisticRegressionHP . solver , max_iter = hp_attrs . BasicParams . max_iter ) clf . fit ( X_train , y_train ) val_acc = clf . score ( X_valid , y_valid ) # Get the status of the tuner -- this dict will contain all the objects needed to update tuner_status = attrs_obj . tuner_status # Pull the study and trials object out of the return dictionary and pass it to the tell call using the study # object tuner_status [ \"study\" ] . tell ( tuner_status [ \"trial\" ], val_acc ) # Always save the current best set of hyper-parameters attrs_obj . save_best ( user_specified_path = '/tmp' )","title":"Saving"},{"location":"docs/addons/tuner/Saving/#saving-hyper-parameter-configs-base-samples-and-best","text":"spock provides the capability to save the configuration for each stage of hyper-parameter tuning.","title":"Saving Hyper-Parameter Configs -- Base, Samples, and Best"},{"location":"docs/addons/tuner/Saving/#saving-base-hyper-parameter-definitions","text":"First, if we wanted to save the configuration state of the defined hyper-parameter ranges (i.e. the definitions of the parameters that are not sampled) we simply chain the save() call post tuner() call just like we did with basic spock usage. If there are defined hyper-parameters from @spockTuner these will automatically get written into the markdown file along with the fixed parameters. For instance in tune.py : # Chain the .save call which will dump the hyper-parameter definitions to the configuration file attrs_obj = ConfigArgBuilder ( LogisticRegressionHP , BasicParams , desc = \"Example Logistic Regression Hyper-Parameter Tuning\" , ) . tuner ( tuner_config = optuna_config ) . save ( user_specified_path = '/tmp' ) Would result in the following YAML file: BasicParams : max_iter : 150 n_trials : 10 LogisticRegressionHP : c : bounds : - 0.01 - 10.0 log_scale : true type : float solver : choices : - lbfgs - saga type : str","title":"Saving Base Hyper-Parameter Definitions"},{"location":"docs/addons/tuner/Saving/#saving-individual-hyper-parameter-samples","text":"If we want to save each individual hyper-parameter sample we again use the save() call with the addition of the add_tuner_sample=True keyword arg and chain it before the sample() call. The order might be slightly confusing but this is to allow all methods to return the builder object except for hte sample() and generate() calls which returns a Spockspace . The saver will append hp.sample.[0-9+] to the filename to identify each sample configuration. For instance in tune.py : # Now we iterate through a bunch of optuna trials for _ in range ( fixed_params . BasicParams . n_trials ): hp_attrs = attrs_obj . save ( add_tuner_sample = True , user_specified_path = \"/tmp\" ) . sample () Would result in n_trials files named hp.sample.[0-9]+.{uuid}.spock.cfg . For instance opening a file named hp.sample.1.d1cc7a30-10f0-4d2c-b076-513fe3494566.spock.cfg.yaml we would see the first sample set of the hyper-parameters: BasicParams : max_iter : 150 n_trials : 10 LogisticRegressionHP : c : 0.21495978453310358 solver : lbfgs","title":"Saving Individual Hyper-Parameter Samples"},{"location":"docs/addons/tuner/Saving/#saving-the-best-hyper-parameter-samples","text":"If we want to keep track of the current/final best hyper-parameter set based on the optimization metric we use the save_best() call on the builder object. This function takes all the same arguments as the save() method but maintains only a single configuration file that is the current/final best hyper-parameter configuration. The saver will append hp.best. to the filename to identify the best configuration. Note: Make sure this function is only called post all backend handling (in the case of Optuna -- the 'tell' call) for the sample or else an exception will be raised as the best configuration will not yet be registered. For instance in tune.py : # Now we iterate through a bunch of optuna trials for _ in range ( fixed_params . BasicParams . n_trials ): hp_attrs = attrs_obj . sample () # Use the currently sampled parameters in a simple LogisticRegression from sklearn clf = LogisticRegression ( C = hp_attrs . LogisticRegressionHP . c , solver = hp_attrs . LogisticRegressionHP . solver , max_iter = hp_attrs . BasicParams . max_iter ) clf . fit ( X_train , y_train ) val_acc = clf . score ( X_valid , y_valid ) # Get the status of the tuner -- this dict will contain all the objects needed to update tuner_status = attrs_obj . tuner_status # Pull the study and trials object out of the return dictionary and pass it to the tell call using the study # object tuner_status [ \"study\" ] . tell ( tuner_status [ \"trial\" ], val_acc ) # Always save the current best set of hyper-parameters attrs_obj . save_best ( user_specified_path = '/tmp' )","title":"Saving the Best Hyper-Parameter Samples"},{"location":"docs/advanced_features/About/","text":"Advanced Features This series of pages walks you through most of the advanced features spock provides. At the end you should understand the additional functionality that spock provides and be able to use these advanced features within your own code. The example used in the Advanced Features series of pages can be found here .","title":"About"},{"location":"docs/advanced_features/About/#advanced-features","text":"This series of pages walks you through most of the advanced features spock provides. At the end you should understand the additional functionality that spock provides and be able to use these advanced features within your own code. The example used in the Advanced Features series of pages can be found here .","title":"Advanced Features"},{"location":"docs/advanced_features/Advanced-Types/","text":"Advanced Types spock also supports nested List or Tuple types and advanced argument types (such as repeated objects) that use Enum or @spock decorated classes. All of the advanced types support the use of Optional and setting default values. Example usage of advanced types can be found in the unittests here . Nested List/Tuple Types List[List[int]] -- Defines a list of list of integers. List[List[str]] -- Defines a list of list of strings. Lists/Tuples of Enum from enum import Enum from spock.config import spock from typing import List class StrChoice ( Enum ): option_1 = 'option_1' option_2 = 'option_2' @spock class TypeConfig : list_choice_p_str : List [ StrChoice ] With YAML definitions: list_choice_p_str : [ 'option_1' , 'option_2' ] List/Tuple of Repeated @spock Classes These can be accessed by index and are iterable. from spock.config import spock from typing import List @spock class NestedListStuff : one : int two : str @spock class TypeConfig : nested_list : List [ NestedListStuff ] # To Set Default Value append '= NestedListStuff' With YAML definitions: # Nested List configuration nested_list : NestedListStuff NestedListStuff : - one : 10 two : hello - one : 20 two : bye Enum of @spock Classes from enum import Enum from spock.config import spock @spock class ClassOne : one : int two : str @spock class ClassTwo : one : int two : str class ClassChoice ( Enum ): class_one = ClassOne class_two = ClassTwo @spock class TypeConfig : param : ClassChoice With YAML definitions: # Nested List configuration TypeConfig : param : ClassTwo ClassOne : one : 20 two : bye ClassTwo : one : 10 two : hello","title":"Advanced Types"},{"location":"docs/advanced_features/Advanced-Types/#advanced-types","text":"spock also supports nested List or Tuple types and advanced argument types (such as repeated objects) that use Enum or @spock decorated classes. All of the advanced types support the use of Optional and setting default values. Example usage of advanced types can be found in the unittests here .","title":"Advanced Types"},{"location":"docs/advanced_features/Advanced-Types/#nested-listtuple-types","text":"List[List[int]] -- Defines a list of list of integers. List[List[str]] -- Defines a list of list of strings.","title":"Nested List/Tuple Types"},{"location":"docs/advanced_features/Advanced-Types/#liststuples-of-enum","text":"from enum import Enum from spock.config import spock from typing import List class StrChoice ( Enum ): option_1 = 'option_1' option_2 = 'option_2' @spock class TypeConfig : list_choice_p_str : List [ StrChoice ] With YAML definitions: list_choice_p_str : [ 'option_1' , 'option_2' ]","title":"Lists/Tuples of Enum"},{"location":"docs/advanced_features/Advanced-Types/#listtuple-of-repeated-spock-classes","text":"These can be accessed by index and are iterable. from spock.config import spock from typing import List @spock class NestedListStuff : one : int two : str @spock class TypeConfig : nested_list : List [ NestedListStuff ] # To Set Default Value append '= NestedListStuff' With YAML definitions: # Nested List configuration nested_list : NestedListStuff NestedListStuff : - one : 10 two : hello - one : 20 two : bye","title":"List/Tuple of Repeated @spock Classes"},{"location":"docs/advanced_features/Advanced-Types/#enum-of-spock-classes","text":"from enum import Enum from spock.config import spock @spock class ClassOne : one : int two : str @spock class ClassTwo : one : int two : str class ClassChoice ( Enum ): class_one = ClassOne class_two = ClassTwo @spock class TypeConfig : param : ClassChoice With YAML definitions: # Nested List configuration TypeConfig : param : ClassTwo ClassOne : one : 20 two : bye ClassTwo : one : 10 two : hello","title":"Enum of @spock Classes"},{"location":"docs/advanced_features/Command-Line-Overrides/","text":"Utilizing Command Line Overrides spock supports overriding parameter values set from configuration files via the command line. This can be useful for exploration of parameter values, quick-and-dirty value overrides, or to parse other command-line arguments that would normally require use of another argparser. Automatic Command-Line Argument Generation spock will automatically generate command line arguments for each parameter, unless the no_cmd_line=True flag is passed to the ConfigArgBuilder . Let's look at two of the @spock decorated classes from the tutorial.py file to illustrate how this works in practice: from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple @spock class DataConfig : batch_size : int = 2 n_samples : int = 8 cache_path : Optional [ str ] @spock class OptimizerConfig : lr : float = 0.01 n_epochs : int = 2 grad_clip : Optional [ float ] Given these definitions, spock will automatically generate a command-line argument (via an internally maintained argparser) for each parameter within each @spock decorated class. The syntax follows simple dot notation of --classname.parameter . Thus, for our sample classes above, spock will automatically generate the following valid command-line arguments: --DataConfig.batch_size *value* --DataConfig.n_samples *value* --DataConfig.cache_path *value* --OptimizerConfig.lr *value* --OptimizerConfig.n_epochs *value* --OptimizerConfig.grad_clip *value* None of these command-line arguments are required (i.e. sets required=False within the argparser), but a value must be set via one of the three core mechanisms: (1) a default value (set withing the @spock decorated class), (2) the configuration file (passed in with the --config argument), or (3) the command-line argument (this takes precedence over all other methods). Overriding Configuration File Values Using the automatically generated command-line arguments, let's override a few values from our example in tutorial.py : from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' class Optimizer ( Enum ): sgd = 'SGD' adam = 'Adam' @spock class ModelConfig : save_path : SavePath n_features : int dropout : Optional [ List [ float ]] hidden_sizes : Tuple [ int , int , int ] = ( 32 , 32 , 32 ) activation : Activation = 'relu' optimizer : Optimizer cache_path : Optional [ str ] @spock class DataConfig : batch_size : int = 2 n_samples : int = 8 cache_path : Optional [ str ] @spock class OptimizerConfig : lr : float = 0.01 n_epochs : int = 2 grad_clip : Optional [ float ] @spock class SGDConfig ( OptimizerConfig ): weight_decay : float momentum : float nesterov : bool To run tutorial.py we would normally pass just the path to the configuration file as a command line argument: $ python tutorial.py --config tutorial.yaml But with command line overrides we can also pass parameter arguments to override their value within the configuration file: $ python tutorial.py --config tutorial.yaml --DataConfig.cache_path /tmp/trash Each parameter can be overridden ONLY at the class specific level with the syntax --classname.parameter . For instance, our previous example would only override the DataConfig.cache_path and not the ModelConfig.cache_path even though they have the same parameter name (due to the different class names). $ python tutorial.py --config tutorial.yaml --DataConfig.cache_path /tmp/trash Overriding List/Tuple of Repeated @spock Classes For List of Repeated @spock Classes the syntax is slightly different to allow for the repeated nature of the type. Given the below example code: from spock.config import spock from typing import List @spock class NestedListStuff : one : int two : str @spock class TypeConfig : nested_list : List [ NestedListStuff ] # To Set Default Value append '= NestedListStuff' With YAML definitions: # Nested List configuration nested_list : NestedListStuff NestedListStuff : - one : 10 two : hello - one : 20 two : bye We could override the parameters like so (note that the len must match the defined length from the YAML): $ python tutorial.py --config tutorial.yaml --TypeConfig.nested_list.NestedListStuff.one [ 1 ,2 ] \\ --TypeConfig.nested_list.NestedListStuff.two [ 'ciao' , 'ciao' ] Spock As a Drop In Replacement For Argparser spock can easily be used as a drop in replacement for argparser. This means that all parameter definitions as required to come in from the command line or from setting defaults within the @spock decorated classes. Simply do not pass a -c or --config argument at the command line and instead pass in values to all of the automatically generated cmd-line arguments. See more information here .","title":"Command Line Overrides"},{"location":"docs/advanced_features/Command-Line-Overrides/#utilizing-command-line-overrides","text":"spock supports overriding parameter values set from configuration files via the command line. This can be useful for exploration of parameter values, quick-and-dirty value overrides, or to parse other command-line arguments that would normally require use of another argparser.","title":"Utilizing Command Line Overrides"},{"location":"docs/advanced_features/Command-Line-Overrides/#automatic-command-line-argument-generation","text":"spock will automatically generate command line arguments for each parameter, unless the no_cmd_line=True flag is passed to the ConfigArgBuilder . Let's look at two of the @spock decorated classes from the tutorial.py file to illustrate how this works in practice: from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple @spock class DataConfig : batch_size : int = 2 n_samples : int = 8 cache_path : Optional [ str ] @spock class OptimizerConfig : lr : float = 0.01 n_epochs : int = 2 grad_clip : Optional [ float ] Given these definitions, spock will automatically generate a command-line argument (via an internally maintained argparser) for each parameter within each @spock decorated class. The syntax follows simple dot notation of --classname.parameter . Thus, for our sample classes above, spock will automatically generate the following valid command-line arguments: --DataConfig.batch_size *value* --DataConfig.n_samples *value* --DataConfig.cache_path *value* --OptimizerConfig.lr *value* --OptimizerConfig.n_epochs *value* --OptimizerConfig.grad_clip *value* None of these command-line arguments are required (i.e. sets required=False within the argparser), but a value must be set via one of the three core mechanisms: (1) a default value (set withing the @spock decorated class), (2) the configuration file (passed in with the --config argument), or (3) the command-line argument (this takes precedence over all other methods).","title":"Automatic Command-Line Argument Generation"},{"location":"docs/advanced_features/Command-Line-Overrides/#overriding-configuration-file-values","text":"Using the automatically generated command-line arguments, let's override a few values from our example in tutorial.py : from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' class Optimizer ( Enum ): sgd = 'SGD' adam = 'Adam' @spock class ModelConfig : save_path : SavePath n_features : int dropout : Optional [ List [ float ]] hidden_sizes : Tuple [ int , int , int ] = ( 32 , 32 , 32 ) activation : Activation = 'relu' optimizer : Optimizer cache_path : Optional [ str ] @spock class DataConfig : batch_size : int = 2 n_samples : int = 8 cache_path : Optional [ str ] @spock class OptimizerConfig : lr : float = 0.01 n_epochs : int = 2 grad_clip : Optional [ float ] @spock class SGDConfig ( OptimizerConfig ): weight_decay : float momentum : float nesterov : bool To run tutorial.py we would normally pass just the path to the configuration file as a command line argument: $ python tutorial.py --config tutorial.yaml But with command line overrides we can also pass parameter arguments to override their value within the configuration file: $ python tutorial.py --config tutorial.yaml --DataConfig.cache_path /tmp/trash Each parameter can be overridden ONLY at the class specific level with the syntax --classname.parameter . For instance, our previous example would only override the DataConfig.cache_path and not the ModelConfig.cache_path even though they have the same parameter name (due to the different class names). $ python tutorial.py --config tutorial.yaml --DataConfig.cache_path /tmp/trash","title":"Overriding Configuration File Values"},{"location":"docs/advanced_features/Command-Line-Overrides/#overriding-listtuple-of-repeated-spock-classes","text":"For List of Repeated @spock Classes the syntax is slightly different to allow for the repeated nature of the type. Given the below example code: from spock.config import spock from typing import List @spock class NestedListStuff : one : int two : str @spock class TypeConfig : nested_list : List [ NestedListStuff ] # To Set Default Value append '= NestedListStuff' With YAML definitions: # Nested List configuration nested_list : NestedListStuff NestedListStuff : - one : 10 two : hello - one : 20 two : bye We could override the parameters like so (note that the len must match the defined length from the YAML): $ python tutorial.py --config tutorial.yaml --TypeConfig.nested_list.NestedListStuff.one [ 1 ,2 ] \\ --TypeConfig.nested_list.NestedListStuff.two [ 'ciao' , 'ciao' ]","title":"Overriding List/Tuple of Repeated @spock Classes"},{"location":"docs/advanced_features/Command-Line-Overrides/#spock-as-a-drop-in-replacement-for-argparser","text":"spock can easily be used as a drop in replacement for argparser. This means that all parameter definitions as required to come in from the command line or from setting defaults within the @spock decorated classes. Simply do not pass a -c or --config argument at the command line and instead pass in values to all of the automatically generated cmd-line arguments. See more information here .","title":"Spock As a Drop In Replacement For Argparser"},{"location":"docs/advanced_features/Composition/","text":"Composing Configuration Files spock supports hierarchical composition of configuration files with a simple syntax. Composing Two YAML Files Going back to our example. Let's say we had a a portion of our configuration that does not change that often while another portion changes frequently or are parameters that are being experimented with. For instance, let's say we have finalized things related to our data set (although in our examples it is random... let's just imagine for now) but we are still experimenting with our neural network parameters. Instead of maintaining multiple copies of configuration files where parameters related to the data set are not changing, we can compose two separate configuration files together. One static file related to the data set parameters and a more dynamic file that is changing. This separation helps keep errors from propagating into the static set of data set related parameters. For instance we can break our tutorial.yaml file into two. First, let's split out the static data related parameters into: data.yaml ################ # data.yaml ################ # DataConfig batch_size : 2 n_samples : 8 DataConfig : cache_path : /home/user/cache/ And then in our second configuration file we can use the config: key to define the other configuration files we want to compose into this configuration file: changing.yaml ################ # changing.yaml ################ # Global cache_path : /tmp/cache/ config : [ /data.yaml ] # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true Warning You can add as many configuration files as you want to a config tag however be aware of circular dependencies (this should get caught and raise an exception) and that the lower a configuration file is in the order (i.e. later in the list) that it will take precedence over the others.","title":"Composition"},{"location":"docs/advanced_features/Composition/#composing-configuration-files","text":"spock supports hierarchical composition of configuration files with a simple syntax.","title":"Composing Configuration Files"},{"location":"docs/advanced_features/Composition/#composing-two-yaml-files","text":"Going back to our example. Let's say we had a a portion of our configuration that does not change that often while another portion changes frequently or are parameters that are being experimented with. For instance, let's say we have finalized things related to our data set (although in our examples it is random... let's just imagine for now) but we are still experimenting with our neural network parameters. Instead of maintaining multiple copies of configuration files where parameters related to the data set are not changing, we can compose two separate configuration files together. One static file related to the data set parameters and a more dynamic file that is changing. This separation helps keep errors from propagating into the static set of data set related parameters. For instance we can break our tutorial.yaml file into two. First, let's split out the static data related parameters into: data.yaml ################ # data.yaml ################ # DataConfig batch_size : 2 n_samples : 8 DataConfig : cache_path : /home/user/cache/ And then in our second configuration file we can use the config: key to define the other configuration files we want to compose into this configuration file: changing.yaml ################ # changing.yaml ################ # Global cache_path : /tmp/cache/ config : [ /data.yaml ] # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true","title":"Composing Two YAML Files"},{"location":"docs/advanced_features/Composition/#warning","text":"You can add as many configuration files as you want to a config tag however be aware of circular dependencies (this should get caught and raise an exception) and that the lower a configuration file is in the order (i.e. later in the list) that it will take precedence over the others.","title":"Warning"},{"location":"docs/advanced_features/Defaults/","text":"Defaults spock allows you to set defaults for parameters that are either not set from a configuration file or you no longer need to set (maybe you've finally settled on a standard or would like to fall back to defaults if the user does not know the correct/best parameter to choose). This is done in the spock class definition. Setting Defaults Say we want defaults for the hidden layer sizes and the activation function as well as add a new parameter with a default value. Default values are simply set with the = operator Let's modify the definition in: tutorial.py from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : save_path : SavePath lr : float = 0.01 n_features : int dropout : List [ float ] hidden_sizes : Tuple [ int , int , int ] = ( 32 , 32 , 32 ) activation : Activation = 'relu' We added a new parameter lr that has a default value of 0.01 , and set defaults for hidden_sizes and activation . These values will be used if no values are specified in the configuration file and prevent spock from raising an Exception for required parameters.","title":"Default Values"},{"location":"docs/advanced_features/Defaults/#defaults","text":"spock allows you to set defaults for parameters that are either not set from a configuration file or you no longer need to set (maybe you've finally settled on a standard or would like to fall back to defaults if the user does not know the correct/best parameter to choose). This is done in the spock class definition.","title":"Defaults"},{"location":"docs/advanced_features/Defaults/#setting-defaults","text":"Say we want defaults for the hidden layer sizes and the activation function as well as add a new parameter with a default value. Default values are simply set with the = operator Let's modify the definition in: tutorial.py from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : save_path : SavePath lr : float = 0.01 n_features : int dropout : List [ float ] hidden_sizes : Tuple [ int , int , int ] = ( 32 , 32 , 32 ) activation : Activation = 'relu' We added a new parameter lr that has a default value of 0.01 , and set defaults for hidden_sizes and activation . These values will be used if no values are specified in the configuration file and prevent spock from raising an Exception for required parameters.","title":"Setting Defaults"},{"location":"docs/advanced_features/Inheritance/","text":"Inheritance spock supports class inheritance between different defined spock classes. This allows you to build complex configurations derived from a common base class or classes. Defining an Inherited spock Class Back to our example. We have implemented two different optimizers to train our neural network. In its current state we have overlooked the fact that the two different optimizers share a set of common parameters but each also has a set of specific parameters. Instead of defining redundant parameter definitions let's use spock inheritance. We create a new spock class that inherits from another spock class. This functions just like traditional inheritance where the child will inherit the parameter definitions from the parent class. Editing our definition in: tutorial.py from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' class Optimizer ( Enum ): sgd = 'SGD' adam = 'Adam' @spock class ModelConfig : save_path : SavePath n_features : int dropout : Optional [ List [ float ]] hidden_sizes : Tuple [ int , int , int ] = ( 32 , 32 , 32 ) activation : Activation = 'relu' optimizer : Optimizer @spock class DataConfig : batch_size : int = 2 n_samples : int = 8 @spock class OptimizerConfig : lr : float = 0.01 n_epochs : int = 2 grad_clip : Optional [ float ] @spock class SGDConfig ( OptimizerConfig ): weight_decay : float momentum : float nesterov : bool Editing our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true Using an Inherited spock Class Let's use our inherited class to use the SGD optimizer with the defined parameter on our basic neural network: tutorial.py import torch from .basic_nn import BasicNet def train ( x_data , y_data , model , model_config , data_config , optimizer_config ): if model_config . optimizer == 'SGD' : optimizer = torch . optim . SGD ( model . parameters (), lr = optimizer_config . lr , momentum = optimizer_config . momentum , nesterov = optimizer_config . nesterov ) elif model_config . optimizer == 'Adam' : optimizer = torch . optim . Adam ( model . parameters (), lr = optimizer_config . lr ) else : raise ValueError ( f 'Optimizer choice { optimizer_config . optimizer } not available' ) n_steps_per_epoch = data_config . n_samples % data_config . batch_size for epoch in range ( optimizer_config . n_epochs ): for i in range ( n_steps_per_epoch ): # Ugly data slicing for simplicity x_batch = x_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch , ] y_batch = y_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch , ] optimizer . zero_grad () output = model ( x_batch ) loss = torch . nn . CrossEntropyLoss ( output , y_batch ) loss . backward () if optimizer_config . grad_clip : torch . nn . utils . clip_grad_value ( model . parameters (), optimizer_config . grad_clip ) optimizer . step () print ( f 'Finished Epoch { epoch + 1 } ' ) def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , SGDConfig , desc = description ) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in x_data = torch . rand ( config . DataConfig . n_samples , config . ModelConfig . n_features ) y_data = torch . randint ( 0 , 3 , ( config . DataConfig . n_samples ,)) # Run some training train ( x_data , y_data , basic_nn , config . ModelConfig , config . DataConfig , config . SGDConfig )","title":"Inheritance"},{"location":"docs/advanced_features/Inheritance/#inheritance","text":"spock supports class inheritance between different defined spock classes. This allows you to build complex configurations derived from a common base class or classes.","title":"Inheritance"},{"location":"docs/advanced_features/Inheritance/#defining-an-inherited-spock-class","text":"Back to our example. We have implemented two different optimizers to train our neural network. In its current state we have overlooked the fact that the two different optimizers share a set of common parameters but each also has a set of specific parameters. Instead of defining redundant parameter definitions let's use spock inheritance. We create a new spock class that inherits from another spock class. This functions just like traditional inheritance where the child will inherit the parameter definitions from the parent class. Editing our definition in: tutorial.py from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' class Optimizer ( Enum ): sgd = 'SGD' adam = 'Adam' @spock class ModelConfig : save_path : SavePath n_features : int dropout : Optional [ List [ float ]] hidden_sizes : Tuple [ int , int , int ] = ( 32 , 32 , 32 ) activation : Activation = 'relu' optimizer : Optimizer @spock class DataConfig : batch_size : int = 2 n_samples : int = 8 @spock class OptimizerConfig : lr : float = 0.01 n_epochs : int = 2 grad_clip : Optional [ float ] @spock class SGDConfig ( OptimizerConfig ): weight_decay : float momentum : float nesterov : bool Editing our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true","title":"Defining an Inherited spock Class"},{"location":"docs/advanced_features/Inheritance/#using-an-inherited-spock-class","text":"Let's use our inherited class to use the SGD optimizer with the defined parameter on our basic neural network: tutorial.py import torch from .basic_nn import BasicNet def train ( x_data , y_data , model , model_config , data_config , optimizer_config ): if model_config . optimizer == 'SGD' : optimizer = torch . optim . SGD ( model . parameters (), lr = optimizer_config . lr , momentum = optimizer_config . momentum , nesterov = optimizer_config . nesterov ) elif model_config . optimizer == 'Adam' : optimizer = torch . optim . Adam ( model . parameters (), lr = optimizer_config . lr ) else : raise ValueError ( f 'Optimizer choice { optimizer_config . optimizer } not available' ) n_steps_per_epoch = data_config . n_samples % data_config . batch_size for epoch in range ( optimizer_config . n_epochs ): for i in range ( n_steps_per_epoch ): # Ugly data slicing for simplicity x_batch = x_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch , ] y_batch = y_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch , ] optimizer . zero_grad () output = model ( x_batch ) loss = torch . nn . CrossEntropyLoss ( output , y_batch ) loss . backward () if optimizer_config . grad_clip : torch . nn . utils . clip_grad_value ( model . parameters (), optimizer_config . grad_clip ) optimizer . step () print ( f 'Finished Epoch { epoch + 1 } ' ) def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , SGDConfig , desc = description ) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in x_data = torch . rand ( config . DataConfig . n_samples , config . ModelConfig . n_features ) y_data = torch . randint ( 0 , 3 , ( config . DataConfig . n_samples ,)) # Run some training train ( x_data , y_data , basic_nn , config . ModelConfig , config . DataConfig , config . SGDConfig )","title":"Using an Inherited spock Class"},{"location":"docs/advanced_features/Keyword-Configs/","text":"Keyword Configs spock supports adding and/or overriding the config file path(s) normally specified via the command line argument -c with keyword arguments. Specifying The Config Keyword Argument Let's pass in the yaml configuration file via the config keyword argument instead of at the command line. Simply add the config keyword argument to the ConfigArgBuilder . Note: This is not the recommended best practice as it creates a dependency between code and configuration files. Please use the -c command line argument whenever possible. The config keyword arg should be used ONLY when necessary. Editing our definition in: tutorial.py ... def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , SGDConfig , desc = description , config = [ './tutorial.yaml' ]) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in x_data = torch . rand ( config . DataConfig . n_samples , config . ModelConfig . n_features ) y_data = torch . randint ( 0 , 3 , ( config . DataConfig . n_samples ,)) # Run some training train ( x_data , y_data , basic_nn , config . ModelConfig , config . DataConfig , config . SGDConfig ) Now to run tutorial.py we don't need to pass a command line argument: $ python tutorial.py Specifying The Config Keyword Argument & The No Command Line Flag So if the config keyword arg is not recommended why do we support it? Mainly for two reasons: - Programmatic access to configuration files for other code/infrastructure (e.g. dispatching jobs from a work queue that might need to be parametrized) - To prevent command line arg clashes with other python code/libraries that might use the same or similar syntax (e.g. FastAPI) For instance, let's say we were wrapping our simple neural net example into an async REST API (using something like FastAPI and a message queue such as redis). The FastAPI docker image has it's own set of command line arguments that get called and will clash with spock . Therefore we need to pass the configuration file(s) through the config keyword argument and deactivate the command line argument. For instance, we create a route for our basic neural network (shown below). We add the no_cmd_line=True flag to the ConfigArgBuilder to prevent spock from references command line arguments: @api . post ( \"/inference/\" , status_code = 201 ) def create_job ( * , data : schemata . Inference ): # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , SGDConfig , desc = description , config = [ './tutorial.yaml' ], no_cmd_line = True ) . generate () # Let's assume we have a model loading function based on our params basic_nn = LoadBasicNet ( model_config = config . ModelConfig ) # Make a prediction y = basic_nn ( data . x ) # Return the predictions return_schema = schemata . InferenceReturn ( y = y , ) return return_schema","title":"Keyword Configs"},{"location":"docs/advanced_features/Keyword-Configs/#keyword-configs","text":"spock supports adding and/or overriding the config file path(s) normally specified via the command line argument -c with keyword arguments.","title":"Keyword Configs"},{"location":"docs/advanced_features/Keyword-Configs/#specifying-the-config-keyword-argument","text":"Let's pass in the yaml configuration file via the config keyword argument instead of at the command line. Simply add the config keyword argument to the ConfigArgBuilder . Note: This is not the recommended best practice as it creates a dependency between code and configuration files. Please use the -c command line argument whenever possible. The config keyword arg should be used ONLY when necessary. Editing our definition in: tutorial.py ... def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , SGDConfig , desc = description , config = [ './tutorial.yaml' ]) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in x_data = torch . rand ( config . DataConfig . n_samples , config . ModelConfig . n_features ) y_data = torch . randint ( 0 , 3 , ( config . DataConfig . n_samples ,)) # Run some training train ( x_data , y_data , basic_nn , config . ModelConfig , config . DataConfig , config . SGDConfig ) Now to run tutorial.py we don't need to pass a command line argument: $ python tutorial.py","title":"Specifying The Config Keyword Argument"},{"location":"docs/advanced_features/Keyword-Configs/#specifying-the-config-keyword-argument-the-no-command-line-flag","text":"So if the config keyword arg is not recommended why do we support it? Mainly for two reasons: - Programmatic access to configuration files for other code/infrastructure (e.g. dispatching jobs from a work queue that might need to be parametrized) - To prevent command line arg clashes with other python code/libraries that might use the same or similar syntax (e.g. FastAPI) For instance, let's say we were wrapping our simple neural net example into an async REST API (using something like FastAPI and a message queue such as redis). The FastAPI docker image has it's own set of command line arguments that get called and will clash with spock . Therefore we need to pass the configuration file(s) through the config keyword argument and deactivate the command line argument. For instance, we create a route for our basic neural network (shown below). We add the no_cmd_line=True flag to the ConfigArgBuilder to prevent spock from references command line arguments: @api . post ( \"/inference/\" , status_code = 201 ) def create_job ( * , data : schemata . Inference ): # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , SGDConfig , desc = description , config = [ './tutorial.yaml' ], no_cmd_line = True ) . generate () # Let's assume we have a model loading function based on our params basic_nn = LoadBasicNet ( model_config = config . ModelConfig ) # Make a prediction y = basic_nn ( data . x ) # Return the predictions return_schema = schemata . InferenceReturn ( y = y , ) return return_schema","title":"Specifying The Config Keyword Argument &amp; The No Command Line Flag"},{"location":"docs/advanced_features/Local-Definitions/","text":"Local Definitions The class based solution within spock provides the ability to change a global parameter value within a local class context. Overriding a Global Value Let's define two new parameters with the same name but in two different classes that represent where some stuff is going to be cached. One for the model and one for some data. Editing our definition in: tutorial.py from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' class Optimizer ( Enum ): sgd = 'SGD' adam = 'Adam' @spock class ModelConfig : save_path : SavePath n_features : int dropout : Optional [ List [ float ]] hidden_sizes : Tuple [ int , int , int ] = ( 32 , 32 , 32 ) activation : Activation = 'relu' optimizer : Optimizer cache_path : Optional [ str ] @spock class DataConfig : batch_size : int = 2 n_samples : int = 8 cache_path : Optional [ str ] @spock class OptimizerConfig : lr : float = 0.01 n_epochs : int = 2 grad_clip : Optional [ float ] @spock class SGDConfig ( OptimizerConfig ): weight_decay : float momentum : float nesterov : bool Now, if we edit our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Global cache_path : /tmp/cache/ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true This configuration file would set both parameters to use the /tmp/cache/ value (i.e. it would set the parameter value globally). But what if we want to the data cache to be a different path? We can override the global parameter value with a local parameter value. Editing our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Global cache_path : /tmp/cache/ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] n_hidden : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 DataConfig : cache_path : /home/user/cache/ # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true","title":"Local Defintions"},{"location":"docs/advanced_features/Local-Definitions/#local-definitions","text":"The class based solution within spock provides the ability to change a global parameter value within a local class context.","title":"Local Definitions"},{"location":"docs/advanced_features/Local-Definitions/#overriding-a-global-value","text":"Let's define two new parameters with the same name but in two different classes that represent where some stuff is going to be cached. One for the model and one for some data. Editing our definition in: tutorial.py from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' class Optimizer ( Enum ): sgd = 'SGD' adam = 'Adam' @spock class ModelConfig : save_path : SavePath n_features : int dropout : Optional [ List [ float ]] hidden_sizes : Tuple [ int , int , int ] = ( 32 , 32 , 32 ) activation : Activation = 'relu' optimizer : Optimizer cache_path : Optional [ str ] @spock class DataConfig : batch_size : int = 2 n_samples : int = 8 cache_path : Optional [ str ] @spock class OptimizerConfig : lr : float = 0.01 n_epochs : int = 2 grad_clip : Optional [ float ] @spock class SGDConfig ( OptimizerConfig ): weight_decay : float momentum : float nesterov : bool Now, if we edit our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Global cache_path : /tmp/cache/ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true This configuration file would set both parameters to use the /tmp/cache/ value (i.e. it would set the parameter value globally). But what if we want to the data cache to be a different path? We can override the global parameter value with a local parameter value. Editing our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Global cache_path : /tmp/cache/ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] n_hidden : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 DataConfig : cache_path : /home/user/cache/ # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true","title":"Overriding a Global Value"},{"location":"docs/advanced_features/Optional-Parameters/","text":"Optional Parameters spock allows for parameters to be defined as optional. This means that if the parameter value is not set from either a configuration file or a default value it will be assigned the None value. Optional spock parameters are defined using the Optional type from the typing standard library. Defining Optional spock Parameters Optional parameters commonly occur in applications with complex behavior (like neural networks). For instance, what if you want to execute a specific behavior with some parameter(s) if the parameter is defined and if the parameter is not defined either skip the behavior or so something different. Normally this would require a combination of boolean logic and parameter definition (which might be useless...). spock remedies this with optional parameters. As an example, let's assume we want to make dropout within our basic neural network optional. Let's modify the definition in: tutorial.py from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : save_path : SavePath lr : float = 0.01 n_features : int dropout : Optional [ List [ float ]] hidden_sizes : Tuple [ int , int , int ] = ( 32 , 32 , 32 ) activation : Activation = 'relu' Notice that all we did was change the type from List[float] to Optional[List[float]] . Now let's edit our simple neural network code to reflect that dropout is now optional. We have to change the code a bit to be more modular (but still ugly for demonstration): basic_nn.py import torch.nn as nn class BasicNet ( nn . Module ): def __init__ ( self , model_config ): super ( BasicNet , self ) . __init__ () # Make a dictionary of activation functions to select from self . act_fncs = { 'relu' : nn . ReLU , 'gelu' : nn . GELU , 'tanh' : nn . Tanh } self . use_act = self . act_fncs . get ( model_config . activation )() # Define the layers manually (avoiding list comprehension for clarity) self . layer_1 = nn . Linear ( model_config . n_features , model_config . hidden_sizes [ 0 ]) self . layer_2 = nn . Linear ( model_config . hidden_sizes [ 0 ], model_config . hidden_sizes [ 1 ]) self . layer_3 = nn . Linear ( model_config . hidden_sizes [ 1 ], model_config . hidden_sizes [ 2 ]) # Define some dropout layers self . dropout = [] if model_config . dropout is not None : self . dropout = [ nn . Dropout ( val ) for val in model_config . dropout ] # Define the output layer self . softmax = nn . Softmax ( dim = 1 ) def forward ( self , x ): # x is the data input # Layer 1 # Linear x = self . layer_1 ( x ) # Activation x = self . use_act ( x ) # Dropout if len ( self . dropout ) != 0 : x = self . dropout [ 0 ]( x ) # Layer 2 # Linear x = self . layer_2 ( x ) # Activation x = self . use_act ( x ) # Dropout if len ( self . dropout ) != 0 : x = self . dropout [ 1 ]( x ) # Layer 3 # Linear x = self . layer_3 ( x ) # Softmax output = self . softmax ( x ) return output Code Behavior If we use the same configuration file defined in: tutorial.yaml ################ # tutorial.yaml ################ # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu With this configuration file, the parameter dropout is assigned the specified value. Thus our basic neural network will have dropout layers between Layer 1, Layer 2, and Layer 3. However, if we use the following configuration file: tutorial_no_dropout.yaml ################ # tutorial.yaml ################ # ModelConfig n_features : 64 hidden_sizes : [ 32 , 32 , 16 ] activation : relu With this configuration file, the parameter dropout is assigned None . Thus our based on the logic in our code our basic neural network will not have dropout between layers. This simple example demonstrates the power of spock optional parameters. Flow through code can easily be modified by simply changing the configuration file.","title":"Optional Parameters"},{"location":"docs/advanced_features/Optional-Parameters/#optional-parameters","text":"spock allows for parameters to be defined as optional. This means that if the parameter value is not set from either a configuration file or a default value it will be assigned the None value. Optional spock parameters are defined using the Optional type from the typing standard library.","title":"Optional Parameters"},{"location":"docs/advanced_features/Optional-Parameters/#defining-optional-spock-parameters","text":"Optional parameters commonly occur in applications with complex behavior (like neural networks). For instance, what if you want to execute a specific behavior with some parameter(s) if the parameter is defined and if the parameter is not defined either skip the behavior or so something different. Normally this would require a combination of boolean logic and parameter definition (which might be useless...). spock remedies this with optional parameters. As an example, let's assume we want to make dropout within our basic neural network optional. Let's modify the definition in: tutorial.py from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : save_path : SavePath lr : float = 0.01 n_features : int dropout : Optional [ List [ float ]] hidden_sizes : Tuple [ int , int , int ] = ( 32 , 32 , 32 ) activation : Activation = 'relu' Notice that all we did was change the type from List[float] to Optional[List[float]] . Now let's edit our simple neural network code to reflect that dropout is now optional. We have to change the code a bit to be more modular (but still ugly for demonstration): basic_nn.py import torch.nn as nn class BasicNet ( nn . Module ): def __init__ ( self , model_config ): super ( BasicNet , self ) . __init__ () # Make a dictionary of activation functions to select from self . act_fncs = { 'relu' : nn . ReLU , 'gelu' : nn . GELU , 'tanh' : nn . Tanh } self . use_act = self . act_fncs . get ( model_config . activation )() # Define the layers manually (avoiding list comprehension for clarity) self . layer_1 = nn . Linear ( model_config . n_features , model_config . hidden_sizes [ 0 ]) self . layer_2 = nn . Linear ( model_config . hidden_sizes [ 0 ], model_config . hidden_sizes [ 1 ]) self . layer_3 = nn . Linear ( model_config . hidden_sizes [ 1 ], model_config . hidden_sizes [ 2 ]) # Define some dropout layers self . dropout = [] if model_config . dropout is not None : self . dropout = [ nn . Dropout ( val ) for val in model_config . dropout ] # Define the output layer self . softmax = nn . Softmax ( dim = 1 ) def forward ( self , x ): # x is the data input # Layer 1 # Linear x = self . layer_1 ( x ) # Activation x = self . use_act ( x ) # Dropout if len ( self . dropout ) != 0 : x = self . dropout [ 0 ]( x ) # Layer 2 # Linear x = self . layer_2 ( x ) # Activation x = self . use_act ( x ) # Dropout if len ( self . dropout ) != 0 : x = self . dropout [ 1 ]( x ) # Layer 3 # Linear x = self . layer_3 ( x ) # Softmax output = self . softmax ( x ) return output","title":"Defining Optional spock Parameters"},{"location":"docs/advanced_features/Optional-Parameters/#code-behavior","text":"If we use the same configuration file defined in: tutorial.yaml ################ # tutorial.yaml ################ # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu With this configuration file, the parameter dropout is assigned the specified value. Thus our basic neural network will have dropout layers between Layer 1, Layer 2, and Layer 3. However, if we use the following configuration file: tutorial_no_dropout.yaml ################ # tutorial.yaml ################ # ModelConfig n_features : 64 hidden_sizes : [ 32 , 32 , 16 ] activation : relu With this configuration file, the parameter dropout is assigned None . Thus our based on the logic in our code our basic neural network will not have dropout between layers. This simple example demonstrates the power of spock optional parameters. Flow through code can easily be modified by simply changing the configuration file.","title":"Code Behavior"},{"location":"docs/advanced_features/Parameter-Groups/","text":"Parameter Groups Since spock manages complex configurations via a class based solution we can define and decorate multiple classes with @spock . Each class gets created as a separate class object within the spock namespace object. Building spock Parameter Groups Let's go back to our example. Say we need to add a few more parameters to our code. We could just keep adding them to the single defined class, but this would lead to a 'mega' class that has parameters for many different parts of your code. Instead, we will define two new spock classes for our new parameters and begin to organize them by functionality. Editing our definition in: tutorial.py from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' class Optimizer ( Enum ): sgd = 'SGD' adam = 'Adam' @spock class ModelConfig : save_path : SavePath n_features : int dropout : Optional [ List [ float ]] hidden_sizes : Tuple [ int , int , int ] = ( 32 , 32 , 32 ) activation : Activation = 'relu' optimizer : Optimizer @spock class DataConfig : batch_size : int = 2 n_samples : int = 8 @spock class OptimizerConfig : lr : float = 0.01 n_epochs : int = 2 grad_clip : Optional [ float ] Now we have three separate spock classes that we need to generate the namespace object from. Simply add the new classes to *args in the ConfigArgBuilder . Editing tutorial.py : from spock.builder import ConfigArgBuilder def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , OptimizerConfig , desc = description ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) print ( config . OptimizerConfig ) Editing our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 Adding More Code Let's add a bit more functionality to our code that uses our new parameters by running a 'basic training loop' (this is kept very simple for illustrative purposes, hence the simple data slicing) on our basic neural network: tutorial.py import torch from .basic_nn import BasicNet def train ( x_data , y_data , model , model_config , data_config , optimizer_config ): if model_config . optimizer == 'SGD' : optimizer = torch . optim . SGD ( model . parameters (), lr = optimizer_config . lr ) elif model_config . optimizer == 'Adam' : optimizer = torch . optim . Adam ( model . parameters (), lr = optimizer_config . lr ) else : raise ValueError ( f 'Optimizer choice { optimizer_config . optimizer } not available' ) n_steps_per_epoch = data_config . n_samples % data_config . batch_size for epoch in range ( optimizer_config . n_epochs ): for i in range ( n_steps_per_epoch ): # Ugly data slicing for simplicity x_batch = x_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch ,] y_batch = y_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch ,] optimizer . zero_grad () output = model ( x_batch ) loss = torch . nn . CrossEntropyLoss ( output , y_batch ) loss . backward () if optimizer_config . grad_clip : torch . nn . utils . clip_grad_value ( model . parameters (), optimizer_config . grad_clip ) optimizer . step () def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , OptimizerConfig , desc = description ) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in x_data = torch . rand ( config . DataConfig . n_samples , config . ModelConfig . n_features ) y_data = torch . randint ( 0 , 3 , ( config . DataConig . n_samples ,)) # Run some training train ( x_data , y_data , basic_nn , config . ModelConfig , config . DataConfig , config . OptimizerConfig )","title":"Parameter Groups"},{"location":"docs/advanced_features/Parameter-Groups/#parameter-groups","text":"Since spock manages complex configurations via a class based solution we can define and decorate multiple classes with @spock . Each class gets created as a separate class object within the spock namespace object.","title":"Parameter Groups"},{"location":"docs/advanced_features/Parameter-Groups/#building-spock-parameter-groups","text":"Let's go back to our example. Say we need to add a few more parameters to our code. We could just keep adding them to the single defined class, but this would lead to a 'mega' class that has parameters for many different parts of your code. Instead, we will define two new spock classes for our new parameters and begin to organize them by functionality. Editing our definition in: tutorial.py from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Optional from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' class Optimizer ( Enum ): sgd = 'SGD' adam = 'Adam' @spock class ModelConfig : save_path : SavePath n_features : int dropout : Optional [ List [ float ]] hidden_sizes : Tuple [ int , int , int ] = ( 32 , 32 , 32 ) activation : Activation = 'relu' optimizer : Optimizer @spock class DataConfig : batch_size : int = 2 n_samples : int = 8 @spock class OptimizerConfig : lr : float = 0.01 n_epochs : int = 2 grad_clip : Optional [ float ] Now we have three separate spock classes that we need to generate the namespace object from. Simply add the new classes to *args in the ConfigArgBuilder . Editing tutorial.py : from spock.builder import ConfigArgBuilder def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , OptimizerConfig , desc = description ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) print ( config . OptimizerConfig ) Editing our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0","title":"Building spock Parameter Groups"},{"location":"docs/advanced_features/Parameter-Groups/#adding-more-code","text":"Let's add a bit more functionality to our code that uses our new parameters by running a 'basic training loop' (this is kept very simple for illustrative purposes, hence the simple data slicing) on our basic neural network: tutorial.py import torch from .basic_nn import BasicNet def train ( x_data , y_data , model , model_config , data_config , optimizer_config ): if model_config . optimizer == 'SGD' : optimizer = torch . optim . SGD ( model . parameters (), lr = optimizer_config . lr ) elif model_config . optimizer == 'Adam' : optimizer = torch . optim . Adam ( model . parameters (), lr = optimizer_config . lr ) else : raise ValueError ( f 'Optimizer choice { optimizer_config . optimizer } not available' ) n_steps_per_epoch = data_config . n_samples % data_config . batch_size for epoch in range ( optimizer_config . n_epochs ): for i in range ( n_steps_per_epoch ): # Ugly data slicing for simplicity x_batch = x_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch ,] y_batch = y_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch ,] optimizer . zero_grad () output = model ( x_batch ) loss = torch . nn . CrossEntropyLoss ( output , y_batch ) loss . backward () if optimizer_config . grad_clip : torch . nn . utils . clip_grad_value ( model . parameters (), optimizer_config . grad_clip ) optimizer . step () def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , OptimizerConfig , desc = description ) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in x_data = torch . rand ( config . DataConfig . n_samples , config . ModelConfig . n_features ) y_data = torch . randint ( 0 , 3 , ( config . DataConig . n_samples ,)) # Run some training train ( x_data , y_data , basic_nn , config . ModelConfig , config . DataConfig , config . OptimizerConfig )","title":"Adding More Code"},{"location":"docs/basic_tutorial/About/","text":"Basic Tutorial This series of pages will walk you through the basics of spock . At the end you should understand the basic functionality that spock provides and be able to use the core functional units within your own code. The example used in the Basic tutorial series of pages can be found here .","title":"About"},{"location":"docs/basic_tutorial/About/#basic-tutorial","text":"This series of pages will walk you through the basics of spock . At the end you should understand the basic functionality that spock provides and be able to use the core functional units within your own code. The example used in the Basic tutorial series of pages can be found here .","title":"Basic Tutorial"},{"location":"docs/basic_tutorial/Building/","text":"Build Once all of the parameters we need have been defined in our spock class and we've written some code to use those parameters we need to generate the namespace object. The namespace object is the heart of spock and is how one accesses all of the defined parameters. The generation of the namespace should happen at the highest level of code, preferably in the main guard protected call or main function call. This allows the namespace object, the spock classes, or the individual parameters to be passed to lower level functionality. Generate the spock Namespace Object So let's continue in: tutorial.py Recall that we defined our spock class as such: class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : n_features : int dropout : List [ float ] hidden_sizes : Tuple [ int , int , int ] activation : Activation To generate the namespace object, import the ConfigArgBuilder class, pass in your @spock classes as *args , add an optional description, and then chain call the generate() method. Each spock class is defined in the namespace object given by the class name. from spock.builder import ConfigArgBuilder def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) if __name__ == '__main__' : main () Using spock Parameters Our simple neural network code referenced some spock defined parameters. So let's link them together correctly and test our model. We will pass the full spock class from the generated namespace object to our nn.module class. Continuing in: tutorial.py import torch from .basic_nn import BasicNet def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in test_data = torch . rand ( 10 , config . ModelConfig . n_features ) result = basic_nn ( test_data ) print ( result )","title":"Building"},{"location":"docs/basic_tutorial/Building/#build","text":"Once all of the parameters we need have been defined in our spock class and we've written some code to use those parameters we need to generate the namespace object. The namespace object is the heart of spock and is how one accesses all of the defined parameters. The generation of the namespace should happen at the highest level of code, preferably in the main guard protected call or main function call. This allows the namespace object, the spock classes, or the individual parameters to be passed to lower level functionality.","title":"Build"},{"location":"docs/basic_tutorial/Building/#generate-the-spock-namespace-object","text":"So let's continue in: tutorial.py Recall that we defined our spock class as such: class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : n_features : int dropout : List [ float ] hidden_sizes : Tuple [ int , int , int ] activation : Activation To generate the namespace object, import the ConfigArgBuilder class, pass in your @spock classes as *args , add an optional description, and then chain call the generate() method. Each spock class is defined in the namespace object given by the class name. from spock.builder import ConfigArgBuilder def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) if __name__ == '__main__' : main ()","title":"Generate the spock Namespace Object"},{"location":"docs/basic_tutorial/Building/#using-spock-parameters","text":"Our simple neural network code referenced some spock defined parameters. So let's link them together correctly and test our model. We will pass the full spock class from the generated namespace object to our nn.module class. Continuing in: tutorial.py import torch from .basic_nn import BasicNet def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in test_data = torch . rand ( 10 , config . ModelConfig . n_features ) result = basic_nn ( test_data ) print ( result )","title":"Using spock Parameters"},{"location":"docs/basic_tutorial/Configuration-Files/","text":"Configuration Files Values in spock are set using external configuration files. Supported Configuration Formats YAML Requires file extension of .yaml . Supported using the external PyYAML library. TOML Requires file extension of .toml . Supported using the external toml library. JSON Requires file extension of .json . Supported using the built-in json module. Creating a Configuration File Recall that we defined our spock class as such: class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : n_features : int dropout : List [ float ] hidden_sizes : Tuple [ int , int , int ] activation : Activation Note that all of the types of our parameters int , List , Tuple , and Activation are required types. This means that if we do not specify values for these parameters in our configuration file spock will throw an Exception. Let's create our configuration file using the YAML standard: tutorial.yaml ################ # tutorial.yaml ################ # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu","title":"Configuration Files"},{"location":"docs/basic_tutorial/Configuration-Files/#configuration-files","text":"Values in spock are set using external configuration files.","title":"Configuration Files"},{"location":"docs/basic_tutorial/Configuration-Files/#supported-configuration-formats","text":"","title":"Supported Configuration Formats"},{"location":"docs/basic_tutorial/Configuration-Files/#yaml","text":"Requires file extension of .yaml . Supported using the external PyYAML library.","title":"YAML"},{"location":"docs/basic_tutorial/Configuration-Files/#toml","text":"Requires file extension of .toml . Supported using the external toml library.","title":"TOML"},{"location":"docs/basic_tutorial/Configuration-Files/#json","text":"Requires file extension of .json . Supported using the built-in json module.","title":"JSON"},{"location":"docs/basic_tutorial/Configuration-Files/#creating-a-configuration-file","text":"Recall that we defined our spock class as such: class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : n_features : int dropout : List [ float ] hidden_sizes : Tuple [ int , int , int ] activation : Activation Note that all of the types of our parameters int , List , Tuple , and Activation are required types. This means that if we do not specify values for these parameters in our configuration file spock will throw an Exception. Let's create our configuration file using the YAML standard: tutorial.yaml ################ # tutorial.yaml ################ # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu","title":"Creating a Configuration File"},{"location":"docs/basic_tutorial/Define/","text":"Define spock manages complex configurations via a class based solution. All parameters are defined in a class or multiple classes decorated using the @spock decorator. Parameters are defined with base types or those defined within the typing module and are type checked at run time. Once built, all parameters can be found within an automatically generated namespace object that contains each class that can be accessed with the given @spock class name. All examples can be found here . Supported Parameter Types Basic Types spock supports the following basic argument types (note List , Tuple , and Optional are defined in the typing standard library while Enum is within the enum standard library): Python Base or Typing Type (Required) Optional Type Description bool Optional[bool] Basic boolean parameter (e.g. True) float Optional[float] Basic float type parameter (e.g. 10.2) int Optional[int] Basic integer type parameter (e.g. 2) str Optional[str] Basic string type parameter (e.g. 'foo') List[type] Optional[List[type]] Basic list type parameter of base types such as int, float, etc. (e.g. [10.0, 2.0]) Tuple[type] Optional[Tuple[type]] Basic tuple type parameter of base types such as int, float, etc. Length enforced unlike List. (e.g. (10, 2)) Enum Optional[Enum] Parameter that must be from a defined set of values of base types such as int, float, etc. @spock decorated Class Optional[Class] Parameter that is a reference to another @spock decorated class Use List types when the length of the Iterable is not fixed and Tuple when length needs to be strictly enforced. Parameters that are specified without the Optional[] type will be considered REQUIRED and therefore will raise an Exception if not value is specified. Advanced Types spock supports more than just basic types. More information can be found in the Advanced Types section. Defining a spock Class Let's start building out an example (a simple neural net in PyTorch) that we will continue to use within the tutorial: tutorial.py Here we import the basic units of functionality from spock . We define our class using the @spock decorator and define our parameters with supported argument types. Parameters are defined within the class by using the format parameter: type . Note that to create a parameter that is required to be within a specified set one must first define an Enum class object with the given options. The Enum class is then passed to your spock class just like other types. from enum import Enum from spock.config import spock from typing import List from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : n_features : int dropout : List [ float ] hidden_sizes : Tuple [ int , int , int ] activation : Activation Adding Help Information spock uses the Google docstring style format to support adding help information to classes and Enums. spock will look for the first contiguous line of text within the docstring as the class help information. spock looks within the Attributes section of the docstring for help information for each parameter. Modifying the above code to include help information: from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Tuple class Activation ( Enum ): \"\"\"Options for activation functions Attributes: relu: relu activation gelu: gelu activation tanh: tanh activation \"\"\" relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : \"\"\"Main model configuration for a basic neural net Attributes: save_path: spock special keyword -- path to write out spock config state n_features: number of data features dropout: dropout rate for each layer hidden_sizes: hidden size for each layer activation: choice from the Activation enum of the activation function to use \"\"\" save_path : SavePath n_features : int dropout : List [ float ] hidden_sizes : Tuple [ int , int , int ] activation : Activation If we run our tutorial.py script with the --help flag: $ python tutorial.py --help We should see the help information we added to the docstring(s): usage: /Users/a635179/Documents/git_repos/open_source/spock/examples/tutorial/basic/tutorial.py -c [ --config ] config1 [ config2, config3, ... ] spock Basic Tutorial configuration ( s ) : ModelConfig ( Main model configuration for a basic neural net ) save_path Optional [ SavePath ] spock special keyword -- path to write out spock config state ( default: None ) n_features int number of data features dropout List [ float ] dropout rate for each layer hidden_sizes Tuple [ int, int, int ] hidden size for each layer activation Activation choice from the Activation enum of the activation function to use Activation ( Options for activation functions ) relu str relu activation gelu str gelu activation tanh str tanh activation Using spock Parameters: Writing More Code In another file let's write our simple neural network code: basic_nn.py Notice that even before we've built and linked all of the related spock components together we are referencing the parameters we have defined in our spock class. Below we are passing in the ModelConfig class as a parameter model_config to the __init__ function where we can then access the parameters with . notation (if we import the ModelConfig class here and add it as a type hint to model_config most IDE auto-complete will work out of the box). We could have also passed in individual parameters instead if that is the preferred syntax. import torch.nn as nn class BasicNet ( nn . Module ): def __init__ ( self , model_config ): super ( BasicNet , self ) . __init__ () # Make a dictionary of activation functions to select from self . act_fncs = { 'relu' : nn . ReLU , 'gelu' : nn . GELU , 'tanh' : nn . Tanh } self . use_act = self . act_fncs . get ( model_config . activation )() # Define the layers manually (avoiding list comprehension for clarity) self . layer_1 = nn . Linear ( model_config . n_features , model_config . hidden_sizes [ 0 ]) self . layer_2 = nn . Linear ( model_config . hidden_sizes [ 0 ], model_config . hidden_sizes [ 1 ]) self . layer_3 = nn . Linear ( model_config . hidden_sizes [ 1 ], model_config . hidden_sizes [ 2 ]) # Define some dropout layers self . dropout_1 = nn . Dropout ( model_config . dropout [ 0 ]) self . dropout_2 = nn . Dropout ( model_config . dropout [ 1 ]) # Define the output layer self . softmax = nn . Softmax ( dim = 1 ) def forward ( self , x ): # x is the data input # Layer 1 # Linear x = self . layer_1 ( x ) # Activation x = self . use_act ( x ) # Dropout x = self . dropout_1 ( x ) # Layer 2 # Linear x = self . layer_2 ( x ) # Activation x = self . use_act ( x ) # Dropout x = self . dropout_2 ( x ) # Layer 3 # Linear x = self . layer_3 ( x ) # Softmax output = self . softmax ( x ) return output","title":"Define"},{"location":"docs/basic_tutorial/Define/#define","text":"spock manages complex configurations via a class based solution. All parameters are defined in a class or multiple classes decorated using the @spock decorator. Parameters are defined with base types or those defined within the typing module and are type checked at run time. Once built, all parameters can be found within an automatically generated namespace object that contains each class that can be accessed with the given @spock class name. All examples can be found here .","title":"Define"},{"location":"docs/basic_tutorial/Define/#supported-parameter-types","text":"","title":"Supported Parameter Types"},{"location":"docs/basic_tutorial/Define/#basic-types","text":"spock supports the following basic argument types (note List , Tuple , and Optional are defined in the typing standard library while Enum is within the enum standard library): Python Base or Typing Type (Required) Optional Type Description bool Optional[bool] Basic boolean parameter (e.g. True) float Optional[float] Basic float type parameter (e.g. 10.2) int Optional[int] Basic integer type parameter (e.g. 2) str Optional[str] Basic string type parameter (e.g. 'foo') List[type] Optional[List[type]] Basic list type parameter of base types such as int, float, etc. (e.g. [10.0, 2.0]) Tuple[type] Optional[Tuple[type]] Basic tuple type parameter of base types such as int, float, etc. Length enforced unlike List. (e.g. (10, 2)) Enum Optional[Enum] Parameter that must be from a defined set of values of base types such as int, float, etc. @spock decorated Class Optional[Class] Parameter that is a reference to another @spock decorated class Use List types when the length of the Iterable is not fixed and Tuple when length needs to be strictly enforced. Parameters that are specified without the Optional[] type will be considered REQUIRED and therefore will raise an Exception if not value is specified.","title":"Basic Types"},{"location":"docs/basic_tutorial/Define/#advanced-types","text":"spock supports more than just basic types. More information can be found in the Advanced Types section.","title":"Advanced Types"},{"location":"docs/basic_tutorial/Define/#defining-a-spock-class","text":"Let's start building out an example (a simple neural net in PyTorch) that we will continue to use within the tutorial: tutorial.py Here we import the basic units of functionality from spock . We define our class using the @spock decorator and define our parameters with supported argument types. Parameters are defined within the class by using the format parameter: type . Note that to create a parameter that is required to be within a specified set one must first define an Enum class object with the given options. The Enum class is then passed to your spock class just like other types. from enum import Enum from spock.config import spock from typing import List from typing import Tuple class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : n_features : int dropout : List [ float ] hidden_sizes : Tuple [ int , int , int ] activation : Activation","title":"Defining a spock Class"},{"location":"docs/basic_tutorial/Define/#adding-help-information","text":"spock uses the Google docstring style format to support adding help information to classes and Enums. spock will look for the first contiguous line of text within the docstring as the class help information. spock looks within the Attributes section of the docstring for help information for each parameter. Modifying the above code to include help information: from enum import Enum from spock.args import SavePath from spock.config import spock from typing import List from typing import Tuple class Activation ( Enum ): \"\"\"Options for activation functions Attributes: relu: relu activation gelu: gelu activation tanh: tanh activation \"\"\" relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : \"\"\"Main model configuration for a basic neural net Attributes: save_path: spock special keyword -- path to write out spock config state n_features: number of data features dropout: dropout rate for each layer hidden_sizes: hidden size for each layer activation: choice from the Activation enum of the activation function to use \"\"\" save_path : SavePath n_features : int dropout : List [ float ] hidden_sizes : Tuple [ int , int , int ] activation : Activation If we run our tutorial.py script with the --help flag: $ python tutorial.py --help We should see the help information we added to the docstring(s): usage: /Users/a635179/Documents/git_repos/open_source/spock/examples/tutorial/basic/tutorial.py -c [ --config ] config1 [ config2, config3, ... ] spock Basic Tutorial configuration ( s ) : ModelConfig ( Main model configuration for a basic neural net ) save_path Optional [ SavePath ] spock special keyword -- path to write out spock config state ( default: None ) n_features int number of data features dropout List [ float ] dropout rate for each layer hidden_sizes Tuple [ int, int, int ] hidden size for each layer activation Activation choice from the Activation enum of the activation function to use Activation ( Options for activation functions ) relu str relu activation gelu str gelu activation tanh str tanh activation","title":"Adding Help Information"},{"location":"docs/basic_tutorial/Define/#using-spock-parameters-writing-more-code","text":"In another file let's write our simple neural network code: basic_nn.py Notice that even before we've built and linked all of the related spock components together we are referencing the parameters we have defined in our spock class. Below we are passing in the ModelConfig class as a parameter model_config to the __init__ function where we can then access the parameters with . notation (if we import the ModelConfig class here and add it as a type hint to model_config most IDE auto-complete will work out of the box). We could have also passed in individual parameters instead if that is the preferred syntax. import torch.nn as nn class BasicNet ( nn . Module ): def __init__ ( self , model_config ): super ( BasicNet , self ) . __init__ () # Make a dictionary of activation functions to select from self . act_fncs = { 'relu' : nn . ReLU , 'gelu' : nn . GELU , 'tanh' : nn . Tanh } self . use_act = self . act_fncs . get ( model_config . activation )() # Define the layers manually (avoiding list comprehension for clarity) self . layer_1 = nn . Linear ( model_config . n_features , model_config . hidden_sizes [ 0 ]) self . layer_2 = nn . Linear ( model_config . hidden_sizes [ 0 ], model_config . hidden_sizes [ 1 ]) self . layer_3 = nn . Linear ( model_config . hidden_sizes [ 1 ], model_config . hidden_sizes [ 2 ]) # Define some dropout layers self . dropout_1 = nn . Dropout ( model_config . dropout [ 0 ]) self . dropout_2 = nn . Dropout ( model_config . dropout [ 1 ]) # Define the output layer self . softmax = nn . Softmax ( dim = 1 ) def forward ( self , x ): # x is the data input # Layer 1 # Linear x = self . layer_1 ( x ) # Activation x = self . use_act ( x ) # Dropout x = self . dropout_1 ( x ) # Layer 2 # Linear x = self . layer_2 ( x ) # Activation x = self . use_act ( x ) # Dropout x = self . dropout_2 ( x ) # Layer 3 # Linear x = self . layer_3 ( x ) # Softmax output = self . softmax ( x ) return output","title":"Using spock Parameters: Writing More Code"},{"location":"docs/basic_tutorial/Run/","text":"Run In summary, we have constructed the four basic pieces of spock . A spock class that defines our parameters (Basics) Generated the spock namespace object (Building) Referenced spock parameters in other code (Building) Created a configuration file (Configuration Files) Now we can run our basic neural network example. Running Our Code To run tutorial.py we pass the path to the configuration file as a command line argument: $ python tutorial.py --config tutorial.yaml Help To get help for parameters need to run our tutorial.py script: $ python tutorial.py --help The complete basic example can be found here .","title":"Running"},{"location":"docs/basic_tutorial/Run/#run","text":"In summary, we have constructed the four basic pieces of spock . A spock class that defines our parameters (Basics) Generated the spock namespace object (Building) Referenced spock parameters in other code (Building) Created a configuration file (Configuration Files) Now we can run our basic neural network example.","title":"Run"},{"location":"docs/basic_tutorial/Run/#running-our-code","text":"To run tutorial.py we pass the path to the configuration file as a command line argument: $ python tutorial.py --config tutorial.yaml","title":"Running Our Code"},{"location":"docs/basic_tutorial/Run/#help","text":"To get help for parameters need to run our tutorial.py script: $ python tutorial.py --help The complete basic example can be found here .","title":"Help"},{"location":"docs/basic_tutorial/Saving/","text":"Saving The current configuration of running python code can be saved to file by chaining the save() method before the generate() call to the ConfigArgBuilder class. spock supports two different ways to specify the write path and supports multiple output formats (YAML, TOML, or JSON -- via the file_extension keyword argument). Most importantly, the saved markdown file can be used as the configuration input to reproduce prior runtime configurations. What Does spock Save? By default spock will append extra information (via the extra_info kwarg) as well as the entire state of the configuration object. Extra info includes: Git Info: Branch, Commit ID (SHA-1), Commit Date, Status (e.g. dirty), Origin Python Info: Executable Path, Version, Script Entrypoint Run Information: Date, Time Env Information: Machine FQDN, Run w/ Docker, Run w/ Kubernetes Spock Version For instance, here is an example of the tutorial.py saved .toml output: # Spock Version: v2.1.5+0.gf9bf3bc.dirty # Machine FQDN: XXXXX.yyy.com # Python Executable: /Users/XXXXX/.virtualenvs/spock/bin/python # Python Version: 3.8.5 # Python Script: /XXXX/open_source/spock/examples/tutorial/basic/tutorial.py # Run Date: 2021-05-24 # Run Time: 13:33:41 # Run w/ Docker: False # Run w/ Kubernetes: False # Git Branch: master # Git Commit: f9bf3bca0098a98b994eaa2aeb257f0023704e32 # Git Date: 2021-05-10 10:33:56-04:00 # Git Status: DIRTY # Git Origin: https://github.com/fidelity/spock.git [ModelConfig] save_path = \"/tmp\" n_features = 64 dropout = [ 0.2 , 0.1 ,] hidden_sizes = [ 32 , 32 , 16 ,] activation = \"relu\" Specify spock Special Parameter Type We simply specify a SavePath in a spock config, which is a special argument type that is used to set the save path from a configuration file. Adding to: tutorial.py class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : save_path : SavePath n_features : int dropout : List [ float ] hidden_sizes : Tuple [ int , int , int ] activation : Activation And adding in the chained save() call... def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . save ( file_extension = '.toml' ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) Adding the output path to our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu Specify Path In-Line Here we simply specify the path when calling the save() method. In: tutorial.yaml def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . save ( user_specified_path = '/tmp' ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) Does the Directory Exit In either case, if the save path does not exist, it will not be created by default. To change this behavior, set create_save_path when creating the builder. In: tutorial.py def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . save ( create_save_path = True ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) Override UUID Filename By default spock uses an automatically generated UUID as the filename when saving. This can be overridden with the file_name keyword argument. The specified filename will be appended with .spock.cfg.file_extension (e.g. .yaml, .toml or. json). In: tutorial.py def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . save ( file_name = 'cool_name_here' ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig )","title":"Saving"},{"location":"docs/basic_tutorial/Saving/#saving","text":"The current configuration of running python code can be saved to file by chaining the save() method before the generate() call to the ConfigArgBuilder class. spock supports two different ways to specify the write path and supports multiple output formats (YAML, TOML, or JSON -- via the file_extension keyword argument). Most importantly, the saved markdown file can be used as the configuration input to reproduce prior runtime configurations.","title":"Saving"},{"location":"docs/basic_tutorial/Saving/#what-does-spock-save","text":"By default spock will append extra information (via the extra_info kwarg) as well as the entire state of the configuration object. Extra info includes: Git Info: Branch, Commit ID (SHA-1), Commit Date, Status (e.g. dirty), Origin Python Info: Executable Path, Version, Script Entrypoint Run Information: Date, Time Env Information: Machine FQDN, Run w/ Docker, Run w/ Kubernetes Spock Version For instance, here is an example of the tutorial.py saved .toml output: # Spock Version: v2.1.5+0.gf9bf3bc.dirty # Machine FQDN: XXXXX.yyy.com # Python Executable: /Users/XXXXX/.virtualenvs/spock/bin/python # Python Version: 3.8.5 # Python Script: /XXXX/open_source/spock/examples/tutorial/basic/tutorial.py # Run Date: 2021-05-24 # Run Time: 13:33:41 # Run w/ Docker: False # Run w/ Kubernetes: False # Git Branch: master # Git Commit: f9bf3bca0098a98b994eaa2aeb257f0023704e32 # Git Date: 2021-05-10 10:33:56-04:00 # Git Status: DIRTY # Git Origin: https://github.com/fidelity/spock.git [ModelConfig] save_path = \"/tmp\" n_features = 64 dropout = [ 0.2 , 0.1 ,] hidden_sizes = [ 32 , 32 , 16 ,] activation = \"relu\"","title":"What Does spock Save?"},{"location":"docs/basic_tutorial/Saving/#specify-spock-special-parameter-type","text":"We simply specify a SavePath in a spock config, which is a special argument type that is used to set the save path from a configuration file. Adding to: tutorial.py class Activation ( Enum ): relu = 'relu' gelu = 'gelu' tanh = 'tanh' @spock class ModelConfig : save_path : SavePath n_features : int dropout : List [ float ] hidden_sizes : Tuple [ int , int , int ] activation : Activation And adding in the chained save() call... def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . save ( file_extension = '.toml' ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) Adding the output path to our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu","title":"Specify spock Special Parameter Type"},{"location":"docs/basic_tutorial/Saving/#specify-path-in-line","text":"Here we simply specify the path when calling the save() method. In: tutorial.yaml def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . save ( user_specified_path = '/tmp' ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig )","title":"Specify Path In-Line"},{"location":"docs/basic_tutorial/Saving/#does-the-directory-exit","text":"In either case, if the save path does not exist, it will not be created by default. To change this behavior, set create_save_path when creating the builder. In: tutorial.py def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . save ( create_save_path = True ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig )","title":"Does the Directory Exit"},{"location":"docs/basic_tutorial/Saving/#override-uuid-filename","text":"By default spock uses an automatically generated UUID as the filename when saving. This can be overridden with the file_name keyword argument. The specified filename will be appended with .spock.cfg.file_extension (e.g. .yaml, .toml or. json). In: tutorial.py def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . save ( file_name = 'cool_name_here' ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig )","title":"Override UUID Filename"},{"location":"docs/legacy/Installation/","text":"Installation Requirements Python: 3.7+ Dependencies: GitPython, PyYAML, toml Tested OS: Unix (Ubuntu 16.04, Ubuntu 18.04), OSX (10.14.6) Install/Upgrade Pip/PyPi pip install spock-config Pip From Source pip install git+https://github.com/fidelity/spock Build From Source git clone https://github.com/fidelity/spock cd spock pip install setuptools wheel python setup.py bdist_wheel pip install /dist/spock-config-X.X.XxX-py3-none-any.whl","title":"Installation"},{"location":"docs/legacy/Installation/#installation","text":"","title":"Installation"},{"location":"docs/legacy/Installation/#requirements","text":"Python: 3.7+ Dependencies: GitPython, PyYAML, toml Tested OS: Unix (Ubuntu 16.04, Ubuntu 18.04), OSX (10.14.6)","title":"Requirements"},{"location":"docs/legacy/Installation/#installupgrade","text":"","title":"Install/Upgrade"},{"location":"docs/legacy/Installation/#pippypi","text":"pip install spock-config","title":"Pip/PyPi"},{"location":"docs/legacy/Installation/#pip-from-source","text":"pip install git+https://github.com/fidelity/spock","title":"Pip From Source"},{"location":"docs/legacy/Installation/#build-from-source","text":"git clone https://github.com/fidelity/spock cd spock pip install setuptools wheel python setup.py bdist_wheel pip install /dist/spock-config-X.X.XxX-py3-none-any.whl","title":"Build From Source"},{"location":"docs/legacy/Motivation/","text":"Motivation Why Spock? spock arose out of a few observations within the Artificial Intelligence Center of Excellence at Fidelity. Modern ML Models == Spaghetti Parameters During research and development of machine learning (ML) models (especially deep learning models) the total number of configuration parameters within a codebase can quickly spiral out of control: data-related parameters, model hyper-parameters, logging parameters, i/o parameters, etc. After writing parser.add_argument() for the 1000th time we figured there had to be a better way to manage the complex configurations needed for modern ML models. In addition, we found a lot of open source ML/DL models (e.g. NVIDIA OpenSeq2Seq ) that had mind-boggling spaghetti parameter definitions. Just figuring out the configurations in order to run, modify, or re-implement an open-source model/library was becoming a minefield in and of itself. Finding a Consistent Solution Looking across an fast-moving enterprise scale AI organization, we noticed a pretty fractured set of configuration management practices across the organization (and even within groups). We saw some pretty bad practices like the hard-coding of parameters within scripts, functions, modules, ... e.g.: def my_function ( args ): my_parameter = 10 # do something with my_parameter ... Not only is this just bad practice but it is also not reproducible (your parameters become dependent on branch and commit) and really isn't helpful for a collaborative codebase. Additionally, it and makes monitoring, re-training, and/or deployment a nightmare. We saw a lot of colleagues taking an easy way out e.g.: ### in config.py ### PARAMETER_1 = 10 PARAMETER_2 = 'relu' ### in function.py ### from config import * def my_function ( arg_1 = PARAMETER_1 ): # do something with arg_1 ... Almost just as bad of practice as above but at least self-contained. It still is hard to reproduce and code and and parameters are still just as intertwined. Those who tried some configuration/parameter management resorted to re-implementing the same boilerplate code e.g.: import argparse parser = argparse . ArgumentParser () parser . add_argument ( '--foo' , help = 'foo help' ) args = parser . parse_args () Better practice to bring parameters in from the command line (or even an external INI file using configparser) but the amount of boilerplate is high and it's pretty tedious to manage more than 10+ parameters. On top of all of this, most parameter definitions were mutable meaning that they can be changed within code blocks (on purpose or by accident) leading to dangerous and unexpected behavior. What Does spock Do To Resolve These Observations? Thus, the idea of of spock was born... A simple, understandable, and lightweight library to manage complex parameter configurations during Python development. Anyone familiar with Python and knows how to define a basic class object can use spock (or if they don't there is plenty of documentation and tutorials on classes). Simple Declaration: Parameters are defined within a @spock_config decorated class. Supports types, required/optional, and automatic defaults. Easily Managed Parameter Groups: Each class automatically generates its own object within a single namespace. Parameter Inheritance: Classes support inheritance allowing for complex configurations derived from a common base set of parameters. Multiple Configuration File Types: Configurations are specified from YAML, TOML, or JSON files. Hierarchical Configuration: composed from multiple configuration files via simple include statements. Immutable: All classes are frozen preventing any misuse or accidental overwrites. Tractability and Reproducibility: Save currently running parameter configuration with a single chained command. Other Libraries There are other open source complex configuration libraries available for Python. We think that spock fits within this space by providing a simple and lightweight configuration management library in comparison to other libraries. The main two that fill a similar role as spock are: gin-config Provides a lightweight configuration framework for Python, based on dependency injection gin-config does a lot of things and does them pretty well. However, we found that gin-config 's dependency injection and heavy use of decorators just didn't fit what we wanted. Dynamic injection provides a less verbose configuration solution, but can lack reproducibility depending on where and how parameters are defined. In addition, the pretty large 'kitchen sink' of different ways to manage configurations isn't simple nor lightweight enough of a solution. Hydra A framework for elegantly configuring complex applications At the time we started building spock Hydra wasn't available. Hydra is similar to spock but is more restrictive in its functionality and syntax. Similar to gin-config parameters are dynamically injected via a decorator which limits type checking, reproducibility, and traceability. In addition, Hydra doesn't support inheritance which was a big motivator for creating a new solution.","title":"Motivation"},{"location":"docs/legacy/Motivation/#motivation","text":"","title":"Motivation"},{"location":"docs/legacy/Motivation/#why-spock","text":"spock arose out of a few observations within the Artificial Intelligence Center of Excellence at Fidelity.","title":"Why Spock?"},{"location":"docs/legacy/Motivation/#modern-ml-models-spaghetti-parameters","text":"During research and development of machine learning (ML) models (especially deep learning models) the total number of configuration parameters within a codebase can quickly spiral out of control: data-related parameters, model hyper-parameters, logging parameters, i/o parameters, etc. After writing parser.add_argument() for the 1000th time we figured there had to be a better way to manage the complex configurations needed for modern ML models. In addition, we found a lot of open source ML/DL models (e.g. NVIDIA OpenSeq2Seq ) that had mind-boggling spaghetti parameter definitions. Just figuring out the configurations in order to run, modify, or re-implement an open-source model/library was becoming a minefield in and of itself.","title":"Modern ML Models == Spaghetti Parameters"},{"location":"docs/legacy/Motivation/#finding-a-consistent-solution","text":"Looking across an fast-moving enterprise scale AI organization, we noticed a pretty fractured set of configuration management practices across the organization (and even within groups). We saw some pretty bad practices like the hard-coding of parameters within scripts, functions, modules, ... e.g.: def my_function ( args ): my_parameter = 10 # do something with my_parameter ... Not only is this just bad practice but it is also not reproducible (your parameters become dependent on branch and commit) and really isn't helpful for a collaborative codebase. Additionally, it and makes monitoring, re-training, and/or deployment a nightmare. We saw a lot of colleagues taking an easy way out e.g.: ### in config.py ### PARAMETER_1 = 10 PARAMETER_2 = 'relu' ### in function.py ### from config import * def my_function ( arg_1 = PARAMETER_1 ): # do something with arg_1 ... Almost just as bad of practice as above but at least self-contained. It still is hard to reproduce and code and and parameters are still just as intertwined. Those who tried some configuration/parameter management resorted to re-implementing the same boilerplate code e.g.: import argparse parser = argparse . ArgumentParser () parser . add_argument ( '--foo' , help = 'foo help' ) args = parser . parse_args () Better practice to bring parameters in from the command line (or even an external INI file using configparser) but the amount of boilerplate is high and it's pretty tedious to manage more than 10+ parameters. On top of all of this, most parameter definitions were mutable meaning that they can be changed within code blocks (on purpose or by accident) leading to dangerous and unexpected behavior.","title":"Finding a Consistent Solution"},{"location":"docs/legacy/Motivation/#what-does-spock-do-to-resolve-these-observations","text":"Thus, the idea of of spock was born... A simple, understandable, and lightweight library to manage complex parameter configurations during Python development. Anyone familiar with Python and knows how to define a basic class object can use spock (or if they don't there is plenty of documentation and tutorials on classes). Simple Declaration: Parameters are defined within a @spock_config decorated class. Supports types, required/optional, and automatic defaults. Easily Managed Parameter Groups: Each class automatically generates its own object within a single namespace. Parameter Inheritance: Classes support inheritance allowing for complex configurations derived from a common base set of parameters. Multiple Configuration File Types: Configurations are specified from YAML, TOML, or JSON files. Hierarchical Configuration: composed from multiple configuration files via simple include statements. Immutable: All classes are frozen preventing any misuse or accidental overwrites. Tractability and Reproducibility: Save currently running parameter configuration with a single chained command.","title":"What Does spock Do To Resolve These Observations?"},{"location":"docs/legacy/Motivation/#other-libraries","text":"There are other open source complex configuration libraries available for Python. We think that spock fits within this space by providing a simple and lightweight configuration management library in comparison to other libraries. The main two that fill a similar role as spock are:","title":"Other Libraries"},{"location":"docs/legacy/Motivation/#gin-config","text":"Provides a lightweight configuration framework for Python, based on dependency injection gin-config does a lot of things and does them pretty well. However, we found that gin-config 's dependency injection and heavy use of decorators just didn't fit what we wanted. Dynamic injection provides a less verbose configuration solution, but can lack reproducibility depending on where and how parameters are defined. In addition, the pretty large 'kitchen sink' of different ways to manage configurations isn't simple nor lightweight enough of a solution.","title":"gin-config"},{"location":"docs/legacy/Motivation/#hydra","text":"A framework for elegantly configuring complex applications At the time we started building spock Hydra wasn't available. Hydra is similar to spock but is more restrictive in its functionality and syntax. Similar to gin-config parameters are dynamically injected via a decorator which limits type checking, reproducibility, and traceability. In addition, Hydra doesn't support inheritance which was a big motivator for creating a new solution.","title":"Hydra"},{"location":"docs/legacy/Quick-Start/","text":"Quick Start This is a quick and dirty guide to getting up and running with spock . Read the Basic Tutorial as a simple guide and then explore more Advanced Features for in-depth usage. All examples can be found here . TL;DR Import the necessary components from spock Create a basic Python class, decorate it with @spock_config Define your parameters in the class Use the defined parameters in your code Create a configuration file Run your code with --config /path/to/config Simple Example A basic python script, simple.py . First we import the necessary functionality from spock . We define our class using the @spock_config decorator and our parameters with supported argument types from spock.args . from spock.args import * from spock.builder import ConfigArgBuilder from spock.config import spock_config @spock_config class BasicConfig : parameter : BoolArg fancy_parameter : FloatArg fancier_parameter : FloatArg most_fancy_parameter : ListArg [ int ] Next let's add two simple function(s) to our script. They both so the same thing but use our parameters in two different ways. def add_namespace ( config ): # Lets just do some basic algebra here val_sum = sum ([( config . fancy_parameter * val ) + config . fancier_parameter for val in config . most_fancy_parameter ]) # If the boolean is true let's round if config . parameter : val_sum = round ( val_sum ) return val_sum def add_by_parameter ( multiply_param , list_vals , add_param , tf_round ): # Lets just do some basic algebra here val_sum = sum ([( multiply_param * val ) + add_param for val in list_vals ]) # If the boolean is true let's round if tf_round : val_sum = round ( val_sum ) return val_sum Now, we build out the parameter objects by passing in the spock_config objects (as *args ) to the ConfigArgBuilder and chain call the generate method. The returned namespace object contains the dataclasses named with the spock_config class name. We then can pass the whole dataclass to our first function or specific parameters to our second function. def main (): # Chain the generate function to the class call config = ConfigArgBuilder ( BasicConfig ) . generate () # One can now access the Spock config object by class name with the returned namespace print ( config . BasicConfig . parameter ) # And pass the namespace to our first function val_sum_namespace = add_namespace ( config . BasicConfig ) print ( val_sum_namespace ) # Or pass by parameter val_sum_parameter = add_by_parameter ( config . BasicConfig . fancy_parameter , config . BasicConfig . most_fancy_parameter , config . BasicConfig . fancier_parameter , config . BasicConfig . parameter ) print ( val_sum_parameter ) if __name__ == '__main__' : main () Next let's create a simple configuration file that sets the values of our parameters. Let's make a YAML file (you can also use TOML or JSON), simple.yaml : # Parameters parameter : true fancy_parameter : 8.8 fancier_parameter : 64.64 most_fancy_parameter : [ 768 , 768 , 512 , 128 ] Finally, we would then pass the path to the configuration file to the command line (-c or --config): $ python simple.py -c simple.yaml","title":"Quick Start"},{"location":"docs/legacy/Quick-Start/#quick-start","text":"This is a quick and dirty guide to getting up and running with spock . Read the Basic Tutorial as a simple guide and then explore more Advanced Features for in-depth usage. All examples can be found here .","title":"Quick Start"},{"location":"docs/legacy/Quick-Start/#tldr","text":"Import the necessary components from spock Create a basic Python class, decorate it with @spock_config Define your parameters in the class Use the defined parameters in your code Create a configuration file Run your code with --config /path/to/config","title":"TL;DR"},{"location":"docs/legacy/Quick-Start/#simple-example","text":"A basic python script, simple.py . First we import the necessary functionality from spock . We define our class using the @spock_config decorator and our parameters with supported argument types from spock.args . from spock.args import * from spock.builder import ConfigArgBuilder from spock.config import spock_config @spock_config class BasicConfig : parameter : BoolArg fancy_parameter : FloatArg fancier_parameter : FloatArg most_fancy_parameter : ListArg [ int ] Next let's add two simple function(s) to our script. They both so the same thing but use our parameters in two different ways. def add_namespace ( config ): # Lets just do some basic algebra here val_sum = sum ([( config . fancy_parameter * val ) + config . fancier_parameter for val in config . most_fancy_parameter ]) # If the boolean is true let's round if config . parameter : val_sum = round ( val_sum ) return val_sum def add_by_parameter ( multiply_param , list_vals , add_param , tf_round ): # Lets just do some basic algebra here val_sum = sum ([( multiply_param * val ) + add_param for val in list_vals ]) # If the boolean is true let's round if tf_round : val_sum = round ( val_sum ) return val_sum Now, we build out the parameter objects by passing in the spock_config objects (as *args ) to the ConfigArgBuilder and chain call the generate method. The returned namespace object contains the dataclasses named with the spock_config class name. We then can pass the whole dataclass to our first function or specific parameters to our second function. def main (): # Chain the generate function to the class call config = ConfigArgBuilder ( BasicConfig ) . generate () # One can now access the Spock config object by class name with the returned namespace print ( config . BasicConfig . parameter ) # And pass the namespace to our first function val_sum_namespace = add_namespace ( config . BasicConfig ) print ( val_sum_namespace ) # Or pass by parameter val_sum_parameter = add_by_parameter ( config . BasicConfig . fancy_parameter , config . BasicConfig . most_fancy_parameter , config . BasicConfig . fancier_parameter , config . BasicConfig . parameter ) print ( val_sum_parameter ) if __name__ == '__main__' : main () Next let's create a simple configuration file that sets the values of our parameters. Let's make a YAML file (you can also use TOML or JSON), simple.yaml : # Parameters parameter : true fancy_parameter : 8.8 fancier_parameter : 64.64 most_fancy_parameter : [ 768 , 768 , 512 , 128 ] Finally, we would then pass the path to the configuration file to the command line (-c or --config): $ python simple.py -c simple.yaml","title":"Simple Example"},{"location":"docs/legacy/advanced_features/About/","text":"Advanced Features This series of pages walks you through most of the advanced features spock provides. At the end you should understand the additional functionality that spock provides and be able to use these advanced features within your own code. The example used in the Advanced Features series of pages can be found here .","title":"Advanced Features"},{"location":"docs/legacy/advanced_features/About/#advanced-features","text":"This series of pages walks you through most of the advanced features spock provides. At the end you should understand the additional functionality that spock provides and be able to use these advanced features within your own code. The example used in the Advanced Features series of pages can be found here .","title":"Advanced Features"},{"location":"docs/legacy/advanced_features/Composition/","text":"Composing Configuration Files spock supports hierarchical composition of configuration files with a simple syntax. Composing Two YAML Files Going back to our example. Let's say we had a a portion of our configuration that does not change that often while another portion changes frequently or are parameters that are being experimented with. For instance, let's say we have finalized things related to our data set (although in our examples it is random... let's just imagine for now) but we are still experimenting with our neural network parameters. Instead of maintaining multiple copies of configuration files where parameters related to the data set are not changing, we can compose two separate configuration files together. One static file related to the data set parameters and a more dynamic file that is changing. This separation helps keep errors from propagating into the static set of data set related parameters. For instance we can break our tutorial.yaml file into two. First, let's split out the static data related parameters into: data.yaml ################ # data.yaml ################ # DataConfig batch_size : 2 n_samples : 8 DataConfig : cache_path : /home/user/cache/ And then in our second configuration file we can use the config: key to define the other configuration files we want to compose into this configuration file: changing.yaml ################ # changing.yaml ################ # Global cache_path : /tmp/cache/ config : [ /data.yaml ] # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true Warning You can add as many configuration files as you want to a config tag however be aware of circular dependencies (we do not check for these yet) and that the lower a configuration file is in the order (i.e. later in the list) that it will take precedence over the others.","title":"Composing Configuration Files"},{"location":"docs/legacy/advanced_features/Composition/#composing-configuration-files","text":"spock supports hierarchical composition of configuration files with a simple syntax.","title":"Composing Configuration Files"},{"location":"docs/legacy/advanced_features/Composition/#composing-two-yaml-files","text":"Going back to our example. Let's say we had a a portion of our configuration that does not change that often while another portion changes frequently or are parameters that are being experimented with. For instance, let's say we have finalized things related to our data set (although in our examples it is random... let's just imagine for now) but we are still experimenting with our neural network parameters. Instead of maintaining multiple copies of configuration files where parameters related to the data set are not changing, we can compose two separate configuration files together. One static file related to the data set parameters and a more dynamic file that is changing. This separation helps keep errors from propagating into the static set of data set related parameters. For instance we can break our tutorial.yaml file into two. First, let's split out the static data related parameters into: data.yaml ################ # data.yaml ################ # DataConfig batch_size : 2 n_samples : 8 DataConfig : cache_path : /home/user/cache/ And then in our second configuration file we can use the config: key to define the other configuration files we want to compose into this configuration file: changing.yaml ################ # changing.yaml ################ # Global cache_path : /tmp/cache/ config : [ /data.yaml ] # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true","title":"Composing Two YAML Files"},{"location":"docs/legacy/advanced_features/Composition/#warning","text":"You can add as many configuration files as you want to a config tag however be aware of circular dependencies (we do not check for these yet) and that the lower a configuration file is in the order (i.e. later in the list) that it will take precedence over the others.","title":"Warning"},{"location":"docs/legacy/advanced_features/Defaults/","text":"Defaults spock allows you to set defaults for parameters that are either not set from a configuration file or you no longer need to set (maybe you've finally settled on a standard or would like to fall back to defaults if the user does not know the correct/best parameter to choose). This is done in the spock class definition. Setting Defaults Say we want defaults for the hidden layer sizes and the activation function as well as add a new parameter with a default value. For basic types ( FloatArg , IntArg , StrArg , BoolArg ) default values are set with the = operator. For ListArg and TupleArg types, defaults are set using the .default() method. For ChoiceArg the default value is set using the default keyword arg. Let's modify the definition in: tutorial.py from spock.args import * from spock.config import spock_config @spock_config class ModelConfig : save_path : SavePathOptArg lr : FloatArg = 0.01 n_features : IntArg dropout : ListArg [ float ] hidden_sizes : TupleArg [ int ] = TupleArg . defaults (( 32 , 32 , 32 )) activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ], default = 'relu' ) We added a new parameter lr that has a default value of 0.01 , and set defaults for hidden_sizes and activation . These values will be used if no values are specified in the configuration file and prevent spock from raising an exception for required parameters.","title":"Defaults"},{"location":"docs/legacy/advanced_features/Defaults/#defaults","text":"spock allows you to set defaults for parameters that are either not set from a configuration file or you no longer need to set (maybe you've finally settled on a standard or would like to fall back to defaults if the user does not know the correct/best parameter to choose). This is done in the spock class definition.","title":"Defaults"},{"location":"docs/legacy/advanced_features/Defaults/#setting-defaults","text":"Say we want defaults for the hidden layer sizes and the activation function as well as add a new parameter with a default value. For basic types ( FloatArg , IntArg , StrArg , BoolArg ) default values are set with the = operator. For ListArg and TupleArg types, defaults are set using the .default() method. For ChoiceArg the default value is set using the default keyword arg. Let's modify the definition in: tutorial.py from spock.args import * from spock.config import spock_config @spock_config class ModelConfig : save_path : SavePathOptArg lr : FloatArg = 0.01 n_features : IntArg dropout : ListArg [ float ] hidden_sizes : TupleArg [ int ] = TupleArg . defaults (( 32 , 32 , 32 )) activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ], default = 'relu' ) We added a new parameter lr that has a default value of 0.01 , and set defaults for hidden_sizes and activation . These values will be used if no values are specified in the configuration file and prevent spock from raising an exception for required parameters.","title":"Setting Defaults"},{"location":"docs/legacy/advanced_features/Inheritance/","text":"Inheritance spock supports class inheritance between different defined spock classes. This allows you to build complex configurations derived from a common base class or classes. Defining an Inherited spock Class Back to our example. We have implemented two different optimizers to train our neural network. In its current state we have overlooked the fact that the two different optimizers share a set of common parameters but each also has a set of specific parameters. Instead of defining redundant parameter definitions let's use spock inheritance. We create a new spock class that inherits from another spock class. This functions just like traditional inheritance where the child will inherit the parameter definitions from the parent class. Editing our definition in: tutorial.py from spock.args import * from spock.config import spock_config @spock_config class ModelConfig : save_path : SavePathOptArg n_features : IntArg dropout : ListOptArg [ float ] hidden_sizes : TupleArg [ int ] = TupleArg . defaults (( 32 , 32 , 32 )) activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ], default = 'relu' ) optimizer = ChoiceArg ( choice_set = [ 'SGD' , 'Adam' ]) @spock_config class DataConfig : batch_size : IntArg = 2 n_samples : IntArg = 8 @spock_config class OptimizerConfig : lr : FloatArg = 0.01 n_epochs : IntArg = 2 grad_clip : FloatOptArg @spock_config class SGDConfig ( OptimizerConfig ): weight_decay : FloatArg momentum : FloatArg nesterov : BoolArg Editing our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true Using an Inherited spock Class Let's use our inherited class to use the SGD optimizer with the defined parameter on our basic neural network: tutorial.py import torch from .basic_nn import BasicNet def train ( x_data , y_data , model , model_config , data_config , optimizer_config ): if model_config . optimizer == 'SGD' : optimizer = torch . optim . SGD ( model . parameters (), lr = optimizer_config . lr , momentum = optimizer_config . momentum , nesterov = optimizer_config . nesterov ) elif model_config . optimizer == 'Adam' : optimizer = torch . optim . Adam ( model . parameters (), lr = optimizer_config . lr ) else : raise ValueError ( f 'Optimizer choice { optimizer_config . optimizer } not available' ) n_steps_per_epoch = data_config . n_samples % data_config . batch_size for epoch in range ( optimizer_config . n_epochs ): for i in range ( n_steps_per_epoch ): # Ugly data slicing for simplicity x_batch = x_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch , ] y_batch = y_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch , ] optimizer . zero_grad () output = model ( x_batch ) loss = torch . nn . CrossEntropyLoss ( output , y_batch ) loss . backward () if optimizer_config . grad_clip : torch . nn . utils . clip_grad_value ( model . parameters (), optimizer_config . grad_clip ) optimizer . step () print ( f 'Finished Epoch { epoch + 1 } ' ) def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , SGDConfig , desc = description ) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in x_data = torch . rand ( config . DataConfig . n_samples , config . ModelConfig . n_features ) y_data = torch . randint ( 0 , 3 , ( config . DataConfig . n_samples ,)) # Run some training train ( x_data , y_data , basic_nn , config . ModelConfig , config . DataConfig , config . SGDConfig )","title":"Inheritance"},{"location":"docs/legacy/advanced_features/Inheritance/#inheritance","text":"spock supports class inheritance between different defined spock classes. This allows you to build complex configurations derived from a common base class or classes.","title":"Inheritance"},{"location":"docs/legacy/advanced_features/Inheritance/#defining-an-inherited-spock-class","text":"Back to our example. We have implemented two different optimizers to train our neural network. In its current state we have overlooked the fact that the two different optimizers share a set of common parameters but each also has a set of specific parameters. Instead of defining redundant parameter definitions let's use spock inheritance. We create a new spock class that inherits from another spock class. This functions just like traditional inheritance where the child will inherit the parameter definitions from the parent class. Editing our definition in: tutorial.py from spock.args import * from spock.config import spock_config @spock_config class ModelConfig : save_path : SavePathOptArg n_features : IntArg dropout : ListOptArg [ float ] hidden_sizes : TupleArg [ int ] = TupleArg . defaults (( 32 , 32 , 32 )) activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ], default = 'relu' ) optimizer = ChoiceArg ( choice_set = [ 'SGD' , 'Adam' ]) @spock_config class DataConfig : batch_size : IntArg = 2 n_samples : IntArg = 8 @spock_config class OptimizerConfig : lr : FloatArg = 0.01 n_epochs : IntArg = 2 grad_clip : FloatOptArg @spock_config class SGDConfig ( OptimizerConfig ): weight_decay : FloatArg momentum : FloatArg nesterov : BoolArg Editing our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true","title":"Defining an Inherited spock Class"},{"location":"docs/legacy/advanced_features/Inheritance/#using-an-inherited-spock-class","text":"Let's use our inherited class to use the SGD optimizer with the defined parameter on our basic neural network: tutorial.py import torch from .basic_nn import BasicNet def train ( x_data , y_data , model , model_config , data_config , optimizer_config ): if model_config . optimizer == 'SGD' : optimizer = torch . optim . SGD ( model . parameters (), lr = optimizer_config . lr , momentum = optimizer_config . momentum , nesterov = optimizer_config . nesterov ) elif model_config . optimizer == 'Adam' : optimizer = torch . optim . Adam ( model . parameters (), lr = optimizer_config . lr ) else : raise ValueError ( f 'Optimizer choice { optimizer_config . optimizer } not available' ) n_steps_per_epoch = data_config . n_samples % data_config . batch_size for epoch in range ( optimizer_config . n_epochs ): for i in range ( n_steps_per_epoch ): # Ugly data slicing for simplicity x_batch = x_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch , ] y_batch = y_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch , ] optimizer . zero_grad () output = model ( x_batch ) loss = torch . nn . CrossEntropyLoss ( output , y_batch ) loss . backward () if optimizer_config . grad_clip : torch . nn . utils . clip_grad_value ( model . parameters (), optimizer_config . grad_clip ) optimizer . step () print ( f 'Finished Epoch { epoch + 1 } ' ) def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , SGDConfig , desc = description ) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in x_data = torch . rand ( config . DataConfig . n_samples , config . ModelConfig . n_features ) y_data = torch . randint ( 0 , 3 , ( config . DataConfig . n_samples ,)) # Run some training train ( x_data , y_data , basic_nn , config . ModelConfig , config . DataConfig , config . SGDConfig )","title":"Using an Inherited spock Class"},{"location":"docs/legacy/advanced_features/Keyword-Configs/","text":"Keyword Configs spock supports adding and/or overriding the config file path(s) normally specified via the command line argument -c with keyword arguments. Specifying The Config Keyword Argument Let's pass in the yaml configuration file via the config keyword argument instead of at the command line. Simply add the config keyword argument to the ConfigArgBuilder . Note: This is not the recommended best practice as it creates a dependency between code and configuration files. Please use the -c command line argument whenever possible. The config keyword arg should be used ONLY when necessary. Editing our definition in: tutorial.py ... def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , SGDConfig , desc = description , config = [ './tutorial.yaml' ]) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in x_data = torch . rand ( config . DataConfig . n_samples , config . ModelConfig . n_features ) y_data = torch . randint ( 0 , 3 , ( config . DataConfig . n_samples ,)) # Run some training train ( x_data , y_data , basic_nn , config . ModelConfig , config . DataConfig , config . SGDConfig ) Now to run tutorial.py we don't need to pass a command line argument: $ python tutorial.py Specifying The Config Keyword Argument & The No Command Line Flag So if the config keyword arg is not recommended why do we support it? Mainly for two reasons: - Programmatic access to configuration files for other code/infrastructure (e.g. dispatching jobs from a work queue that might need to be parametrized) - To prevent command line arg clashes with other python code/libraries that might use the same or similar syntax (e.g. FastAPI) For instance, let's say we were wrapping our simple neural net example into an async REST API (using something like FastAPI and a message queue such as redis). The FastAPI docker image has it's own set of command line arguments that get called and will clash with spock . Therefore we need to pass the configuration file(s) through the config keyword argument and deactivate the command line argument. For instance, we create a route for our basic neural network (shown below). We add the no_cmd_line=True flag to the ConfigArgBuilder to prevent spock from references command line arguments: @api . post ( \"/inference/\" , status_code = 201 ) def create_job ( * , data : schemata . Inference ): # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , SGDConfig , desc = description , config = [ './tutorial.yaml' ], no_cmd_line = True ) . generate () # Let's assume we have a model loading function based on our params basic_nn = LoadBasicNet ( model_config = config . ModelConfig ) # Make a prediction y = basic_nn ( data . x ) # Return the predictions return_schema = schemata . InferenceReturn ( y = y , ) return return_schema","title":"Keyword Configs"},{"location":"docs/legacy/advanced_features/Keyword-Configs/#keyword-configs","text":"spock supports adding and/or overriding the config file path(s) normally specified via the command line argument -c with keyword arguments.","title":"Keyword Configs"},{"location":"docs/legacy/advanced_features/Keyword-Configs/#specifying-the-config-keyword-argument","text":"Let's pass in the yaml configuration file via the config keyword argument instead of at the command line. Simply add the config keyword argument to the ConfigArgBuilder . Note: This is not the recommended best practice as it creates a dependency between code and configuration files. Please use the -c command line argument whenever possible. The config keyword arg should be used ONLY when necessary. Editing our definition in: tutorial.py ... def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , SGDConfig , desc = description , config = [ './tutorial.yaml' ]) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in x_data = torch . rand ( config . DataConfig . n_samples , config . ModelConfig . n_features ) y_data = torch . randint ( 0 , 3 , ( config . DataConfig . n_samples ,)) # Run some training train ( x_data , y_data , basic_nn , config . ModelConfig , config . DataConfig , config . SGDConfig ) Now to run tutorial.py we don't need to pass a command line argument: $ python tutorial.py","title":"Specifying The Config Keyword Argument"},{"location":"docs/legacy/advanced_features/Keyword-Configs/#specifying-the-config-keyword-argument-the-no-command-line-flag","text":"So if the config keyword arg is not recommended why do we support it? Mainly for two reasons: - Programmatic access to configuration files for other code/infrastructure (e.g. dispatching jobs from a work queue that might need to be parametrized) - To prevent command line arg clashes with other python code/libraries that might use the same or similar syntax (e.g. FastAPI) For instance, let's say we were wrapping our simple neural net example into an async REST API (using something like FastAPI and a message queue such as redis). The FastAPI docker image has it's own set of command line arguments that get called and will clash with spock . Therefore we need to pass the configuration file(s) through the config keyword argument and deactivate the command line argument. For instance, we create a route for our basic neural network (shown below). We add the no_cmd_line=True flag to the ConfigArgBuilder to prevent spock from references command line arguments: @api . post ( \"/inference/\" , status_code = 201 ) def create_job ( * , data : schemata . Inference ): # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , SGDConfig , desc = description , config = [ './tutorial.yaml' ], no_cmd_line = True ) . generate () # Let's assume we have a model loading function based on our params basic_nn = LoadBasicNet ( model_config = config . ModelConfig ) # Make a prediction y = basic_nn ( data . x ) # Return the predictions return_schema = schemata . InferenceReturn ( y = y , ) return return_schema","title":"Specifying The Config Keyword Argument &amp; The No Command Line Flag"},{"location":"docs/legacy/advanced_features/Local-Overrides/","text":"Local Overrides The class based solution within spock provides the ability to override a global parameter value within a local class context. Overriding a Global Value Let's define two new parameters with the same name but in two different classes that represent where some stuff is going to be cached. One for the model and one for some data. Editing our definition in: tutorial.py from spock.args import * from spock.config import spock_config @spock_config class ModelConfig : save_path : SavePathOptArg n_features : IntArg dropout : ListOptArg [ float ] hidden_sizes : TupleArg [ int ] = TupleArg . defaults (( 32 , 32 , 32 )) activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ], default = 'relu' ) optimizer = ChoiceArg ( choice_set = [ 'SGD' , 'Adam' ]) cache_path : StrOptArg @spock_config class DataConfig : batch_size : IntArg = 2 n_samples : IntArg = 8 cache_path : StrOptArg @spock_config class OptimizerConfig : lr : FloatArg = 0.01 n_epochs : IntArg = 2 grad_clip : FloatOptArg @spock_config class SGDConfig ( OptimizerConfig ): weight_decay : FloatArg momentum : FloatArg nesterov : BoolArg Now, if we edit our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Global cache_path : /tmp/cache/ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true This configuration file would set both parameters to use the /tmp/cache/ value (i.e. it would set the parameter value globally). But what if we want to the data cache to be a different path? We can override the global parameter value with a local parameter value. Editing our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Global cache_path : /tmp/cache/ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] n_hidden : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 DataConfig : cache_path : /home/user/cache/ # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true","title":"Local Overrides"},{"location":"docs/legacy/advanced_features/Local-Overrides/#local-overrides","text":"The class based solution within spock provides the ability to override a global parameter value within a local class context.","title":"Local Overrides"},{"location":"docs/legacy/advanced_features/Local-Overrides/#overriding-a-global-value","text":"Let's define two new parameters with the same name but in two different classes that represent where some stuff is going to be cached. One for the model and one for some data. Editing our definition in: tutorial.py from spock.args import * from spock.config import spock_config @spock_config class ModelConfig : save_path : SavePathOptArg n_features : IntArg dropout : ListOptArg [ float ] hidden_sizes : TupleArg [ int ] = TupleArg . defaults (( 32 , 32 , 32 )) activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ], default = 'relu' ) optimizer = ChoiceArg ( choice_set = [ 'SGD' , 'Adam' ]) cache_path : StrOptArg @spock_config class DataConfig : batch_size : IntArg = 2 n_samples : IntArg = 8 cache_path : StrOptArg @spock_config class OptimizerConfig : lr : FloatArg = 0.01 n_epochs : IntArg = 2 grad_clip : FloatOptArg @spock_config class SGDConfig ( OptimizerConfig ): weight_decay : FloatArg momentum : FloatArg nesterov : BoolArg Now, if we edit our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Global cache_path : /tmp/cache/ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true This configuration file would set both parameters to use the /tmp/cache/ value (i.e. it would set the parameter value globally). But what if we want to the data cache to be a different path? We can override the global parameter value with a local parameter value. Editing our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Global cache_path : /tmp/cache/ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] n_hidden : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 DataConfig : cache_path : /home/user/cache/ # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 # SGD Config weight_decay : 1E-5 momentum : 0.9 nesterov : true","title":"Overriding a Global Value"},{"location":"docs/legacy/advanced_features/Optional-Parameters/","text":"Optional Parameters spock allows for parameters to be defined as optional. This means that if the parameter value is not set from either a configuration file or a default value it will be assigned the None value. Optional spock parameters are defined using the optional version of the base type: FloatOptArg , IntOptArg , StrOptArg , ListOptArg , TupleOptArg . Defining Optional spock Parameters Optional parameters commonly occur in applications with complex behavior (like neural networks). For instance, what if you want to execute a specific behavior with some parameter(s) if the parameter is defined and if the parameter is not defined either skip the behavior or so something different. Normally this would require a combination of boolean logic and parameter definition (which might be useless...). spock remedies this with optional parameters. As an example, let's assume we want to make dropout within our basic neural network optional. Let's modify the definition in: tutorial.py @spock_config class ModelConfig : save_path : SavePathOptArg lr : FloatArg = 0.01 n_features : IntArg dropout : ListOptArg [ float ] hidden_sizes : TupleArg [ int ] = TupleArg . defaults (( 32 , 32 , 32 )) activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ], default = 'relu' ) Notice that all we did was change the type from ListArg to ListOptAg . Now let's edit our simple neural network code to reflect that dropout is now optional. We have to change the code a bit to be more modular (but still ugly for demonstration): basic_nn.py import torch.nn as nn class BasicNet ( nn . Module ): def __init__ ( self , model_config ): super ( BasicNet , self ) . __init__ () # Make a dictionary of activation functions to select from self . act_fncs = { 'relu' : nn . ReLU , 'gelu' : nn . GELU , 'tanh' : nn . Tanh } self . use_act = self . act_fncs . get ( model_config . activation )() # Define the layers manually (avoiding list comprehension for clarity) self . layer_1 = nn . Linear ( model_config . n_features , model_config . hidden_sizes [ 0 ]) self . layer_2 = nn . Linear ( model_config . hidden_sizes [ 0 ], model_config . hidden_sizes [ 1 ]) self . layer_3 = nn . Linear ( model_config . hidden_sizes [ 1 ], model_config . hidden_sizes [ 2 ]) # Define some dropout layers self . dropout = [] if model_config . dropout is not None : self . dropout = [ nn . Dropout ( val ) for val in model_config . dropout ] # Define the output layer self . softmax = nn . Softmax ( dim = 1 ) def forward ( self , x ): # x is the data input # Layer 1 # Linear x = self . layer_1 ( x ) # Activation x = self . use_act ( x ) # Dropout if len ( self . dropout ) != 0 : x = self . dropout [ 0 ]( x ) # Layer 2 # Linear x = self . layer_2 ( x ) # Activation x = self . use_act ( x ) # Dropout if len ( self . dropout ) != 0 : x = self . dropout [ 1 ]( x ) # Layer 3 # Linear x = self . layer_3 ( x ) # Softmax output = self . softmax ( x ) return output Code Behavior If we use the same configuration file defined in: tutorial.yaml ################ # tutorial.yaml ################ # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu With this configuration file, the parameter dropout is assigned the specified value. Thus our basic neural network will have dropout layers between Layer 1, Layer 2, and Layer 3. However, if we use the following configuration file: tutorial_no_dropout.yaml ################ # tutorial.yaml ################ # ModelConfig n_features : 64 hidden_sizes : [ 32 , 32 , 16 ] activation : relu With this configuration file, the parameter dropout is assigned None . Thus our based on the logic in our code our basic neural network will not have dropout between layers. This simple example demonstrates the power of spock optional parameters. Flow through code can easily be modified by simply changing the configuration file.","title":"Optional Parameters"},{"location":"docs/legacy/advanced_features/Optional-Parameters/#optional-parameters","text":"spock allows for parameters to be defined as optional. This means that if the parameter value is not set from either a configuration file or a default value it will be assigned the None value. Optional spock parameters are defined using the optional version of the base type: FloatOptArg , IntOptArg , StrOptArg , ListOptArg , TupleOptArg .","title":"Optional Parameters"},{"location":"docs/legacy/advanced_features/Optional-Parameters/#defining-optional-spock-parameters","text":"Optional parameters commonly occur in applications with complex behavior (like neural networks). For instance, what if you want to execute a specific behavior with some parameter(s) if the parameter is defined and if the parameter is not defined either skip the behavior or so something different. Normally this would require a combination of boolean logic and parameter definition (which might be useless...). spock remedies this with optional parameters. As an example, let's assume we want to make dropout within our basic neural network optional. Let's modify the definition in: tutorial.py @spock_config class ModelConfig : save_path : SavePathOptArg lr : FloatArg = 0.01 n_features : IntArg dropout : ListOptArg [ float ] hidden_sizes : TupleArg [ int ] = TupleArg . defaults (( 32 , 32 , 32 )) activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ], default = 'relu' ) Notice that all we did was change the type from ListArg to ListOptAg . Now let's edit our simple neural network code to reflect that dropout is now optional. We have to change the code a bit to be more modular (but still ugly for demonstration): basic_nn.py import torch.nn as nn class BasicNet ( nn . Module ): def __init__ ( self , model_config ): super ( BasicNet , self ) . __init__ () # Make a dictionary of activation functions to select from self . act_fncs = { 'relu' : nn . ReLU , 'gelu' : nn . GELU , 'tanh' : nn . Tanh } self . use_act = self . act_fncs . get ( model_config . activation )() # Define the layers manually (avoiding list comprehension for clarity) self . layer_1 = nn . Linear ( model_config . n_features , model_config . hidden_sizes [ 0 ]) self . layer_2 = nn . Linear ( model_config . hidden_sizes [ 0 ], model_config . hidden_sizes [ 1 ]) self . layer_3 = nn . Linear ( model_config . hidden_sizes [ 1 ], model_config . hidden_sizes [ 2 ]) # Define some dropout layers self . dropout = [] if model_config . dropout is not None : self . dropout = [ nn . Dropout ( val ) for val in model_config . dropout ] # Define the output layer self . softmax = nn . Softmax ( dim = 1 ) def forward ( self , x ): # x is the data input # Layer 1 # Linear x = self . layer_1 ( x ) # Activation x = self . use_act ( x ) # Dropout if len ( self . dropout ) != 0 : x = self . dropout [ 0 ]( x ) # Layer 2 # Linear x = self . layer_2 ( x ) # Activation x = self . use_act ( x ) # Dropout if len ( self . dropout ) != 0 : x = self . dropout [ 1 ]( x ) # Layer 3 # Linear x = self . layer_3 ( x ) # Softmax output = self . softmax ( x ) return output","title":"Defining Optional spock Parameters"},{"location":"docs/legacy/advanced_features/Optional-Parameters/#code-behavior","text":"If we use the same configuration file defined in: tutorial.yaml ################ # tutorial.yaml ################ # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu With this configuration file, the parameter dropout is assigned the specified value. Thus our basic neural network will have dropout layers between Layer 1, Layer 2, and Layer 3. However, if we use the following configuration file: tutorial_no_dropout.yaml ################ # tutorial.yaml ################ # ModelConfig n_features : 64 hidden_sizes : [ 32 , 32 , 16 ] activation : relu With this configuration file, the parameter dropout is assigned None . Thus our based on the logic in our code our basic neural network will not have dropout between layers. This simple example demonstrates the power of spock optional parameters. Flow through code can easily be modified by simply changing the configuration file.","title":"Code Behavior"},{"location":"docs/legacy/advanced_features/Parameter-Groups/","text":"Parameter Groups Since spock manages complex configurations via a class based solution we can define and decorate multiple classes with @spock_config . Each class gets created as a separate class object within the spock namespace object. Building spock Parameter Groups Let's go back to our example. Say we need to add a few more parameters to our code. We could just keep adding them to the single defined class, but this would lead to a 'mega' class that has parameters for many different parts of your code. Instead, we will define two new spock classes for our new parameters and begin to organize them by functionality. Editing our definition in: tutorial.py from spock.args import * from spock.config import spock_config @spock_config class ModelConfig : save_path : SavePathOptArg n_features : IntArg dropout : ListOptArg [ float ] hidden_sizes : TupleArg [ int ] = TupleArg . defaults (( 32 , 32 , 32 )) activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ], default = 'relu' ) optimizer = ChoiceArg ( choice_set = [ 'SGD' , 'Adam' ]) @spock_config class DataConfig : batch_size : IntArg = 2 n_samples : IntArg = 8 @spock_config class OptimizerConfig : lr : FloatArg = 0.01 n_epochs : IntArg = 2 grad_clip : FloatOptArg Now we have three separate spock classes that we need to generate the namespace object from. Simply add the new classes to *args in the ConfigArgBuilder . Editing tutorial.py : from spock.builder import ConfigArgBuilder def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , OptimizerConfig , desc = description ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) print ( config . OptimizerConfig ) Editing our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0 Adding More Code Let's add a bit more functionality to our code that uses our new parameters by running a 'basic training loop' (this is kept very simple for illustrative purposes, hence the simple data slicing) on our basic neural network: tutorial.py import torch from .basic_nn import BasicNet def train ( x_data , y_data , model , model_config , data_config , optimizer_config ): if model_config . optimizer == 'SGD' : optimizer = torch . optim . SGD ( model . parameters (), lr = optimizer_config . lr ) elif model_config . optimizer == 'Adam' : optimizer = torch . optim . Adam ( model . parameters (), lr = optimizer_config . lr ) else : raise ValueError ( f 'Optimizer choice { optimizer_config . optimizer } not available' ) n_steps_per_epoch = data_config . n_samples % data_config . batch_size for epoch in range ( optimizer_config . n_epochs ): for i in range ( n_steps_per_epoch ): # Ugly data slicing for simplicity x_batch = x_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch ,] y_batch = y_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch ,] optimizer . zero_grad () output = model ( x_batch ) loss = torch . nn . CrossEntropyLoss ( output , y_batch ) loss . backward () if optimizer_config . grad_clip : torch . nn . utils . clip_grad_value ( model . parameters (), optimizer_config . grad_clip ) optimizer . step () def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , OptimizerConfig , desc = description ) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in x_data = torch . rand ( config . DataConfig . n_samples , config . ModelConfig . n_features ) y_data = torch . randint ( 0 , 3 , ( config . DataConig . n_samples ,)) # Run some training train ( x_data , y_data , basic_nn , config . ModelConfig , config . DataConfig , config . OptimizerConfig )","title":"Parameter Groups"},{"location":"docs/legacy/advanced_features/Parameter-Groups/#parameter-groups","text":"Since spock manages complex configurations via a class based solution we can define and decorate multiple classes with @spock_config . Each class gets created as a separate class object within the spock namespace object.","title":"Parameter Groups"},{"location":"docs/legacy/advanced_features/Parameter-Groups/#building-spock-parameter-groups","text":"Let's go back to our example. Say we need to add a few more parameters to our code. We could just keep adding them to the single defined class, but this would lead to a 'mega' class that has parameters for many different parts of your code. Instead, we will define two new spock classes for our new parameters and begin to organize them by functionality. Editing our definition in: tutorial.py from spock.args import * from spock.config import spock_config @spock_config class ModelConfig : save_path : SavePathOptArg n_features : IntArg dropout : ListOptArg [ float ] hidden_sizes : TupleArg [ int ] = TupleArg . defaults (( 32 , 32 , 32 )) activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ], default = 'relu' ) optimizer = ChoiceArg ( choice_set = [ 'SGD' , 'Adam' ]) @spock_config class DataConfig : batch_size : IntArg = 2 n_samples : IntArg = 8 @spock_config class OptimizerConfig : lr : FloatArg = 0.01 n_epochs : IntArg = 2 grad_clip : FloatOptArg Now we have three separate spock classes that we need to generate the namespace object from. Simply add the new classes to *args in the ConfigArgBuilder . Editing tutorial.py : from spock.builder import ConfigArgBuilder def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , OptimizerConfig , desc = description ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) print ( config . OptimizerConfig ) Editing our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu optimizer : SGD # DataConfig batch_size : 2 n_samples : 8 # OptimizerConfig lr : 0.01 n_epochs : 2 grad_clip : 5.0","title":"Building spock Parameter Groups"},{"location":"docs/legacy/advanced_features/Parameter-Groups/#adding-more-code","text":"Let's add a bit more functionality to our code that uses our new parameters by running a 'basic training loop' (this is kept very simple for illustrative purposes, hence the simple data slicing) on our basic neural network: tutorial.py import torch from .basic_nn import BasicNet def train ( x_data , y_data , model , model_config , data_config , optimizer_config ): if model_config . optimizer == 'SGD' : optimizer = torch . optim . SGD ( model . parameters (), lr = optimizer_config . lr ) elif model_config . optimizer == 'Adam' : optimizer = torch . optim . Adam ( model . parameters (), lr = optimizer_config . lr ) else : raise ValueError ( f 'Optimizer choice { optimizer_config . optimizer } not available' ) n_steps_per_epoch = data_config . n_samples % data_config . batch_size for epoch in range ( optimizer_config . n_epochs ): for i in range ( n_steps_per_epoch ): # Ugly data slicing for simplicity x_batch = x_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch ,] y_batch = y_data [ i * n_steps_per_epoch :( i + 1 ) * n_steps_per_epoch ,] optimizer . zero_grad () output = model ( x_batch ) loss = torch . nn . CrossEntropyLoss ( output , y_batch ) loss . backward () if optimizer_config . grad_clip : torch . nn . utils . clip_grad_value ( model . parameters (), optimizer_config . grad_clip ) optimizer . step () def main (): # A simple description description = 'spock Advanced Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , DataConfig , OptimizerConfig , desc = description ) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in x_data = torch . rand ( config . DataConfig . n_samples , config . ModelConfig . n_features ) y_data = torch . randint ( 0 , 3 , ( config . DataConig . n_samples ,)) # Run some training train ( x_data , y_data , basic_nn , config . ModelConfig , config . DataConfig , config . OptimizerConfig )","title":"Adding More Code"},{"location":"docs/legacy/basic_tutorial/About/","text":"Basic Tutorial This series of pages will walk you through the basics of spock . At the end you should understand the basic functionality that spock provides and be able to use the core functional units within your own code. The example used in the Basic tutorial series of pages can be found here .","title":"Basic Tutorial"},{"location":"docs/legacy/basic_tutorial/About/#basic-tutorial","text":"This series of pages will walk you through the basics of spock . At the end you should understand the basic functionality that spock provides and be able to use the core functional units within your own code. The example used in the Basic tutorial series of pages can be found here .","title":"Basic Tutorial"},{"location":"docs/legacy/basic_tutorial/Building/","text":"Build Once all of the parameters we need have been defined in our spock class and we've written some code to use those parameters we need to generate the namespace object. The namespace object is the heart of spock and is how one accesses all of the defined parameters. The generation of the namespace should happen at the highest level of code, preferably in the main guard protected call or main function call. This allows the namespace object, the spock classes, or the individual parameters to be passed to lower level functionality. Generate the spock Namespace Object So let's continue in: tutorial.py Recall that we defined our spock class as such: @spock_config class ModelConfig : n_features : IntArg dropout : ListArg [ float ] hidden_sizes : TupleArg [ int ] activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ]) To generate the namespace object, import the ConfigArgBuilder class, pass in your @spock_config classes as *args , add an optional description, and then chain call the generate() method. Each spock class is defined in the namespace object given by the class name. from spock.builder import ConfigArgBuilder def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) if __name__ == '__main__' : main () Using spock Parameters Our simple neural network code referenced some spock defined parameters. So let's link them together correctly and test our model. We will pass the full spock class from the generated namespace object to our nn.module class. Continuing in: tutorial.py import torch from .basic_nn import BasicNet def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in test_data = torch . rand ( 10 , config . ModelConfig . n_features ) result = basic_nn ( test_data ) print ( result )","title":"Build"},{"location":"docs/legacy/basic_tutorial/Building/#build","text":"Once all of the parameters we need have been defined in our spock class and we've written some code to use those parameters we need to generate the namespace object. The namespace object is the heart of spock and is how one accesses all of the defined parameters. The generation of the namespace should happen at the highest level of code, preferably in the main guard protected call or main function call. This allows the namespace object, the spock classes, or the individual parameters to be passed to lower level functionality.","title":"Build"},{"location":"docs/legacy/basic_tutorial/Building/#generate-the-spock-namespace-object","text":"So let's continue in: tutorial.py Recall that we defined our spock class as such: @spock_config class ModelConfig : n_features : IntArg dropout : ListArg [ float ] hidden_sizes : TupleArg [ int ] activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ]) To generate the namespace object, import the ConfigArgBuilder class, pass in your @spock_config classes as *args , add an optional description, and then chain call the generate() method. Each spock class is defined in the namespace object given by the class name. from spock.builder import ConfigArgBuilder def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) if __name__ == '__main__' : main ()","title":"Generate the spock Namespace Object"},{"location":"docs/legacy/basic_tutorial/Building/#using-spock-parameters","text":"Our simple neural network code referenced some spock defined parameters. So let's link them together correctly and test our model. We will pass the full spock class from the generated namespace object to our nn.module class. Continuing in: tutorial.py import torch from .basic_nn import BasicNet def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . generate () # Instantiate our neural net using basic_nn = BasicNet ( model_config = config . ModelConfig ) # Make some random data (BxH): H has dim of features in test_data = torch . rand ( 10 , config . ModelConfig . n_features ) result = basic_nn ( test_data ) print ( result )","title":"Using spock Parameters"},{"location":"docs/legacy/basic_tutorial/Configuration-Files/","text":"Configuration Files Values in spock are set using external configuration files. Supported Configuration Formats YAML Requires file extension of .yaml . Supported using the external PyYAML library. TOML Requires file extension of .toml . Supported using the external toml library. JSON Requires file extension of .json . Supported using the built-in json module. Creating a Configuration File Recall that we defined our spock class as such: @spock_config class ModelConfig : n_features : IntArg dropout : ListArg [ float ] hidden_sizes : TupleArg [ int ] activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ]) Note that all of the types of our parameters IntArg , ListArg , TupleArg , and ChoiceArg are required types. This means that if we do not specify values for these parameters in our configuration file spock will throw an exception. Let's create our configuration file using the YAML standard: tutorial.yaml ################ # tutorial.yaml ################ # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu","title":"Configuration Files"},{"location":"docs/legacy/basic_tutorial/Configuration-Files/#configuration-files","text":"Values in spock are set using external configuration files.","title":"Configuration Files"},{"location":"docs/legacy/basic_tutorial/Configuration-Files/#supported-configuration-formats","text":"","title":"Supported Configuration Formats"},{"location":"docs/legacy/basic_tutorial/Configuration-Files/#yaml","text":"Requires file extension of .yaml . Supported using the external PyYAML library.","title":"YAML"},{"location":"docs/legacy/basic_tutorial/Configuration-Files/#toml","text":"Requires file extension of .toml . Supported using the external toml library.","title":"TOML"},{"location":"docs/legacy/basic_tutorial/Configuration-Files/#json","text":"Requires file extension of .json . Supported using the built-in json module.","title":"JSON"},{"location":"docs/legacy/basic_tutorial/Configuration-Files/#creating-a-configuration-file","text":"Recall that we defined our spock class as such: @spock_config class ModelConfig : n_features : IntArg dropout : ListArg [ float ] hidden_sizes : TupleArg [ int ] activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ]) Note that all of the types of our parameters IntArg , ListArg , TupleArg , and ChoiceArg are required types. This means that if we do not specify values for these parameters in our configuration file spock will throw an exception. Let's create our configuration file using the YAML standard: tutorial.yaml ################ # tutorial.yaml ################ # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu","title":"Creating a Configuration File"},{"location":"docs/legacy/basic_tutorial/Define/","text":"Define spock manages complex configurations via a class based solution. All parameters are defined in a class or multiple classes decorated with @spock_config . Parameters are defined with spock types that are checked at run time. Once built, all parameters can be found within an automatically generated namespace object that contains each class that can be accessed with the spock_config class name. All examples can be found here . Supported Parameter Types spock supports the following argument types: Python Base Type spock Type Optional spock Type Description bool BoolArg N/A Basic boolean parameter (e.g. True) float FloatArg FloatOptArg Basic float type parameter (e.g. 10.2) int IntArg IntOptArg Basic integer type parameter (e.g. 2) str StrArg StrOptArg Basic string type parameter (e.g. 'foo') list ListArg ListOptArg Basic list type parameter of base types such as int, float, etc. (e.g. [10.0, 2.0]) tuple TupleArg TupleOptArg Basic tuple type parameter of base types such as int, float, etc. (e.g. (10, 2)) N/A ChoiceArg N/A Parameter that must be from a defined set of values of base types such as int, float, etc. Defining A spock Class Let's start building out an example (a simple neural net in PyTorch) that we will continue to use within the tutorial: tutorial.py Here we import the basic units of functionality from spock . We define our class using the @spock_config decorator and define our parameters with supported argument types from spock.args . Parameters are defined within the class by using the format parameter: type . from spock.args import * from spock.config import spock_config @spock_config class ModelConfig : n_features : IntArg dropout : ListArg [ float ] hidden_sizes : TupleArg [ int ] activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ]) Using spock Parameters: Writing More Code In another file let's write our simple neural network code: basic_nn.py Notice that even before we've built and linked all of the related spock components together we are referencing the parameters we have defined in our spock class. Below we are passing in the ModelConfig class as a parameter model_config to the __init__ function where we can then access the parameters with . notation. We could have also passed in individual parameters instead if that is the preferred syntax. import torch.nn as nn class BasicNet ( nn . Module ): def __init__ ( self , model_config ): super ( BasicNet , self ) . __init__ () # Make a dictionary of activation functions to select from self . act_fncs = { 'relu' : nn . ReLU , 'gelu' : nn . GELU , 'tanh' : nn . Tanh } self . use_act = self . act_fncs . get ( model_config . activation )() # Define the layers manually (avoiding list comprehension for clarity) self . layer_1 = nn . Linear ( model_config . n_features , model_config . hidden_sizes [ 0 ]) self . layer_2 = nn . Linear ( model_config . hidden_sizes [ 0 ], model_config . hidden_sizes [ 1 ]) self . layer_3 = nn . Linear ( model_config . hidden_sizes [ 1 ], model_config . hidden_sizes [ 2 ]) # Define some dropout layers self . dropout_1 = nn . Dropout ( model_config . dropout [ 0 ]) self . dropout_2 = nn . Dropout ( model_config . dropout [ 1 ]) # Define the output layer self . softmax = nn . Softmax ( dim = 1 ) def forward ( self , x ): # x is the data input # Layer 1 # Linear x = self . layer_1 ( x ) # Activation x = self . use_act ( x ) # Dropout x = self . dropout_1 ( x ) # Layer 2 # Linear x = self . layer_2 ( x ) # Activation x = self . use_act ( x ) # Dropout x = self . dropout_2 ( x ) # Layer 3 # Linear x = self . layer_3 ( x ) # Softmax output = self . softmax ( x ) return output","title":"Define"},{"location":"docs/legacy/basic_tutorial/Define/#define","text":"spock manages complex configurations via a class based solution. All parameters are defined in a class or multiple classes decorated with @spock_config . Parameters are defined with spock types that are checked at run time. Once built, all parameters can be found within an automatically generated namespace object that contains each class that can be accessed with the spock_config class name. All examples can be found here .","title":"Define"},{"location":"docs/legacy/basic_tutorial/Define/#supported-parameter-types","text":"spock supports the following argument types: Python Base Type spock Type Optional spock Type Description bool BoolArg N/A Basic boolean parameter (e.g. True) float FloatArg FloatOptArg Basic float type parameter (e.g. 10.2) int IntArg IntOptArg Basic integer type parameter (e.g. 2) str StrArg StrOptArg Basic string type parameter (e.g. 'foo') list ListArg ListOptArg Basic list type parameter of base types such as int, float, etc. (e.g. [10.0, 2.0]) tuple TupleArg TupleOptArg Basic tuple type parameter of base types such as int, float, etc. (e.g. (10, 2)) N/A ChoiceArg N/A Parameter that must be from a defined set of values of base types such as int, float, etc.","title":"Supported Parameter Types"},{"location":"docs/legacy/basic_tutorial/Define/#defining-a-spock-class","text":"Let's start building out an example (a simple neural net in PyTorch) that we will continue to use within the tutorial: tutorial.py Here we import the basic units of functionality from spock . We define our class using the @spock_config decorator and define our parameters with supported argument types from spock.args . Parameters are defined within the class by using the format parameter: type . from spock.args import * from spock.config import spock_config @spock_config class ModelConfig : n_features : IntArg dropout : ListArg [ float ] hidden_sizes : TupleArg [ int ] activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ])","title":"Defining A spock Class"},{"location":"docs/legacy/basic_tutorial/Define/#using-spock-parameters-writing-more-code","text":"In another file let's write our simple neural network code: basic_nn.py Notice that even before we've built and linked all of the related spock components together we are referencing the parameters we have defined in our spock class. Below we are passing in the ModelConfig class as a parameter model_config to the __init__ function where we can then access the parameters with . notation. We could have also passed in individual parameters instead if that is the preferred syntax. import torch.nn as nn class BasicNet ( nn . Module ): def __init__ ( self , model_config ): super ( BasicNet , self ) . __init__ () # Make a dictionary of activation functions to select from self . act_fncs = { 'relu' : nn . ReLU , 'gelu' : nn . GELU , 'tanh' : nn . Tanh } self . use_act = self . act_fncs . get ( model_config . activation )() # Define the layers manually (avoiding list comprehension for clarity) self . layer_1 = nn . Linear ( model_config . n_features , model_config . hidden_sizes [ 0 ]) self . layer_2 = nn . Linear ( model_config . hidden_sizes [ 0 ], model_config . hidden_sizes [ 1 ]) self . layer_3 = nn . Linear ( model_config . hidden_sizes [ 1 ], model_config . hidden_sizes [ 2 ]) # Define some dropout layers self . dropout_1 = nn . Dropout ( model_config . dropout [ 0 ]) self . dropout_2 = nn . Dropout ( model_config . dropout [ 1 ]) # Define the output layer self . softmax = nn . Softmax ( dim = 1 ) def forward ( self , x ): # x is the data input # Layer 1 # Linear x = self . layer_1 ( x ) # Activation x = self . use_act ( x ) # Dropout x = self . dropout_1 ( x ) # Layer 2 # Linear x = self . layer_2 ( x ) # Activation x = self . use_act ( x ) # Dropout x = self . dropout_2 ( x ) # Layer 3 # Linear x = self . layer_3 ( x ) # Softmax output = self . softmax ( x ) return output","title":"Using spock Parameters: Writing More Code"},{"location":"docs/legacy/basic_tutorial/Run/","text":"Run In summary, we have constructed the four basic pieces of spock . A spock class that defines our parameters (Basics) Generated the spock namespace object (Building) Referenced spock parameters in other code (Building) Created a configuration file (Configuration Files) Now we can run our basic neural network example. Running Our Code To run tutorial.py we pass the path to the configuration file as a command line argument: $ python tutorial.py --config tutorial.yaml The complete basic example can be found here .","title":"Run"},{"location":"docs/legacy/basic_tutorial/Run/#run","text":"In summary, we have constructed the four basic pieces of spock . A spock class that defines our parameters (Basics) Generated the spock namespace object (Building) Referenced spock parameters in other code (Building) Created a configuration file (Configuration Files) Now we can run our basic neural network example.","title":"Run"},{"location":"docs/legacy/basic_tutorial/Run/#running-our-code","text":"To run tutorial.py we pass the path to the configuration file as a command line argument: $ python tutorial.py --config tutorial.yaml The complete basic example can be found here .","title":"Running Our Code"},{"location":"docs/legacy/basic_tutorial/Saving/","text":"Saving The current configuration of running python code can be saved to file by chaining the save() method before the generate() call to the ConfigArgBuilder class. spock supports two ways to specify the path to write and the output file can be either YAML, TOML, or JSON (via the file_extension keyword argument). Specify spock Special Parameter Type We simply specify a SavePathOptArg in a spock config, which is a special argument type that is used to set the save path from a configuration file. Adding to: tutorial.py @spock_config class ModelConfig : save_path : SavePathOptArg n_features : IntArg dropout : ListArg [ float ] hidden_sizes : TupleArg [ int ] activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ]) And adding in the chained save() call... def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . save ( file_extension = '.toml' ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) Adding the output path to our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu Specify Path In-Line Here we simply specify the path when calling the save() method. In: tutorial.yaml def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . save ( user_specified_path = '/tmp' ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) Does the Directory Exit In either case, if the save path does not exist, it will not be created by default. To change this behavior, set create_save_path when creating the builder. In: tutorial.py def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description , create_save_path = True ) . save () . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig )","title":"Saving"},{"location":"docs/legacy/basic_tutorial/Saving/#saving","text":"The current configuration of running python code can be saved to file by chaining the save() method before the generate() call to the ConfigArgBuilder class. spock supports two ways to specify the path to write and the output file can be either YAML, TOML, or JSON (via the file_extension keyword argument).","title":"Saving"},{"location":"docs/legacy/basic_tutorial/Saving/#specify-spock-special-parameter-type","text":"We simply specify a SavePathOptArg in a spock config, which is a special argument type that is used to set the save path from a configuration file. Adding to: tutorial.py @spock_config class ModelConfig : save_path : SavePathOptArg n_features : IntArg dropout : ListArg [ float ] hidden_sizes : TupleArg [ int ] activation : ChoiceArg ( choice_set = [ 'relu' , 'gelu' , 'tanh' ]) And adding in the chained save() call... def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . save ( file_extension = '.toml' ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig ) Adding the output path to our configuration file: tutorial.yaml ################ # tutorial.yaml ################ # Special Key save_path : /tmp # ModelConfig n_features : 64 dropout : [ 0.2 , 0.1 ] hidden_sizes : [ 32 , 32 , 16 ] activation : relu","title":"Specify spock Special Parameter Type"},{"location":"docs/legacy/basic_tutorial/Saving/#specify-path-in-line","text":"Here we simply specify the path when calling the save() method. In: tutorial.yaml def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description ) . save ( user_specified_path = '/tmp' ) . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig )","title":"Specify Path In-Line"},{"location":"docs/legacy/basic_tutorial/Saving/#does-the-directory-exit","text":"In either case, if the save path does not exist, it will not be created by default. To change this behavior, set create_save_path when creating the builder. In: tutorial.py def main (): # A simple description description = 'spock Tutorial' # Build out the parser by passing in Spock config objects as *args after description config = ConfigArgBuilder ( ModelConfig , desc = description , create_save_path = True ) . save () . generate () # One can now access the Spock config object by class name with the returned namespace # For instance... print ( config . ModelConfig )","title":"Does the Directory Exit"},{"location":"reference/spock/","text":"Module spock Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\" Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md \"\"\" from spock._version import get_versions __all__ = [\"args\", \"builder\", \"config\"] __version__ = get_versions()[\"version\"] del get_versions Sub-modules spock.addons spock.args spock.backend spock.builder spock.config spock.handlers spock.utils","title":"Index"},{"location":"reference/spock/#module-spock","text":"Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\" Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md \"\"\" from spock._version import get_versions __all__ = [\"args\", \"builder\", \"config\"] __version__ = get_versions()[\"version\"] del get_versions","title":"Module spock"},{"location":"reference/spock/#sub-modules","text":"spock.addons spock.args spock.backend spock.builder spock.config spock.handlers spock.utils","title":"Sub-modules"},{"location":"reference/spock/args/","text":"Module spock.args Handles import aliases to allow backwards compat with backends None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles import aliases to allow backwards compat with backends\"\"\" # from spock.backend.dataclass.args import * from spock.backend.typed import SavePath","title":"Args"},{"location":"reference/spock/args/#module-spockargs","text":"Handles import aliases to allow backwards compat with backends None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles import aliases to allow backwards compat with backends\"\"\" # from spock.backend.dataclass.args import * from spock.backend.typed import SavePath","title":"Module spock.args"},{"location":"reference/spock/builder/","text":"Module spock.builder Handles the building/saving of the configurations from the Spock config classes None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the building/saving of the configurations from the Spock config classes\"\"\" import argparse import sys import typing from pathlib import Path from uuid import uuid4 import attr from spock.backend.builder import AttrBuilder from spock.backend.payload import AttrPayload from spock.backend.saver import AttrSaver from spock.backend.wrappers import Spockspace from spock.utils import check_payload_overwrite, deep_payload_update class ConfigArgBuilder: \"\"\"Automatically generates dataclass instances from config file(s) This class builds out necessary arguments from *args classes, reads the arguments from specified config file(s), and subsequently (via chained call to generate) generates each class instance based on the necessary field values for each backend class instance *Attributes*: _args: all command line args _arg_namespace: generated argument namespace _builder_obj: instance of a BaseBuilder class _dict_args: dictionary args from the command line _payload_obj: instance of a BasePayload class _saver_obj: instance of a BaseSaver class _tune_payload_obj: payload for tuner related objects -- instance of TunerPayload class _tune_obj: instance of TunerBuilder class _tuner_interface: interface that handles the underlying library for sampling -- instance of TunerInterface _tuner_state: current state of the hyper-parameter sampler _tune_namespace: namespace that hold the generated tuner related parameters _sample_count: current call to the sample function _fixed_uuid: fixed uuid to write the best file to the same path \"\"\" def __init__( self, *args, configs: typing.Optional[typing.List] = None, desc: str = \"\", no_cmd_line: bool = False, s3_config=None, **kwargs, ): \"\"\"Init call for ConfigArgBuilder *Args*: *args: tuple of spock decorated classes to process configs: list of config paths desc: description for help no_cmd_line: turn off cmd line args s3_config: s3Config object for S3 support **kwargs: keyword args \"\"\" # Do some verification first self._verify_attr(args) self._configs = configs self._no_cmd_line = no_cmd_line self._desc = desc # Build the payload and saver objects self._payload_obj = AttrPayload(s3_config=s3_config) self._saver_obj = AttrSaver(s3_config=s3_config) # Split the fixed parameters from the tuneable ones (if present) fixed_args, tune_args = self._strip_tune_parameters(args) # The fixed parameter builder self._builder_obj = AttrBuilder(*fixed_args, **kwargs) # The possible tunable parameter builder -- might return None self._tune_obj, self._tune_payload_obj = self._handle_tuner_objects( tune_args, s3_config, kwargs ) self._tuner_interface = None self._tuner_state = None self._tuner_status = None self._sample_count = 0 self._fixed_uuid = str(uuid4()) try: # Get all cmd line args and build overrides self._args = self._handle_cmd_line() # Get the actual payload from the config files -- fixed configs self._dict_args = self._get_payload( payload_obj=self._payload_obj, input_classes=self._builder_obj.input_classes, ignore_args=tune_args, ) # Build the Spockspace from the payload and the classes # Fixed configs self._arg_namespace = self._builder_obj.generate(self._dict_args) # Get the payload from the config files -- hyper-parameters -- only if the obj is not None if self._tune_obj is not None: self._tune_args = self._get_payload( payload_obj=self._tune_payload_obj, input_classes=self._tune_obj.input_classes, ignore_args=fixed_args, ) # Build the Spockspace from the payload and the classes # Tuneable parameters self._tune_namespace = self._tune_obj.generate(self._tune_args) except Exception as e: self._print_usage_and_exit(str(e), sys_exit=False) raise ValueError(e) def __call__(self, *args, **kwargs): \"\"\"Call to self to allow chaining *Args*: *args: non-keyword args **kwargs: keyword args *Returns*: ConfigArgBuilder: self instance \"\"\" return ConfigArgBuilder(*args, **kwargs) def generate(self): \"\"\"Generate method that returns the actual argument namespace *Returns*: argument namespace consisting of all config classes \"\"\" return self._arg_namespace @property def tuner_status(self): \"\"\"Returns a dictionary of all the necessary underlying tuner internals to report the result\"\"\" return self._tuner_status @property def best(self): \"\"\"Returns a Spockspace of the best hyper-parameter config and the associated metric value\"\"\" return self._tuner_interface.best def sample(self): \"\"\"Sample method that constructs a namespace from the fixed parameters and samples from the tuner space to generate a Spockspace derived from both *Returns*: argument namespace(s) -- fixed + drawn sample from tuner backend \"\"\" if self._tune_obj is None: raise ValueError( f\"Called sample method without passing any @spockTuner decorated classes\" ) if self._tuner_interface is None: raise ValueError( f\"Called sample method without first calling the tuner method that initializes the \" f\"backend library\" ) return_tuple = self._tuner_state self._tuner_status = self._tuner_interface.tuner_status self._tuner_state = self._tuner_interface.sample() self._sample_count += 1 return return_tuple def tuner(self, tuner_config): \"\"\"Chained call that builds the tuner interface for either optuna or ax depending upon the type of the tuner_obj *Args*: tuner_config: a class of type optuna.study.Study or AX**** *Returns*: self so that functions can be chained \"\"\" if self._tune_obj is None: raise ValueError( f\"Called tuner method without passing any @spockTuner decorated classes\" ) try: from spock.addons.tune.tuner import TunerInterface self._tuner_interface = TunerInterface( tuner_config=tuner_config, tuner_namespace=self._tune_namespace, fixed_namespace=self._arg_namespace, ) self._tuner_state = self._tuner_interface.sample() except ImportError: print( \"Missing libraries to support tune functionality. Please re-install with the extra tune \" \"dependencies -- pip install spock-config[tune]\" ) return self def _print_usage_and_exit(self, msg=None, sys_exit=True, exit_code=1): \"\"\"Prints the help message and exits *Args*: msg: message to print pre exit *Returns*: None \"\"\" print(f\"usage: {sys.argv[0]} -c [--config] config1 [config2, config3, ...]\") print(f'\\n{self._desc if self._desc != \"\" else \"\"}\\n') print(\"configuration(s):\\n\") # Call the fixed parameter help info self._builder_obj.handle_help_info() if self._tune_obj is not None: self._tune_obj.handle_help_info() if msg is not None: print(msg) if sys_exit: sys.exit(exit_code) @staticmethod def _handle_tuner_objects(tune_args, s3_config, kwargs): \"\"\"Handles creating the tuner builder object if @spockTuner classes were passed in *Args*: tune_args: list of tuner classes s3_config: s3Config object for S3 support kwargs: optional keyword args *Returns*: tuner builder object or None \"\"\" if len(tune_args) > 0: try: from spock.addons.tune.builder import TunerBuilder from spock.addons.tune.payload import TunerPayload tuner_builder = TunerBuilder(*tune_args, **kwargs) tuner_payload = TunerPayload(s3_config=s3_config) return tuner_builder, tuner_payload except ImportError: print( \"Missing libraries to support tune functionality. Please re-install with the extra tune \" \"dependencies -- pip install spock-config[tune]\" ) else: return None, None @staticmethod def _verify_attr(args: typing.Tuple): \"\"\"Verifies that all the input classes are attr based *Args*: args: tuple of classes passed to the builder *Returns*: None \"\"\" # Gather if all attr backend type_attrs = all([attr.has(arg) for arg in args]) if not type_attrs: which_idx = [attr.has(arg) for arg in args].index(False) if hasattr(args[which_idx], \"__name__\"): raise TypeError( f\"*args must be of all attrs backend -- missing a @spock decorator on class \" f\"{args[which_idx].__name__}\" ) else: raise TypeError( f\"*args must be of all attrs backend -- invalid type \" f\"{type(args[which_idx])}\" ) @staticmethod def _strip_tune_parameters(args: typing.Tuple): \"\"\"Separates the fixed arguments from any hyper-parameter arguments *Args*: args: tuple of classes passed to the builder *Returns*: fixed_args: list of fixed args tune_args: list of args destined for a tuner backend \"\"\" fixed_args = [] tune_args = [] for arg in args: if arg.__module__ == \"spock.backend.config\": fixed_args.append(arg) elif arg.__module__ == \"spock.addons.tune.config\": tune_args.append(arg) return fixed_args, tune_args def _handle_cmd_line(self): \"\"\"Handle all cmd line related tasks Config paths can enter from either the command line or be added in the class init call as a kwarg (configs=[]) -- also trigger the building of the cmd line overrides for each fixed and tunable objects *Returns*: args: namespace of args \"\"\" # Need to hold an overarching parser here that just gets appended to for both fixed and tunable objects # Check if the no_cmd_line is not flagged and if the configs are not empty if self._no_cmd_line and (self._configs is None): raise ValueError( \"Flag set for preventing command line read but no paths were passed to the config kwarg\" ) # If cmd_line is flagged then build the parsers if not make any empty Namespace args = ( self._build_override_parsers(desc=self._desc) if not self._no_cmd_line else argparse.Namespace(config=[], help=False) ) # If configs are present from the init call then roll these into the namespace if self._configs is not None: args = self._get_from_kwargs(args, self._configs) return args def _build_override_parsers(self, desc): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers -- handles calling both the fixed and tunable objects *Args*: desc: argparser description *Returns*: args: argument namespace \"\"\" # Highest level parser object parser = argparse.ArgumentParser(description=desc, add_help=False) parser.add_argument(\"-c\", \"--config\", required=False, nargs=\"+\", default=[]) parser.add_argument(\"-h\", \"--help\", action=\"store_true\") # Handle the builder obj parser = self._builder_obj.build_override_parsers(parser=parser) if self._tune_obj is not None: parser = self._tune_obj.build_override_parsers(parser=parser) args = parser.parse_args() return args @staticmethod def _get_from_kwargs(args, configs): \"\"\"Get configs from the configs kwarg *Args*: args: argument namespace configs: config kwarg *Returns*: args: arg namespace \"\"\" if isinstance(configs, list): args.config.extend(configs) else: raise TypeError( f\"configs kwarg must be of type list -- given {type(configs)}\" ) return args def _get_payload(self, payload_obj, input_classes, ignore_args: typing.List): \"\"\"Get the parameter payload from the config file(s) Calls the various ways to get configs and then parses to retrieve the parameter payload - make sure to call deep update so as to not lose some parameters when only partially updating the payload *Args*: payload_obj: current payload object to call input_classes: classes to use to get payload ignore_args: args that were decorated for hyper-parameter tuning *Returns*: payload: dictionary of parameter values \"\"\" if self._args.help: # Call sys exit with a clean code as this is the help call which is not unexpected behavior self._print_usage_and_exit(sys_exit=True, exit_code=0) payload = {} dependencies = {\"paths\": [], \"rel_paths\": [], \"roots\": []} if payload_obj is not None: # Make sure we are actually trying to map to input classes if len(input_classes) > 0: # If configs are present then iterate through them and deal with the payload if len(self._args.config) > 0: for configs in self._args.config: payload_update = payload_obj.payload( input_classes, ignore_args, configs, self._args, dependencies, ) check_payload_overwrite(payload, payload_update, configs) deep_payload_update(payload, payload_update) # If there are no configs present we have to fall back only on cmd line args to fill out the necessary # data -- this is essentially using spock as a drop in replacement of arg-parser else: payload_update = payload_obj.payload( input_classes, ignore_args, None, self._args, dependencies ) check_payload_overwrite(payload, payload_update, None) deep_payload_update(payload, payload_update) return payload def _save( self, payload, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", tuner_payload=None, fixed_uuid=None, ): \"\"\"Private interface -- saves the current config setup to file with a UUID *Args*: payload: Spockspace to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite *Returns*: self so that functions can be chained \"\"\" if user_specified_path is not None: save_path = Path(user_specified_path) elif self._builder_obj.save_path is not None: save_path = Path(self._builder_obj.save_path) else: raise ValueError( \"Save did not receive a valid path from: (1) markup file(s) or (2) \" \"the keyword arg user_specified_path\" ) # Call the saver class and save function self._saver_obj.save( payload, save_path, file_name, create_save_path, extra_info, file_extension, tuner_payload, fixed_uuid, ) return self def save( self, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", add_tuner_sample: bool = False, ): \"\"\"Saves the current config setup to file with a UUID *Args*: file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) append_tuner_state: save the current tuner sample to the payload *Returns*: self so that functions can be chained \"\"\" if add_tuner_sample: if self._tune_obj is None: raise ValueError( f\"Called save method with add_tuner_sample as {add_tuner_sample} without passing any @spockTuner \" f\"decorated classes -- please use the add_tuner_sample flag for saving only hyper-parameter tuning \" f\"runs\" ) file_name = ( f\"hp.sample.{self._sample_count+1}\" if file_name is None else f\"{file_name}.hp.sample.{self._sample_count+1}\" ) self._save( self._tuner_state, file_name, user_specified_path, create_save_path, extra_info, file_extension, ) else: self._save( self._arg_namespace, file_name, user_specified_path, create_save_path, extra_info, file_extension, tuner_payload=self._tune_namespace if self._tune_obj is not None else None, ) return self def save_best( self, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", ): \"\"\"Saves the current best config setup to file *Args*: file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) *Returns*: self so that functions can be chained \"\"\" if self._tune_obj is None: raise ValueError( f\"Called save_best method without passing any @spockTuner decorated classes -- please use the save()\" f\" method for saving non hyper-parameter tuning runs\" ) file_name = f\"hp.best\" if file_name is None else f\"{file_name}.hp.best\" self._save( Spockspace(**vars(self._arg_namespace), **vars(self.best[0])), file_name, user_specified_path, create_save_path, extra_info, file_extension, fixed_uuid=self._fixed_uuid, ) return self @property def config_2_dict(self): \"\"\"Dictionary representation of the arg payload\"\"\" return self._saver_obj.dict_payload(self._arg_namespace) Classes ConfigArgBuilder class ConfigArgBuilder ( * args , configs : Union [ List , NoneType ] = None , desc : str = '' , no_cmd_line : bool = False , s3_config = None , ** kwargs ) ??? example \"View Source\" class ConfigArgBuilder: \"\"\"Automatically generates dataclass instances from config file(s) This class builds out necessary arguments from *args classes, reads the arguments from specified config file(s), and subsequently (via chained call to generate) generates each class instance based on the necessary field values for each backend class instance *Attributes*: _args: all command line args _arg_namespace: generated argument namespace _builder_obj: instance of a BaseBuilder class _dict_args: dictionary args from the command line _payload_obj: instance of a BasePayload class _saver_obj: instance of a BaseSaver class _tune_payload_obj: payload for tuner related objects -- instance of TunerPayload class _tune_obj: instance of TunerBuilder class _tuner_interface: interface that handles the underlying library for sampling -- instance of TunerInterface _tuner_state: current state of the hyper-parameter sampler _tune_namespace: namespace that hold the generated tuner related parameters _sample_count: current call to the sample function _fixed_uuid: fixed uuid to write the best file to the same path \"\"\" def __init__( self, *args, configs: typing.Optional[typing.List] = None, desc: str = \"\", no_cmd_line: bool = False, s3_config=None, **kwargs, ): \"\"\"Init call for ConfigArgBuilder *Args*: *args: tuple of spock decorated classes to process configs: list of config paths desc: description for help no_cmd_line: turn off cmd line args s3_config: s3Config object for S3 support **kwargs: keyword args \"\"\" # Do some verification first self._verify_attr(args) self._configs = configs self._no_cmd_line = no_cmd_line self._desc = desc # Build the payload and saver objects self._payload_obj = AttrPayload(s3_config=s3_config) self._saver_obj = AttrSaver(s3_config=s3_config) # Split the fixed parameters from the tuneable ones (if present) fixed_args, tune_args = self._strip_tune_parameters(args) # The fixed parameter builder self._builder_obj = AttrBuilder(*fixed_args, **kwargs) # The possible tunable parameter builder -- might return None self._tune_obj, self._tune_payload_obj = self._handle_tuner_objects( tune_args, s3_config, kwargs ) self._tuner_interface = None self._tuner_state = None self._tuner_status = None self._sample_count = 0 self._fixed_uuid = str(uuid4()) try: # Get all cmd line args and build overrides self._args = self._handle_cmd_line() # Get the actual payload from the config files -- fixed configs self._dict_args = self._get_payload( payload_obj=self._payload_obj, input_classes=self._builder_obj.input_classes, ignore_args=tune_args, ) # Build the Spockspace from the payload and the classes # Fixed configs self._arg_namespace = self._builder_obj.generate(self._dict_args) # Get the payload from the config files -- hyper-parameters -- only if the obj is not None if self._tune_obj is not None: self._tune_args = self._get_payload( payload_obj=self._tune_payload_obj, input_classes=self._tune_obj.input_classes, ignore_args=fixed_args, ) # Build the Spockspace from the payload and the classes # Tuneable parameters self._tune_namespace = self._tune_obj.generate(self._tune_args) except Exception as e: self._print_usage_and_exit(str(e), sys_exit=False) raise ValueError(e) def __call__(self, *args, **kwargs): \"\"\"Call to self to allow chaining *Args*: *args: non-keyword args **kwargs: keyword args *Returns*: ConfigArgBuilder: self instance \"\"\" return ConfigArgBuilder(*args, **kwargs) def generate(self): \"\"\"Generate method that returns the actual argument namespace *Returns*: argument namespace consisting of all config classes \"\"\" return self._arg_namespace @property def tuner_status(self): \"\"\"Returns a dictionary of all the necessary underlying tuner internals to report the result\"\"\" return self._tuner_status @property def best(self): \"\"\"Returns a Spockspace of the best hyper-parameter config and the associated metric value\"\"\" return self._tuner_interface.best def sample(self): \"\"\"Sample method that constructs a namespace from the fixed parameters and samples from the tuner space to generate a Spockspace derived from both *Returns*: argument namespace(s) -- fixed + drawn sample from tuner backend \"\"\" if self._tune_obj is None: raise ValueError( f\"Called sample method without passing any @spockTuner decorated classes\" ) if self._tuner_interface is None: raise ValueError( f\"Called sample method without first calling the tuner method that initializes the \" f\"backend library\" ) return_tuple = self._tuner_state self._tuner_status = self._tuner_interface.tuner_status self._tuner_state = self._tuner_interface.sample() self._sample_count += 1 return return_tuple def tuner(self, tuner_config): \"\"\"Chained call that builds the tuner interface for either optuna or ax depending upon the type of the tuner_obj *Args*: tuner_config: a class of type optuna.study.Study or AX**** *Returns*: self so that functions can be chained \"\"\" if self._tune_obj is None: raise ValueError( f\"Called tuner method without passing any @spockTuner decorated classes\" ) try: from spock.addons.tune.tuner import TunerInterface self._tuner_interface = TunerInterface( tuner_config=tuner_config, tuner_namespace=self._tune_namespace, fixed_namespace=self._arg_namespace, ) self._tuner_state = self._tuner_interface.sample() except ImportError: print( \"Missing libraries to support tune functionality. Please re-install with the extra tune \" \"dependencies -- pip install spock-config[tune]\" ) return self def _print_usage_and_exit(self, msg=None, sys_exit=True, exit_code=1): \"\"\"Prints the help message and exits *Args*: msg: message to print pre exit *Returns*: None \"\"\" print(f\"usage: {sys.argv[0]} -c [--config] config1 [config2, config3, ...]\") print(f'\\n{self._desc if self._desc != \"\" else \"\"}\\n') print(\"configuration(s):\\n\") # Call the fixed parameter help info self._builder_obj.handle_help_info() if self._tune_obj is not None: self._tune_obj.handle_help_info() if msg is not None: print(msg) if sys_exit: sys.exit(exit_code) @staticmethod def _handle_tuner_objects(tune_args, s3_config, kwargs): \"\"\"Handles creating the tuner builder object if @spockTuner classes were passed in *Args*: tune_args: list of tuner classes s3_config: s3Config object for S3 support kwargs: optional keyword args *Returns*: tuner builder object or None \"\"\" if len(tune_args) > 0: try: from spock.addons.tune.builder import TunerBuilder from spock.addons.tune.payload import TunerPayload tuner_builder = TunerBuilder(*tune_args, **kwargs) tuner_payload = TunerPayload(s3_config=s3_config) return tuner_builder, tuner_payload except ImportError: print( \"Missing libraries to support tune functionality. Please re-install with the extra tune \" \"dependencies -- pip install spock-config[tune]\" ) else: return None, None @staticmethod def _verify_attr(args: typing.Tuple): \"\"\"Verifies that all the input classes are attr based *Args*: args: tuple of classes passed to the builder *Returns*: None \"\"\" # Gather if all attr backend type_attrs = all([attr.has(arg) for arg in args]) if not type_attrs: which_idx = [attr.has(arg) for arg in args].index(False) if hasattr(args[which_idx], \"__name__\"): raise TypeError( f\"*args must be of all attrs backend -- missing a @spock decorator on class \" f\"{args[which_idx].__name__}\" ) else: raise TypeError( f\"*args must be of all attrs backend -- invalid type \" f\"{type(args[which_idx])}\" ) @staticmethod def _strip_tune_parameters(args: typing.Tuple): \"\"\"Separates the fixed arguments from any hyper-parameter arguments *Args*: args: tuple of classes passed to the builder *Returns*: fixed_args: list of fixed args tune_args: list of args destined for a tuner backend \"\"\" fixed_args = [] tune_args = [] for arg in args: if arg.__module__ == \"spock.backend.config\": fixed_args.append(arg) elif arg.__module__ == \"spock.addons.tune.config\": tune_args.append(arg) return fixed_args, tune_args def _handle_cmd_line(self): \"\"\"Handle all cmd line related tasks Config paths can enter from either the command line or be added in the class init call as a kwarg (configs=[]) -- also trigger the building of the cmd line overrides for each fixed and tunable objects *Returns*: args: namespace of args \"\"\" # Need to hold an overarching parser here that just gets appended to for both fixed and tunable objects # Check if the no_cmd_line is not flagged and if the configs are not empty if self._no_cmd_line and (self._configs is None): raise ValueError( \"Flag set for preventing command line read but no paths were passed to the config kwarg\" ) # If cmd_line is flagged then build the parsers if not make any empty Namespace args = ( self._build_override_parsers(desc=self._desc) if not self._no_cmd_line else argparse.Namespace(config=[], help=False) ) # If configs are present from the init call then roll these into the namespace if self._configs is not None: args = self._get_from_kwargs(args, self._configs) return args def _build_override_parsers(self, desc): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers -- handles calling both the fixed and tunable objects *Args*: desc: argparser description *Returns*: args: argument namespace \"\"\" # Highest level parser object parser = argparse.ArgumentParser(description=desc, add_help=False) parser.add_argument(\"-c\", \"--config\", required=False, nargs=\"+\", default=[]) parser.add_argument(\"-h\", \"--help\", action=\"store_true\") # Handle the builder obj parser = self._builder_obj.build_override_parsers(parser=parser) if self._tune_obj is not None: parser = self._tune_obj.build_override_parsers(parser=parser) args = parser.parse_args() return args @staticmethod def _get_from_kwargs(args, configs): \"\"\"Get configs from the configs kwarg *Args*: args: argument namespace configs: config kwarg *Returns*: args: arg namespace \"\"\" if isinstance(configs, list): args.config.extend(configs) else: raise TypeError( f\"configs kwarg must be of type list -- given {type(configs)}\" ) return args def _get_payload(self, payload_obj, input_classes, ignore_args: typing.List): \"\"\"Get the parameter payload from the config file(s) Calls the various ways to get configs and then parses to retrieve the parameter payload - make sure to call deep update so as to not lose some parameters when only partially updating the payload *Args*: payload_obj: current payload object to call input_classes: classes to use to get payload ignore_args: args that were decorated for hyper-parameter tuning *Returns*: payload: dictionary of parameter values \"\"\" if self._args.help: # Call sys exit with a clean code as this is the help call which is not unexpected behavior self._print_usage_and_exit(sys_exit=True, exit_code=0) payload = {} dependencies = {\"paths\": [], \"rel_paths\": [], \"roots\": []} if payload_obj is not None: # Make sure we are actually trying to map to input classes if len(input_classes) > 0: # If configs are present then iterate through them and deal with the payload if len(self._args.config) > 0: for configs in self._args.config: payload_update = payload_obj.payload( input_classes, ignore_args, configs, self._args, dependencies, ) check_payload_overwrite(payload, payload_update, configs) deep_payload_update(payload, payload_update) # If there are no configs present we have to fall back only on cmd line args to fill out the necessary # data -- this is essentially using spock as a drop in replacement of arg-parser else: payload_update = payload_obj.payload( input_classes, ignore_args, None, self._args, dependencies ) check_payload_overwrite(payload, payload_update, None) deep_payload_update(payload, payload_update) return payload def _save( self, payload, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", tuner_payload=None, fixed_uuid=None, ): \"\"\"Private interface -- saves the current config setup to file with a UUID *Args*: payload: Spockspace to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite *Returns*: self so that functions can be chained \"\"\" if user_specified_path is not None: save_path = Path(user_specified_path) elif self._builder_obj.save_path is not None: save_path = Path(self._builder_obj.save_path) else: raise ValueError( \"Save did not receive a valid path from: (1) markup file(s) or (2) \" \"the keyword arg user_specified_path\" ) # Call the saver class and save function self._saver_obj.save( payload, save_path, file_name, create_save_path, extra_info, file_extension, tuner_payload, fixed_uuid, ) return self def save( self, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", add_tuner_sample: bool = False, ): \"\"\"Saves the current config setup to file with a UUID *Args*: file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) append_tuner_state: save the current tuner sample to the payload *Returns*: self so that functions can be chained \"\"\" if add_tuner_sample: if self._tune_obj is None: raise ValueError( f\"Called save method with add_tuner_sample as {add_tuner_sample} without passing any @spockTuner \" f\"decorated classes -- please use the add_tuner_sample flag for saving only hyper-parameter tuning \" f\"runs\" ) file_name = ( f\"hp.sample.{self._sample_count+1}\" if file_name is None else f\"{file_name}.hp.sample.{self._sample_count+1}\" ) self._save( self._tuner_state, file_name, user_specified_path, create_save_path, extra_info, file_extension, ) else: self._save( self._arg_namespace, file_name, user_specified_path, create_save_path, extra_info, file_extension, tuner_payload=self._tune_namespace if self._tune_obj is not None else None, ) return self def save_best( self, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", ): \"\"\"Saves the current best config setup to file *Args*: file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) *Returns*: self so that functions can be chained \"\"\" if self._tune_obj is None: raise ValueError( f\"Called save_best method without passing any @spockTuner decorated classes -- please use the save()\" f\" method for saving non hyper-parameter tuning runs\" ) file_name = f\"hp.best\" if file_name is None else f\"{file_name}.hp.best\" self._save( Spockspace(**vars(self._arg_namespace), **vars(self.best[0])), file_name, user_specified_path, create_save_path, extra_info, file_extension, fixed_uuid=self._fixed_uuid, ) return self @property def config_2_dict(self): \"\"\"Dictionary representation of the arg payload\"\"\" return self._saver_obj.dict_payload(self._arg_namespace) Instance variables best Returns a Spockspace of the best hyper-parameter config and the associated metric value config_2_dict Dictionary representation of the arg payload tuner_status Returns a dictionary of all the necessary underlying tuner internals to report the result Methods generate def generate ( self ) Generate method that returns the actual argument namespace Returns : argument namespace consisting of all config classes ??? example \"View Source\" def generate(self): \"\"\"Generate method that returns the actual argument namespace *Returns*: argument namespace consisting of all config classes \"\"\" return self._arg_namespace sample def sample ( self ) Sample method that constructs a namespace from the fixed parameters and samples from the tuner space to generate a Spockspace derived from both Returns : argument namespace(s) -- fixed + drawn sample from tuner backend ??? example \"View Source\" def sample(self): \"\"\"Sample method that constructs a namespace from the fixed parameters and samples from the tuner space to generate a Spockspace derived from both *Returns*: argument namespace(s) -- fixed + drawn sample from tuner backend \"\"\" if self._tune_obj is None: raise ValueError( f\"Called sample method without passing any @spockTuner decorated classes\" ) if self._tuner_interface is None: raise ValueError( f\"Called sample method without first calling the tuner method that initializes the \" f\"backend library\" ) return_tuple = self._tuner_state self._tuner_status = self._tuner_interface.tuner_status self._tuner_state = self._tuner_interface.sample() self._sample_count += 1 return return_tuple save def save ( self , file_name : str = None , user_specified_path : str = None , create_save_path : bool = True , extra_info : bool = True , file_extension : str = '.yaml' , add_tuner_sample : bool = False ) Saves the current config setup to file with a UUID Args : file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) append_tuner_state: save the current tuner sample to the payload Returns : self so that functions can be chained ??? example \"View Source\" def save( self, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", add_tuner_sample: bool = False, ): \"\"\"Saves the current config setup to file with a UUID *Args*: file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) append_tuner_state: save the current tuner sample to the payload *Returns*: self so that functions can be chained \"\"\" if add_tuner_sample: if self._tune_obj is None: raise ValueError( f\"Called save method with add_tuner_sample as {add_tuner_sample} without passing any @spockTuner \" f\"decorated classes -- please use the add_tuner_sample flag for saving only hyper-parameter tuning \" f\"runs\" ) file_name = ( f\"hp.sample.{self._sample_count+1}\" if file_name is None else f\"{file_name}.hp.sample.{self._sample_count+1}\" ) self._save( self._tuner_state, file_name, user_specified_path, create_save_path, extra_info, file_extension, ) else: self._save( self._arg_namespace, file_name, user_specified_path, create_save_path, extra_info, file_extension, tuner_payload=self._tune_namespace if self._tune_obj is not None else None, ) return self save_best def save_best ( self , file_name : str = None , user_specified_path : str = None , create_save_path : bool = True , extra_info : bool = True , file_extension : str = '.yaml' ) Saves the current best config setup to file Args : file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) Returns : self so that functions can be chained ??? example \"View Source\" def save_best( self, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", ): \"\"\"Saves the current best config setup to file *Args*: file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) *Returns*: self so that functions can be chained \"\"\" if self._tune_obj is None: raise ValueError( f\"Called save_best method without passing any @spockTuner decorated classes -- please use the save()\" f\" method for saving non hyper-parameter tuning runs\" ) file_name = f\"hp.best\" if file_name is None else f\"{file_name}.hp.best\" self._save( Spockspace(**vars(self._arg_namespace), **vars(self.best[0])), file_name, user_specified_path, create_save_path, extra_info, file_extension, fixed_uuid=self._fixed_uuid, ) return self tuner def tuner ( self , tuner_config ) Chained call that builds the tuner interface for either optuna or ax depending upon the type of the tuner_obj Args : tuner_config: a class of type optuna.study.Study or AX**** Returns : self so that functions can be chained ??? example \"View Source\" def tuner(self, tuner_config): \"\"\"Chained call that builds the tuner interface for either optuna or ax depending upon the type of the tuner_obj *Args*: tuner_config: a class of type optuna.study.Study or AX**** *Returns*: self so that functions can be chained \"\"\" if self._tune_obj is None: raise ValueError( f\"Called tuner method without passing any @spockTuner decorated classes\" ) try: from spock.addons.tune.tuner import TunerInterface self._tuner_interface = TunerInterface( tuner_config=tuner_config, tuner_namespace=self._tune_namespace, fixed_namespace=self._arg_namespace, ) self._tuner_state = self._tuner_interface.sample() except ImportError: print( \"Missing libraries to support tune functionality. Please re-install with the extra tune \" \"dependencies -- pip install spock-config[tune]\" ) return self","title":"Builder"},{"location":"reference/spock/builder/#module-spockbuilder","text":"Handles the building/saving of the configurations from the Spock config classes None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the building/saving of the configurations from the Spock config classes\"\"\" import argparse import sys import typing from pathlib import Path from uuid import uuid4 import attr from spock.backend.builder import AttrBuilder from spock.backend.payload import AttrPayload from spock.backend.saver import AttrSaver from spock.backend.wrappers import Spockspace from spock.utils import check_payload_overwrite, deep_payload_update class ConfigArgBuilder: \"\"\"Automatically generates dataclass instances from config file(s) This class builds out necessary arguments from *args classes, reads the arguments from specified config file(s), and subsequently (via chained call to generate) generates each class instance based on the necessary field values for each backend class instance *Attributes*: _args: all command line args _arg_namespace: generated argument namespace _builder_obj: instance of a BaseBuilder class _dict_args: dictionary args from the command line _payload_obj: instance of a BasePayload class _saver_obj: instance of a BaseSaver class _tune_payload_obj: payload for tuner related objects -- instance of TunerPayload class _tune_obj: instance of TunerBuilder class _tuner_interface: interface that handles the underlying library for sampling -- instance of TunerInterface _tuner_state: current state of the hyper-parameter sampler _tune_namespace: namespace that hold the generated tuner related parameters _sample_count: current call to the sample function _fixed_uuid: fixed uuid to write the best file to the same path \"\"\" def __init__( self, *args, configs: typing.Optional[typing.List] = None, desc: str = \"\", no_cmd_line: bool = False, s3_config=None, **kwargs, ): \"\"\"Init call for ConfigArgBuilder *Args*: *args: tuple of spock decorated classes to process configs: list of config paths desc: description for help no_cmd_line: turn off cmd line args s3_config: s3Config object for S3 support **kwargs: keyword args \"\"\" # Do some verification first self._verify_attr(args) self._configs = configs self._no_cmd_line = no_cmd_line self._desc = desc # Build the payload and saver objects self._payload_obj = AttrPayload(s3_config=s3_config) self._saver_obj = AttrSaver(s3_config=s3_config) # Split the fixed parameters from the tuneable ones (if present) fixed_args, tune_args = self._strip_tune_parameters(args) # The fixed parameter builder self._builder_obj = AttrBuilder(*fixed_args, **kwargs) # The possible tunable parameter builder -- might return None self._tune_obj, self._tune_payload_obj = self._handle_tuner_objects( tune_args, s3_config, kwargs ) self._tuner_interface = None self._tuner_state = None self._tuner_status = None self._sample_count = 0 self._fixed_uuid = str(uuid4()) try: # Get all cmd line args and build overrides self._args = self._handle_cmd_line() # Get the actual payload from the config files -- fixed configs self._dict_args = self._get_payload( payload_obj=self._payload_obj, input_classes=self._builder_obj.input_classes, ignore_args=tune_args, ) # Build the Spockspace from the payload and the classes # Fixed configs self._arg_namespace = self._builder_obj.generate(self._dict_args) # Get the payload from the config files -- hyper-parameters -- only if the obj is not None if self._tune_obj is not None: self._tune_args = self._get_payload( payload_obj=self._tune_payload_obj, input_classes=self._tune_obj.input_classes, ignore_args=fixed_args, ) # Build the Spockspace from the payload and the classes # Tuneable parameters self._tune_namespace = self._tune_obj.generate(self._tune_args) except Exception as e: self._print_usage_and_exit(str(e), sys_exit=False) raise ValueError(e) def __call__(self, *args, **kwargs): \"\"\"Call to self to allow chaining *Args*: *args: non-keyword args **kwargs: keyword args *Returns*: ConfigArgBuilder: self instance \"\"\" return ConfigArgBuilder(*args, **kwargs) def generate(self): \"\"\"Generate method that returns the actual argument namespace *Returns*: argument namespace consisting of all config classes \"\"\" return self._arg_namespace @property def tuner_status(self): \"\"\"Returns a dictionary of all the necessary underlying tuner internals to report the result\"\"\" return self._tuner_status @property def best(self): \"\"\"Returns a Spockspace of the best hyper-parameter config and the associated metric value\"\"\" return self._tuner_interface.best def sample(self): \"\"\"Sample method that constructs a namespace from the fixed parameters and samples from the tuner space to generate a Spockspace derived from both *Returns*: argument namespace(s) -- fixed + drawn sample from tuner backend \"\"\" if self._tune_obj is None: raise ValueError( f\"Called sample method without passing any @spockTuner decorated classes\" ) if self._tuner_interface is None: raise ValueError( f\"Called sample method without first calling the tuner method that initializes the \" f\"backend library\" ) return_tuple = self._tuner_state self._tuner_status = self._tuner_interface.tuner_status self._tuner_state = self._tuner_interface.sample() self._sample_count += 1 return return_tuple def tuner(self, tuner_config): \"\"\"Chained call that builds the tuner interface for either optuna or ax depending upon the type of the tuner_obj *Args*: tuner_config: a class of type optuna.study.Study or AX**** *Returns*: self so that functions can be chained \"\"\" if self._tune_obj is None: raise ValueError( f\"Called tuner method without passing any @spockTuner decorated classes\" ) try: from spock.addons.tune.tuner import TunerInterface self._tuner_interface = TunerInterface( tuner_config=tuner_config, tuner_namespace=self._tune_namespace, fixed_namespace=self._arg_namespace, ) self._tuner_state = self._tuner_interface.sample() except ImportError: print( \"Missing libraries to support tune functionality. Please re-install with the extra tune \" \"dependencies -- pip install spock-config[tune]\" ) return self def _print_usage_and_exit(self, msg=None, sys_exit=True, exit_code=1): \"\"\"Prints the help message and exits *Args*: msg: message to print pre exit *Returns*: None \"\"\" print(f\"usage: {sys.argv[0]} -c [--config] config1 [config2, config3, ...]\") print(f'\\n{self._desc if self._desc != \"\" else \"\"}\\n') print(\"configuration(s):\\n\") # Call the fixed parameter help info self._builder_obj.handle_help_info() if self._tune_obj is not None: self._tune_obj.handle_help_info() if msg is not None: print(msg) if sys_exit: sys.exit(exit_code) @staticmethod def _handle_tuner_objects(tune_args, s3_config, kwargs): \"\"\"Handles creating the tuner builder object if @spockTuner classes were passed in *Args*: tune_args: list of tuner classes s3_config: s3Config object for S3 support kwargs: optional keyword args *Returns*: tuner builder object or None \"\"\" if len(tune_args) > 0: try: from spock.addons.tune.builder import TunerBuilder from spock.addons.tune.payload import TunerPayload tuner_builder = TunerBuilder(*tune_args, **kwargs) tuner_payload = TunerPayload(s3_config=s3_config) return tuner_builder, tuner_payload except ImportError: print( \"Missing libraries to support tune functionality. Please re-install with the extra tune \" \"dependencies -- pip install spock-config[tune]\" ) else: return None, None @staticmethod def _verify_attr(args: typing.Tuple): \"\"\"Verifies that all the input classes are attr based *Args*: args: tuple of classes passed to the builder *Returns*: None \"\"\" # Gather if all attr backend type_attrs = all([attr.has(arg) for arg in args]) if not type_attrs: which_idx = [attr.has(arg) for arg in args].index(False) if hasattr(args[which_idx], \"__name__\"): raise TypeError( f\"*args must be of all attrs backend -- missing a @spock decorator on class \" f\"{args[which_idx].__name__}\" ) else: raise TypeError( f\"*args must be of all attrs backend -- invalid type \" f\"{type(args[which_idx])}\" ) @staticmethod def _strip_tune_parameters(args: typing.Tuple): \"\"\"Separates the fixed arguments from any hyper-parameter arguments *Args*: args: tuple of classes passed to the builder *Returns*: fixed_args: list of fixed args tune_args: list of args destined for a tuner backend \"\"\" fixed_args = [] tune_args = [] for arg in args: if arg.__module__ == \"spock.backend.config\": fixed_args.append(arg) elif arg.__module__ == \"spock.addons.tune.config\": tune_args.append(arg) return fixed_args, tune_args def _handle_cmd_line(self): \"\"\"Handle all cmd line related tasks Config paths can enter from either the command line or be added in the class init call as a kwarg (configs=[]) -- also trigger the building of the cmd line overrides for each fixed and tunable objects *Returns*: args: namespace of args \"\"\" # Need to hold an overarching parser here that just gets appended to for both fixed and tunable objects # Check if the no_cmd_line is not flagged and if the configs are not empty if self._no_cmd_line and (self._configs is None): raise ValueError( \"Flag set for preventing command line read but no paths were passed to the config kwarg\" ) # If cmd_line is flagged then build the parsers if not make any empty Namespace args = ( self._build_override_parsers(desc=self._desc) if not self._no_cmd_line else argparse.Namespace(config=[], help=False) ) # If configs are present from the init call then roll these into the namespace if self._configs is not None: args = self._get_from_kwargs(args, self._configs) return args def _build_override_parsers(self, desc): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers -- handles calling both the fixed and tunable objects *Args*: desc: argparser description *Returns*: args: argument namespace \"\"\" # Highest level parser object parser = argparse.ArgumentParser(description=desc, add_help=False) parser.add_argument(\"-c\", \"--config\", required=False, nargs=\"+\", default=[]) parser.add_argument(\"-h\", \"--help\", action=\"store_true\") # Handle the builder obj parser = self._builder_obj.build_override_parsers(parser=parser) if self._tune_obj is not None: parser = self._tune_obj.build_override_parsers(parser=parser) args = parser.parse_args() return args @staticmethod def _get_from_kwargs(args, configs): \"\"\"Get configs from the configs kwarg *Args*: args: argument namespace configs: config kwarg *Returns*: args: arg namespace \"\"\" if isinstance(configs, list): args.config.extend(configs) else: raise TypeError( f\"configs kwarg must be of type list -- given {type(configs)}\" ) return args def _get_payload(self, payload_obj, input_classes, ignore_args: typing.List): \"\"\"Get the parameter payload from the config file(s) Calls the various ways to get configs and then parses to retrieve the parameter payload - make sure to call deep update so as to not lose some parameters when only partially updating the payload *Args*: payload_obj: current payload object to call input_classes: classes to use to get payload ignore_args: args that were decorated for hyper-parameter tuning *Returns*: payload: dictionary of parameter values \"\"\" if self._args.help: # Call sys exit with a clean code as this is the help call which is not unexpected behavior self._print_usage_and_exit(sys_exit=True, exit_code=0) payload = {} dependencies = {\"paths\": [], \"rel_paths\": [], \"roots\": []} if payload_obj is not None: # Make sure we are actually trying to map to input classes if len(input_classes) > 0: # If configs are present then iterate through them and deal with the payload if len(self._args.config) > 0: for configs in self._args.config: payload_update = payload_obj.payload( input_classes, ignore_args, configs, self._args, dependencies, ) check_payload_overwrite(payload, payload_update, configs) deep_payload_update(payload, payload_update) # If there are no configs present we have to fall back only on cmd line args to fill out the necessary # data -- this is essentially using spock as a drop in replacement of arg-parser else: payload_update = payload_obj.payload( input_classes, ignore_args, None, self._args, dependencies ) check_payload_overwrite(payload, payload_update, None) deep_payload_update(payload, payload_update) return payload def _save( self, payload, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", tuner_payload=None, fixed_uuid=None, ): \"\"\"Private interface -- saves the current config setup to file with a UUID *Args*: payload: Spockspace to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite *Returns*: self so that functions can be chained \"\"\" if user_specified_path is not None: save_path = Path(user_specified_path) elif self._builder_obj.save_path is not None: save_path = Path(self._builder_obj.save_path) else: raise ValueError( \"Save did not receive a valid path from: (1) markup file(s) or (2) \" \"the keyword arg user_specified_path\" ) # Call the saver class and save function self._saver_obj.save( payload, save_path, file_name, create_save_path, extra_info, file_extension, tuner_payload, fixed_uuid, ) return self def save( self, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", add_tuner_sample: bool = False, ): \"\"\"Saves the current config setup to file with a UUID *Args*: file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) append_tuner_state: save the current tuner sample to the payload *Returns*: self so that functions can be chained \"\"\" if add_tuner_sample: if self._tune_obj is None: raise ValueError( f\"Called save method with add_tuner_sample as {add_tuner_sample} without passing any @spockTuner \" f\"decorated classes -- please use the add_tuner_sample flag for saving only hyper-parameter tuning \" f\"runs\" ) file_name = ( f\"hp.sample.{self._sample_count+1}\" if file_name is None else f\"{file_name}.hp.sample.{self._sample_count+1}\" ) self._save( self._tuner_state, file_name, user_specified_path, create_save_path, extra_info, file_extension, ) else: self._save( self._arg_namespace, file_name, user_specified_path, create_save_path, extra_info, file_extension, tuner_payload=self._tune_namespace if self._tune_obj is not None else None, ) return self def save_best( self, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", ): \"\"\"Saves the current best config setup to file *Args*: file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) *Returns*: self so that functions can be chained \"\"\" if self._tune_obj is None: raise ValueError( f\"Called save_best method without passing any @spockTuner decorated classes -- please use the save()\" f\" method for saving non hyper-parameter tuning runs\" ) file_name = f\"hp.best\" if file_name is None else f\"{file_name}.hp.best\" self._save( Spockspace(**vars(self._arg_namespace), **vars(self.best[0])), file_name, user_specified_path, create_save_path, extra_info, file_extension, fixed_uuid=self._fixed_uuid, ) return self @property def config_2_dict(self): \"\"\"Dictionary representation of the arg payload\"\"\" return self._saver_obj.dict_payload(self._arg_namespace)","title":"Module spock.builder"},{"location":"reference/spock/builder/#classes","text":"","title":"Classes"},{"location":"reference/spock/builder/#configargbuilder","text":"class ConfigArgBuilder ( * args , configs : Union [ List , NoneType ] = None , desc : str = '' , no_cmd_line : bool = False , s3_config = None , ** kwargs ) ??? example \"View Source\" class ConfigArgBuilder: \"\"\"Automatically generates dataclass instances from config file(s) This class builds out necessary arguments from *args classes, reads the arguments from specified config file(s), and subsequently (via chained call to generate) generates each class instance based on the necessary field values for each backend class instance *Attributes*: _args: all command line args _arg_namespace: generated argument namespace _builder_obj: instance of a BaseBuilder class _dict_args: dictionary args from the command line _payload_obj: instance of a BasePayload class _saver_obj: instance of a BaseSaver class _tune_payload_obj: payload for tuner related objects -- instance of TunerPayload class _tune_obj: instance of TunerBuilder class _tuner_interface: interface that handles the underlying library for sampling -- instance of TunerInterface _tuner_state: current state of the hyper-parameter sampler _tune_namespace: namespace that hold the generated tuner related parameters _sample_count: current call to the sample function _fixed_uuid: fixed uuid to write the best file to the same path \"\"\" def __init__( self, *args, configs: typing.Optional[typing.List] = None, desc: str = \"\", no_cmd_line: bool = False, s3_config=None, **kwargs, ): \"\"\"Init call for ConfigArgBuilder *Args*: *args: tuple of spock decorated classes to process configs: list of config paths desc: description for help no_cmd_line: turn off cmd line args s3_config: s3Config object for S3 support **kwargs: keyword args \"\"\" # Do some verification first self._verify_attr(args) self._configs = configs self._no_cmd_line = no_cmd_line self._desc = desc # Build the payload and saver objects self._payload_obj = AttrPayload(s3_config=s3_config) self._saver_obj = AttrSaver(s3_config=s3_config) # Split the fixed parameters from the tuneable ones (if present) fixed_args, tune_args = self._strip_tune_parameters(args) # The fixed parameter builder self._builder_obj = AttrBuilder(*fixed_args, **kwargs) # The possible tunable parameter builder -- might return None self._tune_obj, self._tune_payload_obj = self._handle_tuner_objects( tune_args, s3_config, kwargs ) self._tuner_interface = None self._tuner_state = None self._tuner_status = None self._sample_count = 0 self._fixed_uuid = str(uuid4()) try: # Get all cmd line args and build overrides self._args = self._handle_cmd_line() # Get the actual payload from the config files -- fixed configs self._dict_args = self._get_payload( payload_obj=self._payload_obj, input_classes=self._builder_obj.input_classes, ignore_args=tune_args, ) # Build the Spockspace from the payload and the classes # Fixed configs self._arg_namespace = self._builder_obj.generate(self._dict_args) # Get the payload from the config files -- hyper-parameters -- only if the obj is not None if self._tune_obj is not None: self._tune_args = self._get_payload( payload_obj=self._tune_payload_obj, input_classes=self._tune_obj.input_classes, ignore_args=fixed_args, ) # Build the Spockspace from the payload and the classes # Tuneable parameters self._tune_namespace = self._tune_obj.generate(self._tune_args) except Exception as e: self._print_usage_and_exit(str(e), sys_exit=False) raise ValueError(e) def __call__(self, *args, **kwargs): \"\"\"Call to self to allow chaining *Args*: *args: non-keyword args **kwargs: keyword args *Returns*: ConfigArgBuilder: self instance \"\"\" return ConfigArgBuilder(*args, **kwargs) def generate(self): \"\"\"Generate method that returns the actual argument namespace *Returns*: argument namespace consisting of all config classes \"\"\" return self._arg_namespace @property def tuner_status(self): \"\"\"Returns a dictionary of all the necessary underlying tuner internals to report the result\"\"\" return self._tuner_status @property def best(self): \"\"\"Returns a Spockspace of the best hyper-parameter config and the associated metric value\"\"\" return self._tuner_interface.best def sample(self): \"\"\"Sample method that constructs a namespace from the fixed parameters and samples from the tuner space to generate a Spockspace derived from both *Returns*: argument namespace(s) -- fixed + drawn sample from tuner backend \"\"\" if self._tune_obj is None: raise ValueError( f\"Called sample method without passing any @spockTuner decorated classes\" ) if self._tuner_interface is None: raise ValueError( f\"Called sample method without first calling the tuner method that initializes the \" f\"backend library\" ) return_tuple = self._tuner_state self._tuner_status = self._tuner_interface.tuner_status self._tuner_state = self._tuner_interface.sample() self._sample_count += 1 return return_tuple def tuner(self, tuner_config): \"\"\"Chained call that builds the tuner interface for either optuna or ax depending upon the type of the tuner_obj *Args*: tuner_config: a class of type optuna.study.Study or AX**** *Returns*: self so that functions can be chained \"\"\" if self._tune_obj is None: raise ValueError( f\"Called tuner method without passing any @spockTuner decorated classes\" ) try: from spock.addons.tune.tuner import TunerInterface self._tuner_interface = TunerInterface( tuner_config=tuner_config, tuner_namespace=self._tune_namespace, fixed_namespace=self._arg_namespace, ) self._tuner_state = self._tuner_interface.sample() except ImportError: print( \"Missing libraries to support tune functionality. Please re-install with the extra tune \" \"dependencies -- pip install spock-config[tune]\" ) return self def _print_usage_and_exit(self, msg=None, sys_exit=True, exit_code=1): \"\"\"Prints the help message and exits *Args*: msg: message to print pre exit *Returns*: None \"\"\" print(f\"usage: {sys.argv[0]} -c [--config] config1 [config2, config3, ...]\") print(f'\\n{self._desc if self._desc != \"\" else \"\"}\\n') print(\"configuration(s):\\n\") # Call the fixed parameter help info self._builder_obj.handle_help_info() if self._tune_obj is not None: self._tune_obj.handle_help_info() if msg is not None: print(msg) if sys_exit: sys.exit(exit_code) @staticmethod def _handle_tuner_objects(tune_args, s3_config, kwargs): \"\"\"Handles creating the tuner builder object if @spockTuner classes were passed in *Args*: tune_args: list of tuner classes s3_config: s3Config object for S3 support kwargs: optional keyword args *Returns*: tuner builder object or None \"\"\" if len(tune_args) > 0: try: from spock.addons.tune.builder import TunerBuilder from spock.addons.tune.payload import TunerPayload tuner_builder = TunerBuilder(*tune_args, **kwargs) tuner_payload = TunerPayload(s3_config=s3_config) return tuner_builder, tuner_payload except ImportError: print( \"Missing libraries to support tune functionality. Please re-install with the extra tune \" \"dependencies -- pip install spock-config[tune]\" ) else: return None, None @staticmethod def _verify_attr(args: typing.Tuple): \"\"\"Verifies that all the input classes are attr based *Args*: args: tuple of classes passed to the builder *Returns*: None \"\"\" # Gather if all attr backend type_attrs = all([attr.has(arg) for arg in args]) if not type_attrs: which_idx = [attr.has(arg) for arg in args].index(False) if hasattr(args[which_idx], \"__name__\"): raise TypeError( f\"*args must be of all attrs backend -- missing a @spock decorator on class \" f\"{args[which_idx].__name__}\" ) else: raise TypeError( f\"*args must be of all attrs backend -- invalid type \" f\"{type(args[which_idx])}\" ) @staticmethod def _strip_tune_parameters(args: typing.Tuple): \"\"\"Separates the fixed arguments from any hyper-parameter arguments *Args*: args: tuple of classes passed to the builder *Returns*: fixed_args: list of fixed args tune_args: list of args destined for a tuner backend \"\"\" fixed_args = [] tune_args = [] for arg in args: if arg.__module__ == \"spock.backend.config\": fixed_args.append(arg) elif arg.__module__ == \"spock.addons.tune.config\": tune_args.append(arg) return fixed_args, tune_args def _handle_cmd_line(self): \"\"\"Handle all cmd line related tasks Config paths can enter from either the command line or be added in the class init call as a kwarg (configs=[]) -- also trigger the building of the cmd line overrides for each fixed and tunable objects *Returns*: args: namespace of args \"\"\" # Need to hold an overarching parser here that just gets appended to for both fixed and tunable objects # Check if the no_cmd_line is not flagged and if the configs are not empty if self._no_cmd_line and (self._configs is None): raise ValueError( \"Flag set for preventing command line read but no paths were passed to the config kwarg\" ) # If cmd_line is flagged then build the parsers if not make any empty Namespace args = ( self._build_override_parsers(desc=self._desc) if not self._no_cmd_line else argparse.Namespace(config=[], help=False) ) # If configs are present from the init call then roll these into the namespace if self._configs is not None: args = self._get_from_kwargs(args, self._configs) return args def _build_override_parsers(self, desc): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers -- handles calling both the fixed and tunable objects *Args*: desc: argparser description *Returns*: args: argument namespace \"\"\" # Highest level parser object parser = argparse.ArgumentParser(description=desc, add_help=False) parser.add_argument(\"-c\", \"--config\", required=False, nargs=\"+\", default=[]) parser.add_argument(\"-h\", \"--help\", action=\"store_true\") # Handle the builder obj parser = self._builder_obj.build_override_parsers(parser=parser) if self._tune_obj is not None: parser = self._tune_obj.build_override_parsers(parser=parser) args = parser.parse_args() return args @staticmethod def _get_from_kwargs(args, configs): \"\"\"Get configs from the configs kwarg *Args*: args: argument namespace configs: config kwarg *Returns*: args: arg namespace \"\"\" if isinstance(configs, list): args.config.extend(configs) else: raise TypeError( f\"configs kwarg must be of type list -- given {type(configs)}\" ) return args def _get_payload(self, payload_obj, input_classes, ignore_args: typing.List): \"\"\"Get the parameter payload from the config file(s) Calls the various ways to get configs and then parses to retrieve the parameter payload - make sure to call deep update so as to not lose some parameters when only partially updating the payload *Args*: payload_obj: current payload object to call input_classes: classes to use to get payload ignore_args: args that were decorated for hyper-parameter tuning *Returns*: payload: dictionary of parameter values \"\"\" if self._args.help: # Call sys exit with a clean code as this is the help call which is not unexpected behavior self._print_usage_and_exit(sys_exit=True, exit_code=0) payload = {} dependencies = {\"paths\": [], \"rel_paths\": [], \"roots\": []} if payload_obj is not None: # Make sure we are actually trying to map to input classes if len(input_classes) > 0: # If configs are present then iterate through them and deal with the payload if len(self._args.config) > 0: for configs in self._args.config: payload_update = payload_obj.payload( input_classes, ignore_args, configs, self._args, dependencies, ) check_payload_overwrite(payload, payload_update, configs) deep_payload_update(payload, payload_update) # If there are no configs present we have to fall back only on cmd line args to fill out the necessary # data -- this is essentially using spock as a drop in replacement of arg-parser else: payload_update = payload_obj.payload( input_classes, ignore_args, None, self._args, dependencies ) check_payload_overwrite(payload, payload_update, None) deep_payload_update(payload, payload_update) return payload def _save( self, payload, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", tuner_payload=None, fixed_uuid=None, ): \"\"\"Private interface -- saves the current config setup to file with a UUID *Args*: payload: Spockspace to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite *Returns*: self so that functions can be chained \"\"\" if user_specified_path is not None: save_path = Path(user_specified_path) elif self._builder_obj.save_path is not None: save_path = Path(self._builder_obj.save_path) else: raise ValueError( \"Save did not receive a valid path from: (1) markup file(s) or (2) \" \"the keyword arg user_specified_path\" ) # Call the saver class and save function self._saver_obj.save( payload, save_path, file_name, create_save_path, extra_info, file_extension, tuner_payload, fixed_uuid, ) return self def save( self, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", add_tuner_sample: bool = False, ): \"\"\"Saves the current config setup to file with a UUID *Args*: file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) append_tuner_state: save the current tuner sample to the payload *Returns*: self so that functions can be chained \"\"\" if add_tuner_sample: if self._tune_obj is None: raise ValueError( f\"Called save method with add_tuner_sample as {add_tuner_sample} without passing any @spockTuner \" f\"decorated classes -- please use the add_tuner_sample flag for saving only hyper-parameter tuning \" f\"runs\" ) file_name = ( f\"hp.sample.{self._sample_count+1}\" if file_name is None else f\"{file_name}.hp.sample.{self._sample_count+1}\" ) self._save( self._tuner_state, file_name, user_specified_path, create_save_path, extra_info, file_extension, ) else: self._save( self._arg_namespace, file_name, user_specified_path, create_save_path, extra_info, file_extension, tuner_payload=self._tune_namespace if self._tune_obj is not None else None, ) return self def save_best( self, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", ): \"\"\"Saves the current best config setup to file *Args*: file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) *Returns*: self so that functions can be chained \"\"\" if self._tune_obj is None: raise ValueError( f\"Called save_best method without passing any @spockTuner decorated classes -- please use the save()\" f\" method for saving non hyper-parameter tuning runs\" ) file_name = f\"hp.best\" if file_name is None else f\"{file_name}.hp.best\" self._save( Spockspace(**vars(self._arg_namespace), **vars(self.best[0])), file_name, user_specified_path, create_save_path, extra_info, file_extension, fixed_uuid=self._fixed_uuid, ) return self @property def config_2_dict(self): \"\"\"Dictionary representation of the arg payload\"\"\" return self._saver_obj.dict_payload(self._arg_namespace)","title":"ConfigArgBuilder"},{"location":"reference/spock/builder/#instance-variables","text":"best Returns a Spockspace of the best hyper-parameter config and the associated metric value config_2_dict Dictionary representation of the arg payload tuner_status Returns a dictionary of all the necessary underlying tuner internals to report the result","title":"Instance variables"},{"location":"reference/spock/builder/#methods","text":"","title":"Methods"},{"location":"reference/spock/builder/#generate","text":"def generate ( self ) Generate method that returns the actual argument namespace Returns : argument namespace consisting of all config classes ??? example \"View Source\" def generate(self): \"\"\"Generate method that returns the actual argument namespace *Returns*: argument namespace consisting of all config classes \"\"\" return self._arg_namespace","title":"generate"},{"location":"reference/spock/builder/#sample","text":"def sample ( self ) Sample method that constructs a namespace from the fixed parameters and samples from the tuner space to generate a Spockspace derived from both Returns : argument namespace(s) -- fixed + drawn sample from tuner backend ??? example \"View Source\" def sample(self): \"\"\"Sample method that constructs a namespace from the fixed parameters and samples from the tuner space to generate a Spockspace derived from both *Returns*: argument namespace(s) -- fixed + drawn sample from tuner backend \"\"\" if self._tune_obj is None: raise ValueError( f\"Called sample method without passing any @spockTuner decorated classes\" ) if self._tuner_interface is None: raise ValueError( f\"Called sample method without first calling the tuner method that initializes the \" f\"backend library\" ) return_tuple = self._tuner_state self._tuner_status = self._tuner_interface.tuner_status self._tuner_state = self._tuner_interface.sample() self._sample_count += 1 return return_tuple","title":"sample"},{"location":"reference/spock/builder/#save","text":"def save ( self , file_name : str = None , user_specified_path : str = None , create_save_path : bool = True , extra_info : bool = True , file_extension : str = '.yaml' , add_tuner_sample : bool = False ) Saves the current config setup to file with a UUID Args : file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) append_tuner_state: save the current tuner sample to the payload Returns : self so that functions can be chained ??? example \"View Source\" def save( self, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", add_tuner_sample: bool = False, ): \"\"\"Saves the current config setup to file with a UUID *Args*: file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) append_tuner_state: save the current tuner sample to the payload *Returns*: self so that functions can be chained \"\"\" if add_tuner_sample: if self._tune_obj is None: raise ValueError( f\"Called save method with add_tuner_sample as {add_tuner_sample} without passing any @spockTuner \" f\"decorated classes -- please use the add_tuner_sample flag for saving only hyper-parameter tuning \" f\"runs\" ) file_name = ( f\"hp.sample.{self._sample_count+1}\" if file_name is None else f\"{file_name}.hp.sample.{self._sample_count+1}\" ) self._save( self._tuner_state, file_name, user_specified_path, create_save_path, extra_info, file_extension, ) else: self._save( self._arg_namespace, file_name, user_specified_path, create_save_path, extra_info, file_extension, tuner_payload=self._tune_namespace if self._tune_obj is not None else None, ) return self","title":"save"},{"location":"reference/spock/builder/#save_best","text":"def save_best ( self , file_name : str = None , user_specified_path : str = None , create_save_path : bool = True , extra_info : bool = True , file_extension : str = '.yaml' ) Saves the current best config setup to file Args : file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) Returns : self so that functions can be chained ??? example \"View Source\" def save_best( self, file_name: str = None, user_specified_path: str = None, create_save_path: bool = True, extra_info: bool = True, file_extension: str = \".yaml\", ): \"\"\"Saves the current best config setup to file *Args*: file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to just uuid if None user_specified_path: if user provides a path it will be used as the path to write create_save_path: bool to create the path to save if called extra_info: additional info to write to saved config (run date and git info) file_extension: file type to write (default: yaml) *Returns*: self so that functions can be chained \"\"\" if self._tune_obj is None: raise ValueError( f\"Called save_best method without passing any @spockTuner decorated classes -- please use the save()\" f\" method for saving non hyper-parameter tuning runs\" ) file_name = f\"hp.best\" if file_name is None else f\"{file_name}.hp.best\" self._save( Spockspace(**vars(self._arg_namespace), **vars(self.best[0])), file_name, user_specified_path, create_save_path, extra_info, file_extension, fixed_uuid=self._fixed_uuid, ) return self","title":"save_best"},{"location":"reference/spock/builder/#tuner","text":"def tuner ( self , tuner_config ) Chained call that builds the tuner interface for either optuna or ax depending upon the type of the tuner_obj Args : tuner_config: a class of type optuna.study.Study or AX**** Returns : self so that functions can be chained ??? example \"View Source\" def tuner(self, tuner_config): \"\"\"Chained call that builds the tuner interface for either optuna or ax depending upon the type of the tuner_obj *Args*: tuner_config: a class of type optuna.study.Study or AX**** *Returns*: self so that functions can be chained \"\"\" if self._tune_obj is None: raise ValueError( f\"Called tuner method without passing any @spockTuner decorated classes\" ) try: from spock.addons.tune.tuner import TunerInterface self._tuner_interface = TunerInterface( tuner_config=tuner_config, tuner_namespace=self._tune_namespace, fixed_namespace=self._arg_namespace, ) self._tuner_state = self._tuner_interface.sample() except ImportError: print( \"Missing libraries to support tune functionality. Please re-install with the extra tune \" \"dependencies -- pip install spock-config[tune]\" ) return self","title":"tuner"},{"location":"reference/spock/config/","text":"Module spock.config Creates the spock config decorator that wraps attrs None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Creates the spock config decorator that wraps attrs\"\"\" from spock.backend.config import spock_attr from spock.utils import _is_spock_instance # Simplified decorator for attrs spock = spock_attr # Public alias for checking if an object is a @spock annotated class isinstance_spock = _is_spock_instance Functions isinstance_spock def isinstance_spock ( __obj : object ) Checks if the object is a @spock decorated class Private interface that checks to see if the object passed in is registered within the spock module and also is a class with attrs attributes ( attrs_attrs ) Args : __obj: class to inspect Returns : bool ??? example \"View Source\" def _is_spock_instance(__obj: object): \"\"\"Checks if the object is a @spock decorated class Private interface that checks to see if the object passed in is registered within the spock module and also is a class with attrs attributes (__attrs_attrs__) *Args*: __obj: class to inspect *Returns*: bool \"\"\" return (__obj.__module__ == \"spock.backend.config\") and attr.has(__obj) spock def spock ( cls ) Map type hints to katras Connector function that maps type hinting style to the defined katra style which uses the more strict attr.ib() definition Args : cls: basic class def Returns : cls: slotted attrs class that is frozen and kw only ??? example \"View Source\" def spock_attr(cls): \"\"\"Map type hints to katras Connector function that maps type hinting style to the defined katra style which uses the more strict attr.ib() definition *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" bases, attrs_dict = _base_attr(cls) # Dynamically make an attr class obj = attr.make_class( name=cls.__name__, bases=bases, attrs=attrs_dict, kw_only=True, frozen=True ) # For each class we dynamically create we need to register it within the system modules for pickle to work setattr(sys.modules[\"spock\"].backend.config, obj.__name__, obj) # Swap the __doc__ string from cls to obj obj.__doc__ = cls.__doc__ return obj","title":"Config"},{"location":"reference/spock/config/#module-spockconfig","text":"Creates the spock config decorator that wraps attrs None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Creates the spock config decorator that wraps attrs\"\"\" from spock.backend.config import spock_attr from spock.utils import _is_spock_instance # Simplified decorator for attrs spock = spock_attr # Public alias for checking if an object is a @spock annotated class isinstance_spock = _is_spock_instance","title":"Module spock.config"},{"location":"reference/spock/config/#functions","text":"","title":"Functions"},{"location":"reference/spock/config/#isinstance_spock","text":"def isinstance_spock ( __obj : object ) Checks if the object is a @spock decorated class Private interface that checks to see if the object passed in is registered within the spock module and also is a class with attrs attributes ( attrs_attrs ) Args : __obj: class to inspect Returns : bool ??? example \"View Source\" def _is_spock_instance(__obj: object): \"\"\"Checks if the object is a @spock decorated class Private interface that checks to see if the object passed in is registered within the spock module and also is a class with attrs attributes (__attrs_attrs__) *Args*: __obj: class to inspect *Returns*: bool \"\"\" return (__obj.__module__ == \"spock.backend.config\") and attr.has(__obj)","title":"isinstance_spock"},{"location":"reference/spock/config/#spock","text":"def spock ( cls ) Map type hints to katras Connector function that maps type hinting style to the defined katra style which uses the more strict attr.ib() definition Args : cls: basic class def Returns : cls: slotted attrs class that is frozen and kw only ??? example \"View Source\" def spock_attr(cls): \"\"\"Map type hints to katras Connector function that maps type hinting style to the defined katra style which uses the more strict attr.ib() definition *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" bases, attrs_dict = _base_attr(cls) # Dynamically make an attr class obj = attr.make_class( name=cls.__name__, bases=bases, attrs=attrs_dict, kw_only=True, frozen=True ) # For each class we dynamically create we need to register it within the system modules for pickle to work setattr(sys.modules[\"spock\"].backend.config, obj.__name__, obj) # Swap the __doc__ string from cls to obj obj.__doc__ = cls.__doc__ return obj","title":"spock"},{"location":"reference/spock/handlers/","text":"Module spock.handlers I/O handlers for various file formats None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"I/O handlers for various file formats\"\"\" import json import os import re import typing from abc import ABC, abstractmethod from warnings import warn import pytomlpp import yaml from spock import __version__ from spock.utils import check_path_s3 class Handler(ABC): \"\"\"Base class for file type loaders ABC for loaders \"\"\" def load(self, path: str, s3_config=None) -> typing.Dict: \"\"\"Load function for file type This handles s3 path conversion for all handler types pre load call *Args*: path: path to file s3_config: optional s3 config object if using s3 storage *Returns*: dictionary of read file \"\"\" path = self._handle_possible_s3_load_path(path=path, s3_config=s3_config) return self._load(path=path) @abstractmethod def _load(self, path: str) -> typing.Dict: \"\"\"Private load function for file type *Args*: path: path to file *Returns*: dictionary of read file \"\"\" raise NotImplementedError def save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str, name: str, create_path: bool = False, s3_config=None, ): \"\"\"Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage *Returns*: \"\"\" write_path, is_s3 = self._handle_possible_s3_save_path( path=path, name=name, create_path=create_path, s3_config=s3_config ) write_path = self._save(out_dict=out_dict, info_dict=info_dict, path=write_path) # After write check if it needs to be pushed to S3 if is_s3: try: from spock.addons.s3.utils import handle_s3_save_path handle_s3_save_path( temp_path=write_path, s3_path=path, name=name, s3_config=s3_config ) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// save path\") @abstractmethod def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ) -> str: \"\"\"Write function for file type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" raise NotImplementedError @staticmethod def _handle_possible_s3_load_path(path: str, s3_config=None) -> str: \"\"\"Handles the possibility of having to handle loading from a S3 path Checks to see if it detects a S3 uri and if so triggers imports of s3 functionality and handles the file download *Args*: path: spock config path s3_config: optional s3 configuration object *Returns*: path: current path for the configuration file \"\"\" is_s3 = check_path_s3(path=path) if is_s3: try: from spock.addons.s3.utils import handle_s3_load_path path = handle_s3_load_path(path=path, s3_config=s3_config) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// load path\") return path @staticmethod def _handle_possible_s3_save_path( path: str, name: str, create_path: bool, s3_config=None ) -> typing.Tuple[str, bool]: \"\"\"Handles the possibility of having to save to a S3 path Checks to see if it detects a S3 uri and if so generates a tmp location to write the file to pre-upload *Args*: path: save path name: spock generated file name create_path: create the path for non s3 data s3_config: s3 config object *Returns*: \"\"\" is_s3 = check_path_s3(path=path) if is_s3: if s3_config is None: raise ValueError( \"Save to S3 -- Missing S3Config object which is necessary to handle S3 style paths\" ) write_path = f\"{s3_config.temp_folder}/{name}\" # Strip double slashes if exist write_path = write_path.replace(r\"//\", r\"/\") else: # Handle the path logic for non S3 if not os.path.exists(path) and create_path: os.makedirs(path) write_path = f\"{path}/{name}\" return write_path, is_s3 @staticmethod def write_extra_info(path, info_dict): \"\"\"Writes extra info to commented newlines *Args*: path: path to write out info_dict: info payload to write *Returns*: \"\"\" # Write the commented info as new lines with open(path, \"w+\") as fid: # Write a spock header fid.write(f\"# Spock Version: {__version__}\\n\") # Write info dict if not None if info_dict is not None: for k, v in info_dict.items(): fid.write(f\"{k}: {v}\\n\") fid.write(\"\\n\") class YAMLHandler(Handler): \"\"\"YAML class for loading YAML config files Base YAML class \"\"\" # override default SafeLoader behavior to correctly # interpret 1e1 (as opposed to 1.e+1) as 10 # https://stackoverflow.com/questions/30458977/yaml-loads-5e-6-as-string-and-not-a-number/30462009#30462009 yaml.SafeLoader.add_implicit_resolver( \"tag:yaml.org,2002:float\", re.compile( \"\"\"^(?: [-+]?(?:[0-9][0-9_]*)\\\\.[0-9_]*(?:[eE][-+]?[0-9]+)? |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+) |\\\\.[0-9_]+(?:[eE][-+][0-9]+)? |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\\\.[0-9_]* |[-+]?\\\\.(?:inf|Inf|INF) |\\\\.(?:nan|NaN|NAN))$\"\"\", re.X, ), list(\"-+0123456789.\"), ) def _load(self, path: str) -> typing.Dict: \"\"\"YAML load function *Args*: path: path to YAML file *Returns*: base_payload: dictionary of read file \"\"\" file_contents = open(path, \"r\").read() file_contents = re.sub(r\"--([a-zA-Z0-9_]*)\", r\"\\g<1>: True\", file_contents) base_payload = yaml.safe_load(file_contents) return base_payload def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ): \"\"\"Write function for YAML type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" # First write the commented info self.write_extra_info(path=path, info_dict=info_dict) # Remove aliases in YAML dump yaml.Dumper.ignore_aliases = lambda *args: True with open(path, \"a\") as yaml_fid: yaml.safe_dump(out_dict, yaml_fid, default_flow_style=False) return path class TOMLHandler(Handler): \"\"\"TOML class for loading TOML config files Base TOML class \"\"\" def _load(self, path: str) -> typing.Dict: \"\"\"TOML load function *Args*: path: path to TOML file Returns: base_payload: dictionary of read file \"\"\" base_payload = pytomlpp.load(path) return base_payload def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ): \"\"\"Write function for TOML type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" # First write the commented info self.write_extra_info(path=path, info_dict=info_dict) with open(path, \"a\") as toml_fid: pytomlpp.dump(out_dict, toml_fid) return path class JSONHandler(Handler): \"\"\"JSON class for loading JSON config files Base JSON class \"\"\" def _load(self, path: str) -> typing.Dict: \"\"\"JSON load function *Args*: path: path to JSON file Returns: base_payload: dictionary of read file \"\"\" with open(path) as json_fid: base_payload = json.load(json_fid) return base_payload def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ): \"\"\"Write function for JSON type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" if info_dict is not None: warn( \"JSON does not support comments and thus cannot save extra info to file... removing extra info\" ) info_dict = None with open(path, \"a\") as json_fid: json.dump(out_dict, json_fid, indent=4, separators=(\",\", \": \")) return path Classes Handler class Handler ( / , * args , ** kwargs ) ??? example \"View Source\" class Handler(ABC): \"\"\"Base class for file type loaders ABC for loaders \"\"\" def load(self, path: str, s3_config=None) -> typing.Dict: \"\"\"Load function for file type This handles s3 path conversion for all handler types pre load call *Args*: path: path to file s3_config: optional s3 config object if using s3 storage *Returns*: dictionary of read file \"\"\" path = self._handle_possible_s3_load_path(path=path, s3_config=s3_config) return self._load(path=path) @abstractmethod def _load(self, path: str) -> typing.Dict: \"\"\"Private load function for file type *Args*: path: path to file *Returns*: dictionary of read file \"\"\" raise NotImplementedError def save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str, name: str, create_path: bool = False, s3_config=None, ): \"\"\"Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage *Returns*: \"\"\" write_path, is_s3 = self._handle_possible_s3_save_path( path=path, name=name, create_path=create_path, s3_config=s3_config ) write_path = self._save(out_dict=out_dict, info_dict=info_dict, path=write_path) # After write check if it needs to be pushed to S3 if is_s3: try: from spock.addons.s3.utils import handle_s3_save_path handle_s3_save_path( temp_path=write_path, s3_path=path, name=name, s3_config=s3_config ) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// save path\") @abstractmethod def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ) -> str: \"\"\"Write function for file type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" raise NotImplementedError @staticmethod def _handle_possible_s3_load_path(path: str, s3_config=None) -> str: \"\"\"Handles the possibility of having to handle loading from a S3 path Checks to see if it detects a S3 uri and if so triggers imports of s3 functionality and handles the file download *Args*: path: spock config path s3_config: optional s3 configuration object *Returns*: path: current path for the configuration file \"\"\" is_s3 = check_path_s3(path=path) if is_s3: try: from spock.addons.s3.utils import handle_s3_load_path path = handle_s3_load_path(path=path, s3_config=s3_config) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// load path\") return path @staticmethod def _handle_possible_s3_save_path( path: str, name: str, create_path: bool, s3_config=None ) -> typing.Tuple[str, bool]: \"\"\"Handles the possibility of having to save to a S3 path Checks to see if it detects a S3 uri and if so generates a tmp location to write the file to pre-upload *Args*: path: save path name: spock generated file name create_path: create the path for non s3 data s3_config: s3 config object *Returns*: \"\"\" is_s3 = check_path_s3(path=path) if is_s3: if s3_config is None: raise ValueError( \"Save to S3 -- Missing S3Config object which is necessary to handle S3 style paths\" ) write_path = f\"{s3_config.temp_folder}/{name}\" # Strip double slashes if exist write_path = write_path.replace(r\"//\", r\"/\") else: # Handle the path logic for non S3 if not os.path.exists(path) and create_path: os.makedirs(path) write_path = f\"{path}/{name}\" return write_path, is_s3 @staticmethod def write_extra_info(path, info_dict): \"\"\"Writes extra info to commented newlines *Args*: path: path to write out info_dict: info payload to write *Returns*: \"\"\" # Write the commented info as new lines with open(path, \"w+\") as fid: # Write a spock header fid.write(f\"# Spock Version: {__version__}\\n\") # Write info dict if not None if info_dict is not None: for k, v in info_dict.items(): fid.write(f\"{k}: {v}\\n\") fid.write(\"\\n\") Ancestors (in MRO) abc.ABC Descendants spock.handlers.YAMLHandler spock.handlers.TOMLHandler spock.handlers.JSONHandler Static methods write_extra_info def write_extra_info ( path , info_dict ) Writes extra info to commented newlines Args : path: path to write out info_dict: info payload to write Returns : ??? example \"View Source\" @staticmethod def write_extra_info(path, info_dict): \"\"\"Writes extra info to commented newlines *Args*: path: path to write out info_dict: info payload to write *Returns*: \"\"\" # Write the commented info as new lines with open(path, \"w+\") as fid: # Write a spock header fid.write(f\"# Spock Version: {__version__}\\n\") # Write info dict if not None if info_dict is not None: for k, v in info_dict.items(): fid.write(f\"{k}: {v}\\n\") fid.write(\"\\n\") Methods load def load ( self , path : str , s3_config = None ) -> Dict Load function for file type This handles s3 path conversion for all handler types pre load call Args : path: path to file s3_config: optional s3 config object if using s3 storage Returns : dictionary of read file ??? example \"View Source\" def load(self, path: str, s3_config=None) -> typing.Dict: \"\"\"Load function for file type This handles s3 path conversion for all handler types pre load call *Args*: path: path to file s3_config: optional s3 config object if using s3 storage *Returns*: dictionary of read file \"\"\" path = self._handle_possible_s3_load_path(path=path, s3_config=s3_config) return self._load(path=path) save def save ( self , out_dict : Dict , info_dict : Union [ Dict , NoneType ], path : str , name : str , create_path : bool = False , s3_config = None ) Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload Args : out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage Returns : ??? example \"View Source\" def save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str, name: str, create_path: bool = False, s3_config=None, ): \"\"\"Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage *Returns*: \"\"\" write_path, is_s3 = self._handle_possible_s3_save_path( path=path, name=name, create_path=create_path, s3_config=s3_config ) write_path = self._save(out_dict=out_dict, info_dict=info_dict, path=write_path) # After write check if it needs to be pushed to S3 if is_s3: try: from spock.addons.s3.utils import handle_s3_save_path handle_s3_save_path( temp_path=write_path, s3_path=path, name=name, s3_config=s3_config ) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// save path\") JSONHandler class JSONHandler ( / , * args , ** kwargs ) ??? example \"View Source\" class JSONHandler(Handler): \"\"\"JSON class for loading JSON config files Base JSON class \"\"\" def _load(self, path: str) -> typing.Dict: \"\"\"JSON load function *Args*: path: path to JSON file Returns: base_payload: dictionary of read file \"\"\" with open(path) as json_fid: base_payload = json.load(json_fid) return base_payload def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ): \"\"\"Write function for JSON type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" if info_dict is not None: warn( \"JSON does not support comments and thus cannot save extra info to file... removing extra info\" ) info_dict = None with open(path, \"a\") as json_fid: json.dump(out_dict, json_fid, indent=4, separators=(\",\", \": \")) return path Ancestors (in MRO) spock.handlers.Handler abc.ABC Static methods write_extra_info def write_extra_info ( path , info_dict ) Writes extra info to commented newlines Args : path: path to write out info_dict: info payload to write Returns : ??? example \"View Source\" @staticmethod def write_extra_info(path, info_dict): \"\"\"Writes extra info to commented newlines *Args*: path: path to write out info_dict: info payload to write *Returns*: \"\"\" # Write the commented info as new lines with open(path, \"w+\") as fid: # Write a spock header fid.write(f\"# Spock Version: {__version__}\\n\") # Write info dict if not None if info_dict is not None: for k, v in info_dict.items(): fid.write(f\"{k}: {v}\\n\") fid.write(\"\\n\") Methods load def load ( self , path : str , s3_config = None ) -> Dict Load function for file type This handles s3 path conversion for all handler types pre load call Args : path: path to file s3_config: optional s3 config object if using s3 storage Returns : dictionary of read file ??? example \"View Source\" def load(self, path: str, s3_config=None) -> typing.Dict: \"\"\"Load function for file type This handles s3 path conversion for all handler types pre load call *Args*: path: path to file s3_config: optional s3 config object if using s3 storage *Returns*: dictionary of read file \"\"\" path = self._handle_possible_s3_load_path(path=path, s3_config=s3_config) return self._load(path=path) save def save ( self , out_dict : Dict , info_dict : Union [ Dict , NoneType ], path : str , name : str , create_path : bool = False , s3_config = None ) Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload Args : out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage Returns : ??? example \"View Source\" def save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str, name: str, create_path: bool = False, s3_config=None, ): \"\"\"Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage *Returns*: \"\"\" write_path, is_s3 = self._handle_possible_s3_save_path( path=path, name=name, create_path=create_path, s3_config=s3_config ) write_path = self._save(out_dict=out_dict, info_dict=info_dict, path=write_path) # After write check if it needs to be pushed to S3 if is_s3: try: from spock.addons.s3.utils import handle_s3_save_path handle_s3_save_path( temp_path=write_path, s3_path=path, name=name, s3_config=s3_config ) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// save path\") TOMLHandler class TOMLHandler ( / , * args , ** kwargs ) ??? example \"View Source\" class TOMLHandler(Handler): \"\"\"TOML class for loading TOML config files Base TOML class \"\"\" def _load(self, path: str) -> typing.Dict: \"\"\"TOML load function *Args*: path: path to TOML file Returns: base_payload: dictionary of read file \"\"\" base_payload = pytomlpp.load(path) return base_payload def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ): \"\"\"Write function for TOML type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" # First write the commented info self.write_extra_info(path=path, info_dict=info_dict) with open(path, \"a\") as toml_fid: pytomlpp.dump(out_dict, toml_fid) return path Ancestors (in MRO) spock.handlers.Handler abc.ABC Static methods write_extra_info def write_extra_info ( path , info_dict ) Writes extra info to commented newlines Args : path: path to write out info_dict: info payload to write Returns : ??? example \"View Source\" @staticmethod def write_extra_info(path, info_dict): \"\"\"Writes extra info to commented newlines *Args*: path: path to write out info_dict: info payload to write *Returns*: \"\"\" # Write the commented info as new lines with open(path, \"w+\") as fid: # Write a spock header fid.write(f\"# Spock Version: {__version__}\\n\") # Write info dict if not None if info_dict is not None: for k, v in info_dict.items(): fid.write(f\"{k}: {v}\\n\") fid.write(\"\\n\") Methods load def load ( self , path : str , s3_config = None ) -> Dict Load function for file type This handles s3 path conversion for all handler types pre load call Args : path: path to file s3_config: optional s3 config object if using s3 storage Returns : dictionary of read file ??? example \"View Source\" def load(self, path: str, s3_config=None) -> typing.Dict: \"\"\"Load function for file type This handles s3 path conversion for all handler types pre load call *Args*: path: path to file s3_config: optional s3 config object if using s3 storage *Returns*: dictionary of read file \"\"\" path = self._handle_possible_s3_load_path(path=path, s3_config=s3_config) return self._load(path=path) save def save ( self , out_dict : Dict , info_dict : Union [ Dict , NoneType ], path : str , name : str , create_path : bool = False , s3_config = None ) Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload Args : out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage Returns : ??? example \"View Source\" def save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str, name: str, create_path: bool = False, s3_config=None, ): \"\"\"Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage *Returns*: \"\"\" write_path, is_s3 = self._handle_possible_s3_save_path( path=path, name=name, create_path=create_path, s3_config=s3_config ) write_path = self._save(out_dict=out_dict, info_dict=info_dict, path=write_path) # After write check if it needs to be pushed to S3 if is_s3: try: from spock.addons.s3.utils import handle_s3_save_path handle_s3_save_path( temp_path=write_path, s3_path=path, name=name, s3_config=s3_config ) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// save path\") YAMLHandler class YAMLHandler ( / , * args , ** kwargs ) ??? example \"View Source\" class YAMLHandler(Handler): \"\"\"YAML class for loading YAML config files Base YAML class \"\"\" # override default SafeLoader behavior to correctly # interpret 1e1 (as opposed to 1.e+1) as 10 # https://stackoverflow.com/questions/30458977/yaml-loads-5e-6-as-string-and-not-a-number/30462009#30462009 yaml.SafeLoader.add_implicit_resolver( \"tag:yaml.org,2002:float\", re.compile( \"\"\"^(?: [-+]?(?:[0-9][0-9_]*)\\\\.[0-9_]*(?:[eE][-+]?[0-9]+)? |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+) |\\\\.[0-9_]+(?:[eE][-+][0-9]+)? |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\\\.[0-9_]* |[-+]?\\\\.(?:inf|Inf|INF) |\\\\.(?:nan|NaN|NAN))$\"\"\", re.X, ), list(\"-+0123456789.\"), ) def _load(self, path: str) -> typing.Dict: \"\"\"YAML load function *Args*: path: path to YAML file *Returns*: base_payload: dictionary of read file \"\"\" file_contents = open(path, \"r\").read() file_contents = re.sub(r\"--([a-zA-Z0-9_]*)\", r\"\\g<1>: True\", file_contents) base_payload = yaml.safe_load(file_contents) return base_payload def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ): \"\"\"Write function for YAML type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" # First write the commented info self.write_extra_info(path=path, info_dict=info_dict) # Remove aliases in YAML dump yaml.Dumper.ignore_aliases = lambda *args: True with open(path, \"a\") as yaml_fid: yaml.safe_dump(out_dict, yaml_fid, default_flow_style=False) return path Ancestors (in MRO) spock.handlers.Handler abc.ABC Static methods write_extra_info def write_extra_info ( path , info_dict ) Writes extra info to commented newlines Args : path: path to write out info_dict: info payload to write Returns : ??? example \"View Source\" @staticmethod def write_extra_info(path, info_dict): \"\"\"Writes extra info to commented newlines *Args*: path: path to write out info_dict: info payload to write *Returns*: \"\"\" # Write the commented info as new lines with open(path, \"w+\") as fid: # Write a spock header fid.write(f\"# Spock Version: {__version__}\\n\") # Write info dict if not None if info_dict is not None: for k, v in info_dict.items(): fid.write(f\"{k}: {v}\\n\") fid.write(\"\\n\") Methods load def load ( self , path : str , s3_config = None ) -> Dict Load function for file type This handles s3 path conversion for all handler types pre load call Args : path: path to file s3_config: optional s3 config object if using s3 storage Returns : dictionary of read file ??? example \"View Source\" def load(self, path: str, s3_config=None) -> typing.Dict: \"\"\"Load function for file type This handles s3 path conversion for all handler types pre load call *Args*: path: path to file s3_config: optional s3 config object if using s3 storage *Returns*: dictionary of read file \"\"\" path = self._handle_possible_s3_load_path(path=path, s3_config=s3_config) return self._load(path=path) save def save ( self , out_dict : Dict , info_dict : Union [ Dict , NoneType ], path : str , name : str , create_path : bool = False , s3_config = None ) Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload Args : out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage Returns : ??? example \"View Source\" def save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str, name: str, create_path: bool = False, s3_config=None, ): \"\"\"Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage *Returns*: \"\"\" write_path, is_s3 = self._handle_possible_s3_save_path( path=path, name=name, create_path=create_path, s3_config=s3_config ) write_path = self._save(out_dict=out_dict, info_dict=info_dict, path=write_path) # After write check if it needs to be pushed to S3 if is_s3: try: from spock.addons.s3.utils import handle_s3_save_path handle_s3_save_path( temp_path=write_path, s3_path=path, name=name, s3_config=s3_config ) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// save path\")","title":"Handlers"},{"location":"reference/spock/handlers/#module-spockhandlers","text":"I/O handlers for various file formats None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"I/O handlers for various file formats\"\"\" import json import os import re import typing from abc import ABC, abstractmethod from warnings import warn import pytomlpp import yaml from spock import __version__ from spock.utils import check_path_s3 class Handler(ABC): \"\"\"Base class for file type loaders ABC for loaders \"\"\" def load(self, path: str, s3_config=None) -> typing.Dict: \"\"\"Load function for file type This handles s3 path conversion for all handler types pre load call *Args*: path: path to file s3_config: optional s3 config object if using s3 storage *Returns*: dictionary of read file \"\"\" path = self._handle_possible_s3_load_path(path=path, s3_config=s3_config) return self._load(path=path) @abstractmethod def _load(self, path: str) -> typing.Dict: \"\"\"Private load function for file type *Args*: path: path to file *Returns*: dictionary of read file \"\"\" raise NotImplementedError def save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str, name: str, create_path: bool = False, s3_config=None, ): \"\"\"Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage *Returns*: \"\"\" write_path, is_s3 = self._handle_possible_s3_save_path( path=path, name=name, create_path=create_path, s3_config=s3_config ) write_path = self._save(out_dict=out_dict, info_dict=info_dict, path=write_path) # After write check if it needs to be pushed to S3 if is_s3: try: from spock.addons.s3.utils import handle_s3_save_path handle_s3_save_path( temp_path=write_path, s3_path=path, name=name, s3_config=s3_config ) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// save path\") @abstractmethod def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ) -> str: \"\"\"Write function for file type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" raise NotImplementedError @staticmethod def _handle_possible_s3_load_path(path: str, s3_config=None) -> str: \"\"\"Handles the possibility of having to handle loading from a S3 path Checks to see if it detects a S3 uri and if so triggers imports of s3 functionality and handles the file download *Args*: path: spock config path s3_config: optional s3 configuration object *Returns*: path: current path for the configuration file \"\"\" is_s3 = check_path_s3(path=path) if is_s3: try: from spock.addons.s3.utils import handle_s3_load_path path = handle_s3_load_path(path=path, s3_config=s3_config) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// load path\") return path @staticmethod def _handle_possible_s3_save_path( path: str, name: str, create_path: bool, s3_config=None ) -> typing.Tuple[str, bool]: \"\"\"Handles the possibility of having to save to a S3 path Checks to see if it detects a S3 uri and if so generates a tmp location to write the file to pre-upload *Args*: path: save path name: spock generated file name create_path: create the path for non s3 data s3_config: s3 config object *Returns*: \"\"\" is_s3 = check_path_s3(path=path) if is_s3: if s3_config is None: raise ValueError( \"Save to S3 -- Missing S3Config object which is necessary to handle S3 style paths\" ) write_path = f\"{s3_config.temp_folder}/{name}\" # Strip double slashes if exist write_path = write_path.replace(r\"//\", r\"/\") else: # Handle the path logic for non S3 if not os.path.exists(path) and create_path: os.makedirs(path) write_path = f\"{path}/{name}\" return write_path, is_s3 @staticmethod def write_extra_info(path, info_dict): \"\"\"Writes extra info to commented newlines *Args*: path: path to write out info_dict: info payload to write *Returns*: \"\"\" # Write the commented info as new lines with open(path, \"w+\") as fid: # Write a spock header fid.write(f\"# Spock Version: {__version__}\\n\") # Write info dict if not None if info_dict is not None: for k, v in info_dict.items(): fid.write(f\"{k}: {v}\\n\") fid.write(\"\\n\") class YAMLHandler(Handler): \"\"\"YAML class for loading YAML config files Base YAML class \"\"\" # override default SafeLoader behavior to correctly # interpret 1e1 (as opposed to 1.e+1) as 10 # https://stackoverflow.com/questions/30458977/yaml-loads-5e-6-as-string-and-not-a-number/30462009#30462009 yaml.SafeLoader.add_implicit_resolver( \"tag:yaml.org,2002:float\", re.compile( \"\"\"^(?: [-+]?(?:[0-9][0-9_]*)\\\\.[0-9_]*(?:[eE][-+]?[0-9]+)? |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+) |\\\\.[0-9_]+(?:[eE][-+][0-9]+)? |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\\\.[0-9_]* |[-+]?\\\\.(?:inf|Inf|INF) |\\\\.(?:nan|NaN|NAN))$\"\"\", re.X, ), list(\"-+0123456789.\"), ) def _load(self, path: str) -> typing.Dict: \"\"\"YAML load function *Args*: path: path to YAML file *Returns*: base_payload: dictionary of read file \"\"\" file_contents = open(path, \"r\").read() file_contents = re.sub(r\"--([a-zA-Z0-9_]*)\", r\"\\g<1>: True\", file_contents) base_payload = yaml.safe_load(file_contents) return base_payload def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ): \"\"\"Write function for YAML type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" # First write the commented info self.write_extra_info(path=path, info_dict=info_dict) # Remove aliases in YAML dump yaml.Dumper.ignore_aliases = lambda *args: True with open(path, \"a\") as yaml_fid: yaml.safe_dump(out_dict, yaml_fid, default_flow_style=False) return path class TOMLHandler(Handler): \"\"\"TOML class for loading TOML config files Base TOML class \"\"\" def _load(self, path: str) -> typing.Dict: \"\"\"TOML load function *Args*: path: path to TOML file Returns: base_payload: dictionary of read file \"\"\" base_payload = pytomlpp.load(path) return base_payload def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ): \"\"\"Write function for TOML type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" # First write the commented info self.write_extra_info(path=path, info_dict=info_dict) with open(path, \"a\") as toml_fid: pytomlpp.dump(out_dict, toml_fid) return path class JSONHandler(Handler): \"\"\"JSON class for loading JSON config files Base JSON class \"\"\" def _load(self, path: str) -> typing.Dict: \"\"\"JSON load function *Args*: path: path to JSON file Returns: base_payload: dictionary of read file \"\"\" with open(path) as json_fid: base_payload = json.load(json_fid) return base_payload def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ): \"\"\"Write function for JSON type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" if info_dict is not None: warn( \"JSON does not support comments and thus cannot save extra info to file... removing extra info\" ) info_dict = None with open(path, \"a\") as json_fid: json.dump(out_dict, json_fid, indent=4, separators=(\",\", \": \")) return path","title":"Module spock.handlers"},{"location":"reference/spock/handlers/#classes","text":"","title":"Classes"},{"location":"reference/spock/handlers/#handler","text":"class Handler ( / , * args , ** kwargs ) ??? example \"View Source\" class Handler(ABC): \"\"\"Base class for file type loaders ABC for loaders \"\"\" def load(self, path: str, s3_config=None) -> typing.Dict: \"\"\"Load function for file type This handles s3 path conversion for all handler types pre load call *Args*: path: path to file s3_config: optional s3 config object if using s3 storage *Returns*: dictionary of read file \"\"\" path = self._handle_possible_s3_load_path(path=path, s3_config=s3_config) return self._load(path=path) @abstractmethod def _load(self, path: str) -> typing.Dict: \"\"\"Private load function for file type *Args*: path: path to file *Returns*: dictionary of read file \"\"\" raise NotImplementedError def save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str, name: str, create_path: bool = False, s3_config=None, ): \"\"\"Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage *Returns*: \"\"\" write_path, is_s3 = self._handle_possible_s3_save_path( path=path, name=name, create_path=create_path, s3_config=s3_config ) write_path = self._save(out_dict=out_dict, info_dict=info_dict, path=write_path) # After write check if it needs to be pushed to S3 if is_s3: try: from spock.addons.s3.utils import handle_s3_save_path handle_s3_save_path( temp_path=write_path, s3_path=path, name=name, s3_config=s3_config ) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// save path\") @abstractmethod def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ) -> str: \"\"\"Write function for file type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" raise NotImplementedError @staticmethod def _handle_possible_s3_load_path(path: str, s3_config=None) -> str: \"\"\"Handles the possibility of having to handle loading from a S3 path Checks to see if it detects a S3 uri and if so triggers imports of s3 functionality and handles the file download *Args*: path: spock config path s3_config: optional s3 configuration object *Returns*: path: current path for the configuration file \"\"\" is_s3 = check_path_s3(path=path) if is_s3: try: from spock.addons.s3.utils import handle_s3_load_path path = handle_s3_load_path(path=path, s3_config=s3_config) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// load path\") return path @staticmethod def _handle_possible_s3_save_path( path: str, name: str, create_path: bool, s3_config=None ) -> typing.Tuple[str, bool]: \"\"\"Handles the possibility of having to save to a S3 path Checks to see if it detects a S3 uri and if so generates a tmp location to write the file to pre-upload *Args*: path: save path name: spock generated file name create_path: create the path for non s3 data s3_config: s3 config object *Returns*: \"\"\" is_s3 = check_path_s3(path=path) if is_s3: if s3_config is None: raise ValueError( \"Save to S3 -- Missing S3Config object which is necessary to handle S3 style paths\" ) write_path = f\"{s3_config.temp_folder}/{name}\" # Strip double slashes if exist write_path = write_path.replace(r\"//\", r\"/\") else: # Handle the path logic for non S3 if not os.path.exists(path) and create_path: os.makedirs(path) write_path = f\"{path}/{name}\" return write_path, is_s3 @staticmethod def write_extra_info(path, info_dict): \"\"\"Writes extra info to commented newlines *Args*: path: path to write out info_dict: info payload to write *Returns*: \"\"\" # Write the commented info as new lines with open(path, \"w+\") as fid: # Write a spock header fid.write(f\"# Spock Version: {__version__}\\n\") # Write info dict if not None if info_dict is not None: for k, v in info_dict.items(): fid.write(f\"{k}: {v}\\n\") fid.write(\"\\n\")","title":"Handler"},{"location":"reference/spock/handlers/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/handlers/#descendants","text":"spock.handlers.YAMLHandler spock.handlers.TOMLHandler spock.handlers.JSONHandler","title":"Descendants"},{"location":"reference/spock/handlers/#static-methods","text":"","title":"Static methods"},{"location":"reference/spock/handlers/#write_extra_info","text":"def write_extra_info ( path , info_dict ) Writes extra info to commented newlines Args : path: path to write out info_dict: info payload to write Returns : ??? example \"View Source\" @staticmethod def write_extra_info(path, info_dict): \"\"\"Writes extra info to commented newlines *Args*: path: path to write out info_dict: info payload to write *Returns*: \"\"\" # Write the commented info as new lines with open(path, \"w+\") as fid: # Write a spock header fid.write(f\"# Spock Version: {__version__}\\n\") # Write info dict if not None if info_dict is not None: for k, v in info_dict.items(): fid.write(f\"{k}: {v}\\n\") fid.write(\"\\n\")","title":"write_extra_info"},{"location":"reference/spock/handlers/#methods","text":"","title":"Methods"},{"location":"reference/spock/handlers/#load","text":"def load ( self , path : str , s3_config = None ) -> Dict Load function for file type This handles s3 path conversion for all handler types pre load call Args : path: path to file s3_config: optional s3 config object if using s3 storage Returns : dictionary of read file ??? example \"View Source\" def load(self, path: str, s3_config=None) -> typing.Dict: \"\"\"Load function for file type This handles s3 path conversion for all handler types pre load call *Args*: path: path to file s3_config: optional s3 config object if using s3 storage *Returns*: dictionary of read file \"\"\" path = self._handle_possible_s3_load_path(path=path, s3_config=s3_config) return self._load(path=path)","title":"load"},{"location":"reference/spock/handlers/#save","text":"def save ( self , out_dict : Dict , info_dict : Union [ Dict , NoneType ], path : str , name : str , create_path : bool = False , s3_config = None ) Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload Args : out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage Returns : ??? example \"View Source\" def save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str, name: str, create_path: bool = False, s3_config=None, ): \"\"\"Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage *Returns*: \"\"\" write_path, is_s3 = self._handle_possible_s3_save_path( path=path, name=name, create_path=create_path, s3_config=s3_config ) write_path = self._save(out_dict=out_dict, info_dict=info_dict, path=write_path) # After write check if it needs to be pushed to S3 if is_s3: try: from spock.addons.s3.utils import handle_s3_save_path handle_s3_save_path( temp_path=write_path, s3_path=path, name=name, s3_config=s3_config ) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// save path\")","title":"save"},{"location":"reference/spock/handlers/#jsonhandler","text":"class JSONHandler ( / , * args , ** kwargs ) ??? example \"View Source\" class JSONHandler(Handler): \"\"\"JSON class for loading JSON config files Base JSON class \"\"\" def _load(self, path: str) -> typing.Dict: \"\"\"JSON load function *Args*: path: path to JSON file Returns: base_payload: dictionary of read file \"\"\" with open(path) as json_fid: base_payload = json.load(json_fid) return base_payload def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ): \"\"\"Write function for JSON type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" if info_dict is not None: warn( \"JSON does not support comments and thus cannot save extra info to file... removing extra info\" ) info_dict = None with open(path, \"a\") as json_fid: json.dump(out_dict, json_fid, indent=4, separators=(\",\", \": \")) return path","title":"JSONHandler"},{"location":"reference/spock/handlers/#ancestors-in-mro_1","text":"spock.handlers.Handler abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/handlers/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/spock/handlers/#write_extra_info_1","text":"def write_extra_info ( path , info_dict ) Writes extra info to commented newlines Args : path: path to write out info_dict: info payload to write Returns : ??? example \"View Source\" @staticmethod def write_extra_info(path, info_dict): \"\"\"Writes extra info to commented newlines *Args*: path: path to write out info_dict: info payload to write *Returns*: \"\"\" # Write the commented info as new lines with open(path, \"w+\") as fid: # Write a spock header fid.write(f\"# Spock Version: {__version__}\\n\") # Write info dict if not None if info_dict is not None: for k, v in info_dict.items(): fid.write(f\"{k}: {v}\\n\") fid.write(\"\\n\")","title":"write_extra_info"},{"location":"reference/spock/handlers/#methods_1","text":"","title":"Methods"},{"location":"reference/spock/handlers/#load_1","text":"def load ( self , path : str , s3_config = None ) -> Dict Load function for file type This handles s3 path conversion for all handler types pre load call Args : path: path to file s3_config: optional s3 config object if using s3 storage Returns : dictionary of read file ??? example \"View Source\" def load(self, path: str, s3_config=None) -> typing.Dict: \"\"\"Load function for file type This handles s3 path conversion for all handler types pre load call *Args*: path: path to file s3_config: optional s3 config object if using s3 storage *Returns*: dictionary of read file \"\"\" path = self._handle_possible_s3_load_path(path=path, s3_config=s3_config) return self._load(path=path)","title":"load"},{"location":"reference/spock/handlers/#save_1","text":"def save ( self , out_dict : Dict , info_dict : Union [ Dict , NoneType ], path : str , name : str , create_path : bool = False , s3_config = None ) Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload Args : out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage Returns : ??? example \"View Source\" def save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str, name: str, create_path: bool = False, s3_config=None, ): \"\"\"Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage *Returns*: \"\"\" write_path, is_s3 = self._handle_possible_s3_save_path( path=path, name=name, create_path=create_path, s3_config=s3_config ) write_path = self._save(out_dict=out_dict, info_dict=info_dict, path=write_path) # After write check if it needs to be pushed to S3 if is_s3: try: from spock.addons.s3.utils import handle_s3_save_path handle_s3_save_path( temp_path=write_path, s3_path=path, name=name, s3_config=s3_config ) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// save path\")","title":"save"},{"location":"reference/spock/handlers/#tomlhandler","text":"class TOMLHandler ( / , * args , ** kwargs ) ??? example \"View Source\" class TOMLHandler(Handler): \"\"\"TOML class for loading TOML config files Base TOML class \"\"\" def _load(self, path: str) -> typing.Dict: \"\"\"TOML load function *Args*: path: path to TOML file Returns: base_payload: dictionary of read file \"\"\" base_payload = pytomlpp.load(path) return base_payload def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ): \"\"\"Write function for TOML type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" # First write the commented info self.write_extra_info(path=path, info_dict=info_dict) with open(path, \"a\") as toml_fid: pytomlpp.dump(out_dict, toml_fid) return path","title":"TOMLHandler"},{"location":"reference/spock/handlers/#ancestors-in-mro_2","text":"spock.handlers.Handler abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/handlers/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/spock/handlers/#write_extra_info_2","text":"def write_extra_info ( path , info_dict ) Writes extra info to commented newlines Args : path: path to write out info_dict: info payload to write Returns : ??? example \"View Source\" @staticmethod def write_extra_info(path, info_dict): \"\"\"Writes extra info to commented newlines *Args*: path: path to write out info_dict: info payload to write *Returns*: \"\"\" # Write the commented info as new lines with open(path, \"w+\") as fid: # Write a spock header fid.write(f\"# Spock Version: {__version__}\\n\") # Write info dict if not None if info_dict is not None: for k, v in info_dict.items(): fid.write(f\"{k}: {v}\\n\") fid.write(\"\\n\")","title":"write_extra_info"},{"location":"reference/spock/handlers/#methods_2","text":"","title":"Methods"},{"location":"reference/spock/handlers/#load_2","text":"def load ( self , path : str , s3_config = None ) -> Dict Load function for file type This handles s3 path conversion for all handler types pre load call Args : path: path to file s3_config: optional s3 config object if using s3 storage Returns : dictionary of read file ??? example \"View Source\" def load(self, path: str, s3_config=None) -> typing.Dict: \"\"\"Load function for file type This handles s3 path conversion for all handler types pre load call *Args*: path: path to file s3_config: optional s3 config object if using s3 storage *Returns*: dictionary of read file \"\"\" path = self._handle_possible_s3_load_path(path=path, s3_config=s3_config) return self._load(path=path)","title":"load"},{"location":"reference/spock/handlers/#save_2","text":"def save ( self , out_dict : Dict , info_dict : Union [ Dict , NoneType ], path : str , name : str , create_path : bool = False , s3_config = None ) Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload Args : out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage Returns : ??? example \"View Source\" def save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str, name: str, create_path: bool = False, s3_config=None, ): \"\"\"Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage *Returns*: \"\"\" write_path, is_s3 = self._handle_possible_s3_save_path( path=path, name=name, create_path=create_path, s3_config=s3_config ) write_path = self._save(out_dict=out_dict, info_dict=info_dict, path=write_path) # After write check if it needs to be pushed to S3 if is_s3: try: from spock.addons.s3.utils import handle_s3_save_path handle_s3_save_path( temp_path=write_path, s3_path=path, name=name, s3_config=s3_config ) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// save path\")","title":"save"},{"location":"reference/spock/handlers/#yamlhandler","text":"class YAMLHandler ( / , * args , ** kwargs ) ??? example \"View Source\" class YAMLHandler(Handler): \"\"\"YAML class for loading YAML config files Base YAML class \"\"\" # override default SafeLoader behavior to correctly # interpret 1e1 (as opposed to 1.e+1) as 10 # https://stackoverflow.com/questions/30458977/yaml-loads-5e-6-as-string-and-not-a-number/30462009#30462009 yaml.SafeLoader.add_implicit_resolver( \"tag:yaml.org,2002:float\", re.compile( \"\"\"^(?: [-+]?(?:[0-9][0-9_]*)\\\\.[0-9_]*(?:[eE][-+]?[0-9]+)? |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+) |\\\\.[0-9_]+(?:[eE][-+][0-9]+)? |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\\\.[0-9_]* |[-+]?\\\\.(?:inf|Inf|INF) |\\\\.(?:nan|NaN|NAN))$\"\"\", re.X, ), list(\"-+0123456789.\"), ) def _load(self, path: str) -> typing.Dict: \"\"\"YAML load function *Args*: path: path to YAML file *Returns*: base_payload: dictionary of read file \"\"\" file_contents = open(path, \"r\").read() file_contents = re.sub(r\"--([a-zA-Z0-9_]*)\", r\"\\g<1>: True\", file_contents) base_payload = yaml.safe_load(file_contents) return base_payload def _save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str ): \"\"\"Write function for YAML type *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out *Returns*: \"\"\" # First write the commented info self.write_extra_info(path=path, info_dict=info_dict) # Remove aliases in YAML dump yaml.Dumper.ignore_aliases = lambda *args: True with open(path, \"a\") as yaml_fid: yaml.safe_dump(out_dict, yaml_fid, default_flow_style=False) return path","title":"YAMLHandler"},{"location":"reference/spock/handlers/#ancestors-in-mro_3","text":"spock.handlers.Handler abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/handlers/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/spock/handlers/#write_extra_info_3","text":"def write_extra_info ( path , info_dict ) Writes extra info to commented newlines Args : path: path to write out info_dict: info payload to write Returns : ??? example \"View Source\" @staticmethod def write_extra_info(path, info_dict): \"\"\"Writes extra info to commented newlines *Args*: path: path to write out info_dict: info payload to write *Returns*: \"\"\" # Write the commented info as new lines with open(path, \"w+\") as fid: # Write a spock header fid.write(f\"# Spock Version: {__version__}\\n\") # Write info dict if not None if info_dict is not None: for k, v in info_dict.items(): fid.write(f\"{k}: {v}\\n\") fid.write(\"\\n\")","title":"write_extra_info"},{"location":"reference/spock/handlers/#methods_3","text":"","title":"Methods"},{"location":"reference/spock/handlers/#load_3","text":"def load ( self , path : str , s3_config = None ) -> Dict Load function for file type This handles s3 path conversion for all handler types pre load call Args : path: path to file s3_config: optional s3 config object if using s3 storage Returns : dictionary of read file ??? example \"View Source\" def load(self, path: str, s3_config=None) -> typing.Dict: \"\"\"Load function for file type This handles s3 path conversion for all handler types pre load call *Args*: path: path to file s3_config: optional s3 config object if using s3 storage *Returns*: dictionary of read file \"\"\" path = self._handle_possible_s3_load_path(path=path, s3_config=s3_config) return self._load(path=path)","title":"load"},{"location":"reference/spock/handlers/#save_3","text":"def save ( self , out_dict : Dict , info_dict : Union [ Dict , NoneType ], path : str , name : str , create_path : bool = False , s3_config = None ) Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload Args : out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage Returns : ??? example \"View Source\" def save( self, out_dict: typing.Dict, info_dict: typing.Optional[typing.Dict], path: str, name: str, create_path: bool = False, s3_config=None, ): \"\"\"Write function for file type This will handle local or s3 writes with the boolean is_s3 flag. If detected it will conditionally import the necessary addons to handle the upload *Args*: out_dict: payload to write info_dict: info payload to write path: path to write out name: spock generated file name create_path: boolean to create the path if non-existent (for non S3) s3_config: optional s3 config object if using s3 storage *Returns*: \"\"\" write_path, is_s3 = self._handle_possible_s3_save_path( path=path, name=name, create_path=create_path, s3_config=s3_config ) write_path = self._save(out_dict=out_dict, info_dict=info_dict, path=write_path) # After write check if it needs to be pushed to S3 if is_s3: try: from spock.addons.s3.utils import handle_s3_save_path handle_s3_save_path( temp_path=write_path, s3_path=path, name=name, s3_config=s3_config ) except ImportError: print(\"Error importing spock s3 utils after detecting s3:// save path\")","title":"save"},{"location":"reference/spock/utils/","text":"Module spock.utils Utility functions for Spock None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Utility functions for Spock\"\"\" import ast import os import re import socket import subprocess import sys from enum import EnumMeta from time import localtime, strftime from warnings import warn import attr import git minor = sys.version_info.minor if minor < 7: from typing import GenericMeta as _GenericAlias else: from typing import _GenericAlias from typing import Union def check_path_s3(path: str) -> bool: \"\"\"Checks the given path to see if it matches the s3:// regex *Args*: path: a spock config path *Returns*: boolean of regex match \"\"\" # Make a case insensitive s3 regex with single or double forward slash (due to posix stripping) s3_regex = re.compile(r\"(?i)^s3://?\").search(path) # If it returns an object then the path is an s3 style reference return s3_regex is not None def _is_spock_instance(__obj: object): \"\"\"Checks if the object is a @spock decorated class Private interface that checks to see if the object passed in is registered within the spock module and also is a class with attrs attributes (__attrs_attrs__) *Args*: __obj: class to inspect *Returns*: bool \"\"\" return (__obj.__module__ == \"spock.backend.config\") and attr.has(__obj) def make_argument(arg_name, arg_type, parser): \"\"\"Make argparser argument based on type Based on the type passed in handle the creation of the argparser argument so that overrides will have the correct behavior when set *Args*: arg_name: name for the argument arg_type: type of the argument parser: current parser *Returns*: parser: updated argparser \"\"\" # For generic alias we take the input string and use a custom type callable to convert if isinstance(arg_type, _GenericAlias): parser.add_argument(arg_name, required=False, type=_handle_generic_type_args) # For Unions -- python 3.6 can't deal with them correctly -- use the same ast method that generics require elif ( hasattr(arg_type, \"__origin__\") and (arg_type.__origin__ is Union) and (minor < 7) ): parser.add_argument(arg_name, required=False, type=_handle_generic_type_args) # For choice enums we need to check a few things first elif isinstance(arg_type, EnumMeta): type_set = list({type(val.value) for val in arg_type})[0] # if this is an enum of a class switch the type to str as this is how it gets matched type_set = str if type_set.__name__ == \"type\" else type_set parser.add_argument(arg_name, required=False, type=type_set) # For booleans we map to store true elif arg_type == bool: parser.add_argument(arg_name, required=False, action=\"store_true\") # Else we are a simple base type which we can cast to else: parser.add_argument(arg_name, required=False, type=arg_type) return parser def _handle_generic_type_args(val): \"\"\"Evaluates a string containing a Python literal Seeing a list and tuple types will come in as string literal format, use ast to get the actual type *Args*: val: string literal *Returns*: the underlying string literal type \"\"\" return ast.literal_eval(val) def add_info(): \"\"\"Adds extra information to the output dictionary *Args*: *Returns*: out_dict: output dictionary \"\"\" out_dict = {} out_dict = add_generic_info(out_dict) out_dict = add_repo_info(out_dict) return out_dict def make_blank_git(out_dict): \"\"\"Adds blank git info *Args*: out_dict: current output dictionary *Returns*: out_dict: output dictionary with added git info \"\"\" for key in (\"BRANCH\", \"COMMIT SHA\", \"STATUS\", \"ORIGIN\"): out_dict.update({f\"# Git {key}\": \"UNKNOWN\"}) return out_dict def add_repo_info(out_dict): \"\"\"Adds GIT information to the output dictionary *Args*: out_dict: output dictionary *Returns*: out_dict: output dictionary \"\"\" try: # pragma: no cover # Assume we are working out of a repo repo = git.Repo(os.getcwd(), search_parent_directories=True) # Check if we are really in a detached head state as later info will fail if we are if minor < 7: head_result = subprocess.run( \"git rev-parse --abbrev-ref --symbolic-full-name HEAD\", stdout=subprocess.PIPE, shell=True, check=False, ) else: head_result = subprocess.run( \"git rev-parse --abbrev-ref --symbolic-full-name HEAD\", capture_output=True, shell=True, check=False, ) if head_result.stdout.decode().rstrip(\"\\n\") == \"HEAD\": out_dict = make_blank_git(out_dict) else: out_dict.update({\"# Git Branch\": repo.active_branch.name}) out_dict.update({\"# Git Commit\": repo.active_branch.commit.hexsha}) out_dict.update( {\"# Git Date\": repo.active_branch.commit.committed_datetime} ) if ( len(repo.untracked_files) > 0 or len(repo.active_branch.commit.diff(None)) > 0 ): git_status = \"DIRTY\" else: git_status = \"CLEAN\" out_dict.update({\"# Git Status\": git_status}) out_dict.update( {\"# Git Origin\": repo.active_branch.commit.repo.remotes.origin.url} ) except git.InvalidGitRepositoryError: # pragma: no cover # But it's okay if we are not out_dict = make_blank_git(out_dict) return out_dict def add_generic_info(out_dict): \"\"\"Adds date, fqdn information to the output dictionary *Args*: out_dict: output dictionary *Returns*: out_dict: output dictionary \"\"\" out_dict.update({\"# Machine FQDN\": socket.getfqdn()}) out_dict.update({\"# Python Executable\": sys.executable}) out_dict.update( { \"# Python Version\": f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\" } ) out_dict.update({\"# Python Script\": os.path.realpath(sys.argv[0])}) out_dict.update({\"# Run Date\": strftime(\"%Y-%m-%d\", localtime())}) out_dict.update({\"# Run Time\": strftime(\"%H:%M:%S\", localtime())}) # Make a best effort to determine if run in a container out_dict.update({\"# Run w/ Docker\": str(_maybe_docker())}) # Make a best effort to determine if run in a container via k8s out_dict.update({\"# Run w/ Kubernetes\": str(_maybe_k8s())}) return out_dict def _maybe_docker(cgroup_path=\"/proc/self/cgroup\"): \"\"\"Make a best effort to determine if run in a docker container *Args*: cgroup_path: path to cgroup file *Returns*: boolean of best effort docker determination \"\"\" # A few options seem to be at play here: # 1. Check for /.dockerenv -- docker should create this is any container bool_env = os.path.exists(\"/.dockerenv\") # 2. Check /proc/self/cgroup for \"docker\" # https://stackoverflow.com/a/48710609 bool_cgroup = os.path.isfile(cgroup_path) and any( \"docker\" in line for line in open(cgroup_path) ) return bool_env or bool_cgroup def _maybe_k8s(cgroup_path=\"/proc/self/cgroup\"): \"\"\"Make a best effort to determine if run in a container via k8s *Args*: cgroup_path: path to cgroup file *Returns*: boolean of best effort k8s determination \"\"\" # A few options seem to be at play here: # 1. Check for KUBERNETES_SERVICE_HOST -- kublet should add this to every running pod bool_env = os.environ.get(\"KUBERNETES_SERVICE_HOST\") is not None # 2. Similar to docker check /proc/self/cgroup for \"kubepods\" # https://stackoverflow.com/a/48710609 bool_cgroup = os.path.isfile(cgroup_path) and any( \"kubepods\" in line for line in open(cgroup_path) ) return bool_env or bool_cgroup def deep_payload_update(source, updates): \"\"\"Deeply updates a dictionary Iterates through a dictionary recursively to update individual values within a possibly nested dictionary of dictionaries -- creates a dictionary if empty and trying to recurse *Args*: source: source dictionary updates: updates to the dictionary *Returns*: source: updated version of the source dictionary \"\"\" for k, v in updates.items(): if isinstance(v, dict) and v: source_dict = {} if source.get(k) is None else source.get(k) updated_dict = deep_payload_update(source_dict, v) if updated_dict: source[k] = updated_dict else: source[k] = v return source def check_payload_overwrite(payload, updates, configs, overwrite=\"\"): \"\"\"Warns when parameters are overwritten across payloads as order will matter *Args*: payload: current payload payload_update: update to add to payload configs: config path overwrite: name of parent *Returns*: \"\"\" for k, v in updates.items(): if isinstance(v, dict) and v: overwrite += k + \":\" current_payload = {} if payload.get(k) is None else payload.get(k) check_payload_overwrite(current_payload, v, configs, overwrite=overwrite) else: if k in payload: warn( f\"Overriding an already set parameter {overwrite + k} from {configs}\\n\" f\"Be aware that value precedence is set by the order of the config files (last to load)...\", SyntaxWarning, ) Variables minor Functions add_generic_info def add_generic_info ( out_dict ) Adds date, fqdn information to the output dictionary Args : out_dict: output dictionary Returns : out_dict: output dictionary ??? example \"View Source\" def add_generic_info(out_dict): \"\"\"Adds date, fqdn information to the output dictionary *Args*: out_dict: output dictionary *Returns*: out_dict: output dictionary \"\"\" out_dict.update({\"# Machine FQDN\": socket.getfqdn()}) out_dict.update({\"# Python Executable\": sys.executable}) out_dict.update( { \"# Python Version\": f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\" } ) out_dict.update({\"# Python Script\": os.path.realpath(sys.argv[0])}) out_dict.update({\"# Run Date\": strftime(\"%Y-%m-%d\", localtime())}) out_dict.update({\"# Run Time\": strftime(\"%H:%M:%S\", localtime())}) # Make a best effort to determine if run in a container out_dict.update({\"# Run w/ Docker\": str(_maybe_docker())}) # Make a best effort to determine if run in a container via k8s out_dict.update({\"# Run w/ Kubernetes\": str(_maybe_k8s())}) return out_dict add_info def add_info ( ) Adds extra information to the output dictionary Args : Returns : out_dict: output dictionary ??? example \"View Source\" def add_info(): \"\"\"Adds extra information to the output dictionary *Args*: *Returns*: out_dict: output dictionary \"\"\" out_dict = {} out_dict = add_generic_info(out_dict) out_dict = add_repo_info(out_dict) return out_dict add_repo_info def add_repo_info ( out_dict ) Adds GIT information to the output dictionary Args : out_dict: output dictionary Returns : out_dict: output dictionary ??? example \"View Source\" def add_repo_info(out_dict): \"\"\"Adds GIT information to the output dictionary *Args*: out_dict: output dictionary *Returns*: out_dict: output dictionary \"\"\" try: # pragma: no cover # Assume we are working out of a repo repo = git.Repo(os.getcwd(), search_parent_directories=True) # Check if we are really in a detached head state as later info will fail if we are if minor < 7: head_result = subprocess.run( \"git rev-parse --abbrev-ref --symbolic-full-name HEAD\", stdout=subprocess.PIPE, shell=True, check=False, ) else: head_result = subprocess.run( \"git rev-parse --abbrev-ref --symbolic-full-name HEAD\", capture_output=True, shell=True, check=False, ) if head_result.stdout.decode().rstrip(\"\\n\") == \"HEAD\": out_dict = make_blank_git(out_dict) else: out_dict.update({\"# Git Branch\": repo.active_branch.name}) out_dict.update({\"# Git Commit\": repo.active_branch.commit.hexsha}) out_dict.update( {\"# Git Date\": repo.active_branch.commit.committed_datetime} ) if ( len(repo.untracked_files) > 0 or len(repo.active_branch.commit.diff(None)) > 0 ): git_status = \"DIRTY\" else: git_status = \"CLEAN\" out_dict.update({\"# Git Status\": git_status}) out_dict.update( {\"# Git Origin\": repo.active_branch.commit.repo.remotes.origin.url} ) except git.InvalidGitRepositoryError: # pragma: no cover # But it's okay if we are not out_dict = make_blank_git(out_dict) return out_dict check_path_s3 def check_path_s3 ( path : str ) -> bool Checks the given path to see if it matches the s3:// regex Args : path: a spock config path Returns : boolean of regex match ??? example \"View Source\" def check_path_s3(path: str) -> bool: \"\"\"Checks the given path to see if it matches the s3:// regex *Args*: path: a spock config path *Returns*: boolean of regex match \"\"\" # Make a case insensitive s3 regex with single or double forward slash (due to posix stripping) s3_regex = re.compile(r\"(?i)^s3://?\").search(path) # If it returns an object then the path is an s3 style reference return s3_regex is not None check_payload_overwrite def check_payload_overwrite ( payload , updates , configs , overwrite = '' ) Warns when parameters are overwritten across payloads as order will matter Args : payload: current payload payload_update: update to add to payload configs: config path overwrite: name of parent Returns : ??? example \"View Source\" def check_payload_overwrite(payload, updates, configs, overwrite=\"\"): \"\"\"Warns when parameters are overwritten across payloads as order will matter *Args*: payload: current payload payload_update: update to add to payload configs: config path overwrite: name of parent *Returns*: \"\"\" for k, v in updates.items(): if isinstance(v, dict) and v: overwrite += k + \":\" current_payload = {} if payload.get(k) is None else payload.get(k) check_payload_overwrite(current_payload, v, configs, overwrite=overwrite) else: if k in payload: warn( f\"Overriding an already set parameter {overwrite + k} from {configs}\\n\" f\"Be aware that value precedence is set by the order of the config files (last to load)...\", SyntaxWarning, ) deep_payload_update def deep_payload_update ( source , updates ) Deeply updates a dictionary Iterates through a dictionary recursively to update individual values within a possibly nested dictionary of dictionaries -- creates a dictionary if empty and trying to recurse Args : source: source dictionary updates: updates to the dictionary Returns : source: updated version of the source dictionary ??? example \"View Source\" def deep_payload_update(source, updates): \"\"\"Deeply updates a dictionary Iterates through a dictionary recursively to update individual values within a possibly nested dictionary of dictionaries -- creates a dictionary if empty and trying to recurse *Args*: source: source dictionary updates: updates to the dictionary *Returns*: source: updated version of the source dictionary \"\"\" for k, v in updates.items(): if isinstance(v, dict) and v: source_dict = {} if source.get(k) is None else source.get(k) updated_dict = deep_payload_update(source_dict, v) if updated_dict: source[k] = updated_dict else: source[k] = v return source make_argument def make_argument ( arg_name , arg_type , parser ) Make argparser argument based on type Based on the type passed in handle the creation of the argparser argument so that overrides will have the correct behavior when set Args : arg_name: name for the argument arg_type: type of the argument parser: current parser Returns : parser: updated argparser ??? example \"View Source\" def make_argument(arg_name, arg_type, parser): \"\"\"Make argparser argument based on type Based on the type passed in handle the creation of the argparser argument so that overrides will have the correct behavior when set *Args*: arg_name: name for the argument arg_type: type of the argument parser: current parser *Returns*: parser: updated argparser \"\"\" # For generic alias we take the input string and use a custom type callable to convert if isinstance(arg_type, _GenericAlias): parser.add_argument(arg_name, required=False, type=_handle_generic_type_args) # For Unions -- python 3.6 can't deal with them correctly -- use the same ast method that generics require elif ( hasattr(arg_type, \"__origin__\") and (arg_type.__origin__ is Union) and (minor < 7) ): parser.add_argument(arg_name, required=False, type=_handle_generic_type_args) # For choice enums we need to check a few things first elif isinstance(arg_type, EnumMeta): type_set = list({type(val.value) for val in arg_type})[0] # if this is an enum of a class switch the type to str as this is how it gets matched type_set = str if type_set.__name__ == \"type\" else type_set parser.add_argument(arg_name, required=False, type=type_set) # For booleans we map to store true elif arg_type == bool: parser.add_argument(arg_name, required=False, action=\"store_true\") # Else we are a simple base type which we can cast to else: parser.add_argument(arg_name, required=False, type=arg_type) return parser make_blank_git def make_blank_git ( out_dict ) Adds blank git info Args : out_dict: current output dictionary Returns : out_dict: output dictionary with added git info ??? example \"View Source\" def make_blank_git(out_dict): \"\"\"Adds blank git info *Args*: out_dict: current output dictionary *Returns*: out_dict: output dictionary with added git info \"\"\" for key in (\"BRANCH\", \"COMMIT SHA\", \"STATUS\", \"ORIGIN\"): out_dict.update({f\"# Git {key}\": \"UNKNOWN\"}) return out_dict","title":"Utils"},{"location":"reference/spock/utils/#module-spockutils","text":"Utility functions for Spock None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Utility functions for Spock\"\"\" import ast import os import re import socket import subprocess import sys from enum import EnumMeta from time import localtime, strftime from warnings import warn import attr import git minor = sys.version_info.minor if minor < 7: from typing import GenericMeta as _GenericAlias else: from typing import _GenericAlias from typing import Union def check_path_s3(path: str) -> bool: \"\"\"Checks the given path to see if it matches the s3:// regex *Args*: path: a spock config path *Returns*: boolean of regex match \"\"\" # Make a case insensitive s3 regex with single or double forward slash (due to posix stripping) s3_regex = re.compile(r\"(?i)^s3://?\").search(path) # If it returns an object then the path is an s3 style reference return s3_regex is not None def _is_spock_instance(__obj: object): \"\"\"Checks if the object is a @spock decorated class Private interface that checks to see if the object passed in is registered within the spock module and also is a class with attrs attributes (__attrs_attrs__) *Args*: __obj: class to inspect *Returns*: bool \"\"\" return (__obj.__module__ == \"spock.backend.config\") and attr.has(__obj) def make_argument(arg_name, arg_type, parser): \"\"\"Make argparser argument based on type Based on the type passed in handle the creation of the argparser argument so that overrides will have the correct behavior when set *Args*: arg_name: name for the argument arg_type: type of the argument parser: current parser *Returns*: parser: updated argparser \"\"\" # For generic alias we take the input string and use a custom type callable to convert if isinstance(arg_type, _GenericAlias): parser.add_argument(arg_name, required=False, type=_handle_generic_type_args) # For Unions -- python 3.6 can't deal with them correctly -- use the same ast method that generics require elif ( hasattr(arg_type, \"__origin__\") and (arg_type.__origin__ is Union) and (minor < 7) ): parser.add_argument(arg_name, required=False, type=_handle_generic_type_args) # For choice enums we need to check a few things first elif isinstance(arg_type, EnumMeta): type_set = list({type(val.value) for val in arg_type})[0] # if this is an enum of a class switch the type to str as this is how it gets matched type_set = str if type_set.__name__ == \"type\" else type_set parser.add_argument(arg_name, required=False, type=type_set) # For booleans we map to store true elif arg_type == bool: parser.add_argument(arg_name, required=False, action=\"store_true\") # Else we are a simple base type which we can cast to else: parser.add_argument(arg_name, required=False, type=arg_type) return parser def _handle_generic_type_args(val): \"\"\"Evaluates a string containing a Python literal Seeing a list and tuple types will come in as string literal format, use ast to get the actual type *Args*: val: string literal *Returns*: the underlying string literal type \"\"\" return ast.literal_eval(val) def add_info(): \"\"\"Adds extra information to the output dictionary *Args*: *Returns*: out_dict: output dictionary \"\"\" out_dict = {} out_dict = add_generic_info(out_dict) out_dict = add_repo_info(out_dict) return out_dict def make_blank_git(out_dict): \"\"\"Adds blank git info *Args*: out_dict: current output dictionary *Returns*: out_dict: output dictionary with added git info \"\"\" for key in (\"BRANCH\", \"COMMIT SHA\", \"STATUS\", \"ORIGIN\"): out_dict.update({f\"# Git {key}\": \"UNKNOWN\"}) return out_dict def add_repo_info(out_dict): \"\"\"Adds GIT information to the output dictionary *Args*: out_dict: output dictionary *Returns*: out_dict: output dictionary \"\"\" try: # pragma: no cover # Assume we are working out of a repo repo = git.Repo(os.getcwd(), search_parent_directories=True) # Check if we are really in a detached head state as later info will fail if we are if minor < 7: head_result = subprocess.run( \"git rev-parse --abbrev-ref --symbolic-full-name HEAD\", stdout=subprocess.PIPE, shell=True, check=False, ) else: head_result = subprocess.run( \"git rev-parse --abbrev-ref --symbolic-full-name HEAD\", capture_output=True, shell=True, check=False, ) if head_result.stdout.decode().rstrip(\"\\n\") == \"HEAD\": out_dict = make_blank_git(out_dict) else: out_dict.update({\"# Git Branch\": repo.active_branch.name}) out_dict.update({\"# Git Commit\": repo.active_branch.commit.hexsha}) out_dict.update( {\"# Git Date\": repo.active_branch.commit.committed_datetime} ) if ( len(repo.untracked_files) > 0 or len(repo.active_branch.commit.diff(None)) > 0 ): git_status = \"DIRTY\" else: git_status = \"CLEAN\" out_dict.update({\"# Git Status\": git_status}) out_dict.update( {\"# Git Origin\": repo.active_branch.commit.repo.remotes.origin.url} ) except git.InvalidGitRepositoryError: # pragma: no cover # But it's okay if we are not out_dict = make_blank_git(out_dict) return out_dict def add_generic_info(out_dict): \"\"\"Adds date, fqdn information to the output dictionary *Args*: out_dict: output dictionary *Returns*: out_dict: output dictionary \"\"\" out_dict.update({\"# Machine FQDN\": socket.getfqdn()}) out_dict.update({\"# Python Executable\": sys.executable}) out_dict.update( { \"# Python Version\": f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\" } ) out_dict.update({\"# Python Script\": os.path.realpath(sys.argv[0])}) out_dict.update({\"# Run Date\": strftime(\"%Y-%m-%d\", localtime())}) out_dict.update({\"# Run Time\": strftime(\"%H:%M:%S\", localtime())}) # Make a best effort to determine if run in a container out_dict.update({\"# Run w/ Docker\": str(_maybe_docker())}) # Make a best effort to determine if run in a container via k8s out_dict.update({\"# Run w/ Kubernetes\": str(_maybe_k8s())}) return out_dict def _maybe_docker(cgroup_path=\"/proc/self/cgroup\"): \"\"\"Make a best effort to determine if run in a docker container *Args*: cgroup_path: path to cgroup file *Returns*: boolean of best effort docker determination \"\"\" # A few options seem to be at play here: # 1. Check for /.dockerenv -- docker should create this is any container bool_env = os.path.exists(\"/.dockerenv\") # 2. Check /proc/self/cgroup for \"docker\" # https://stackoverflow.com/a/48710609 bool_cgroup = os.path.isfile(cgroup_path) and any( \"docker\" in line for line in open(cgroup_path) ) return bool_env or bool_cgroup def _maybe_k8s(cgroup_path=\"/proc/self/cgroup\"): \"\"\"Make a best effort to determine if run in a container via k8s *Args*: cgroup_path: path to cgroup file *Returns*: boolean of best effort k8s determination \"\"\" # A few options seem to be at play here: # 1. Check for KUBERNETES_SERVICE_HOST -- kublet should add this to every running pod bool_env = os.environ.get(\"KUBERNETES_SERVICE_HOST\") is not None # 2. Similar to docker check /proc/self/cgroup for \"kubepods\" # https://stackoverflow.com/a/48710609 bool_cgroup = os.path.isfile(cgroup_path) and any( \"kubepods\" in line for line in open(cgroup_path) ) return bool_env or bool_cgroup def deep_payload_update(source, updates): \"\"\"Deeply updates a dictionary Iterates through a dictionary recursively to update individual values within a possibly nested dictionary of dictionaries -- creates a dictionary if empty and trying to recurse *Args*: source: source dictionary updates: updates to the dictionary *Returns*: source: updated version of the source dictionary \"\"\" for k, v in updates.items(): if isinstance(v, dict) and v: source_dict = {} if source.get(k) is None else source.get(k) updated_dict = deep_payload_update(source_dict, v) if updated_dict: source[k] = updated_dict else: source[k] = v return source def check_payload_overwrite(payload, updates, configs, overwrite=\"\"): \"\"\"Warns when parameters are overwritten across payloads as order will matter *Args*: payload: current payload payload_update: update to add to payload configs: config path overwrite: name of parent *Returns*: \"\"\" for k, v in updates.items(): if isinstance(v, dict) and v: overwrite += k + \":\" current_payload = {} if payload.get(k) is None else payload.get(k) check_payload_overwrite(current_payload, v, configs, overwrite=overwrite) else: if k in payload: warn( f\"Overriding an already set parameter {overwrite + k} from {configs}\\n\" f\"Be aware that value precedence is set by the order of the config files (last to load)...\", SyntaxWarning, )","title":"Module spock.utils"},{"location":"reference/spock/utils/#variables","text":"minor","title":"Variables"},{"location":"reference/spock/utils/#functions","text":"","title":"Functions"},{"location":"reference/spock/utils/#add_generic_info","text":"def add_generic_info ( out_dict ) Adds date, fqdn information to the output dictionary Args : out_dict: output dictionary Returns : out_dict: output dictionary ??? example \"View Source\" def add_generic_info(out_dict): \"\"\"Adds date, fqdn information to the output dictionary *Args*: out_dict: output dictionary *Returns*: out_dict: output dictionary \"\"\" out_dict.update({\"# Machine FQDN\": socket.getfqdn()}) out_dict.update({\"# Python Executable\": sys.executable}) out_dict.update( { \"# Python Version\": f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\" } ) out_dict.update({\"# Python Script\": os.path.realpath(sys.argv[0])}) out_dict.update({\"# Run Date\": strftime(\"%Y-%m-%d\", localtime())}) out_dict.update({\"# Run Time\": strftime(\"%H:%M:%S\", localtime())}) # Make a best effort to determine if run in a container out_dict.update({\"# Run w/ Docker\": str(_maybe_docker())}) # Make a best effort to determine if run in a container via k8s out_dict.update({\"# Run w/ Kubernetes\": str(_maybe_k8s())}) return out_dict","title":"add_generic_info"},{"location":"reference/spock/utils/#add_info","text":"def add_info ( ) Adds extra information to the output dictionary Args : Returns : out_dict: output dictionary ??? example \"View Source\" def add_info(): \"\"\"Adds extra information to the output dictionary *Args*: *Returns*: out_dict: output dictionary \"\"\" out_dict = {} out_dict = add_generic_info(out_dict) out_dict = add_repo_info(out_dict) return out_dict","title":"add_info"},{"location":"reference/spock/utils/#add_repo_info","text":"def add_repo_info ( out_dict ) Adds GIT information to the output dictionary Args : out_dict: output dictionary Returns : out_dict: output dictionary ??? example \"View Source\" def add_repo_info(out_dict): \"\"\"Adds GIT information to the output dictionary *Args*: out_dict: output dictionary *Returns*: out_dict: output dictionary \"\"\" try: # pragma: no cover # Assume we are working out of a repo repo = git.Repo(os.getcwd(), search_parent_directories=True) # Check if we are really in a detached head state as later info will fail if we are if minor < 7: head_result = subprocess.run( \"git rev-parse --abbrev-ref --symbolic-full-name HEAD\", stdout=subprocess.PIPE, shell=True, check=False, ) else: head_result = subprocess.run( \"git rev-parse --abbrev-ref --symbolic-full-name HEAD\", capture_output=True, shell=True, check=False, ) if head_result.stdout.decode().rstrip(\"\\n\") == \"HEAD\": out_dict = make_blank_git(out_dict) else: out_dict.update({\"# Git Branch\": repo.active_branch.name}) out_dict.update({\"# Git Commit\": repo.active_branch.commit.hexsha}) out_dict.update( {\"# Git Date\": repo.active_branch.commit.committed_datetime} ) if ( len(repo.untracked_files) > 0 or len(repo.active_branch.commit.diff(None)) > 0 ): git_status = \"DIRTY\" else: git_status = \"CLEAN\" out_dict.update({\"# Git Status\": git_status}) out_dict.update( {\"# Git Origin\": repo.active_branch.commit.repo.remotes.origin.url} ) except git.InvalidGitRepositoryError: # pragma: no cover # But it's okay if we are not out_dict = make_blank_git(out_dict) return out_dict","title":"add_repo_info"},{"location":"reference/spock/utils/#check_path_s3","text":"def check_path_s3 ( path : str ) -> bool Checks the given path to see if it matches the s3:// regex Args : path: a spock config path Returns : boolean of regex match ??? example \"View Source\" def check_path_s3(path: str) -> bool: \"\"\"Checks the given path to see if it matches the s3:// regex *Args*: path: a spock config path *Returns*: boolean of regex match \"\"\" # Make a case insensitive s3 regex with single or double forward slash (due to posix stripping) s3_regex = re.compile(r\"(?i)^s3://?\").search(path) # If it returns an object then the path is an s3 style reference return s3_regex is not None","title":"check_path_s3"},{"location":"reference/spock/utils/#check_payload_overwrite","text":"def check_payload_overwrite ( payload , updates , configs , overwrite = '' ) Warns when parameters are overwritten across payloads as order will matter Args : payload: current payload payload_update: update to add to payload configs: config path overwrite: name of parent Returns : ??? example \"View Source\" def check_payload_overwrite(payload, updates, configs, overwrite=\"\"): \"\"\"Warns when parameters are overwritten across payloads as order will matter *Args*: payload: current payload payload_update: update to add to payload configs: config path overwrite: name of parent *Returns*: \"\"\" for k, v in updates.items(): if isinstance(v, dict) and v: overwrite += k + \":\" current_payload = {} if payload.get(k) is None else payload.get(k) check_payload_overwrite(current_payload, v, configs, overwrite=overwrite) else: if k in payload: warn( f\"Overriding an already set parameter {overwrite + k} from {configs}\\n\" f\"Be aware that value precedence is set by the order of the config files (last to load)...\", SyntaxWarning, )","title":"check_payload_overwrite"},{"location":"reference/spock/utils/#deep_payload_update","text":"def deep_payload_update ( source , updates ) Deeply updates a dictionary Iterates through a dictionary recursively to update individual values within a possibly nested dictionary of dictionaries -- creates a dictionary if empty and trying to recurse Args : source: source dictionary updates: updates to the dictionary Returns : source: updated version of the source dictionary ??? example \"View Source\" def deep_payload_update(source, updates): \"\"\"Deeply updates a dictionary Iterates through a dictionary recursively to update individual values within a possibly nested dictionary of dictionaries -- creates a dictionary if empty and trying to recurse *Args*: source: source dictionary updates: updates to the dictionary *Returns*: source: updated version of the source dictionary \"\"\" for k, v in updates.items(): if isinstance(v, dict) and v: source_dict = {} if source.get(k) is None else source.get(k) updated_dict = deep_payload_update(source_dict, v) if updated_dict: source[k] = updated_dict else: source[k] = v return source","title":"deep_payload_update"},{"location":"reference/spock/utils/#make_argument","text":"def make_argument ( arg_name , arg_type , parser ) Make argparser argument based on type Based on the type passed in handle the creation of the argparser argument so that overrides will have the correct behavior when set Args : arg_name: name for the argument arg_type: type of the argument parser: current parser Returns : parser: updated argparser ??? example \"View Source\" def make_argument(arg_name, arg_type, parser): \"\"\"Make argparser argument based on type Based on the type passed in handle the creation of the argparser argument so that overrides will have the correct behavior when set *Args*: arg_name: name for the argument arg_type: type of the argument parser: current parser *Returns*: parser: updated argparser \"\"\" # For generic alias we take the input string and use a custom type callable to convert if isinstance(arg_type, _GenericAlias): parser.add_argument(arg_name, required=False, type=_handle_generic_type_args) # For Unions -- python 3.6 can't deal with them correctly -- use the same ast method that generics require elif ( hasattr(arg_type, \"__origin__\") and (arg_type.__origin__ is Union) and (minor < 7) ): parser.add_argument(arg_name, required=False, type=_handle_generic_type_args) # For choice enums we need to check a few things first elif isinstance(arg_type, EnumMeta): type_set = list({type(val.value) for val in arg_type})[0] # if this is an enum of a class switch the type to str as this is how it gets matched type_set = str if type_set.__name__ == \"type\" else type_set parser.add_argument(arg_name, required=False, type=type_set) # For booleans we map to store true elif arg_type == bool: parser.add_argument(arg_name, required=False, action=\"store_true\") # Else we are a simple base type which we can cast to else: parser.add_argument(arg_name, required=False, type=arg_type) return parser","title":"make_argument"},{"location":"reference/spock/utils/#make_blank_git","text":"def make_blank_git ( out_dict ) Adds blank git info Args : out_dict: current output dictionary Returns : out_dict: output dictionary with added git info ??? example \"View Source\" def make_blank_git(out_dict): \"\"\"Adds blank git info *Args*: out_dict: current output dictionary *Returns*: out_dict: output dictionary with added git info \"\"\" for key in (\"BRANCH\", \"COMMIT SHA\", \"STATUS\", \"ORIGIN\"): out_dict.update({f\"# Git {key}\": \"UNKNOWN\"}) return out_dict","title":"make_blank_git"},{"location":"reference/spock/addons/","text":"Module spock.addons Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\" Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md \"\"\" __all__ = [\"s3\", \"tune\"] Sub-modules spock.addons.s3 spock.addons.tune","title":"Index"},{"location":"reference/spock/addons/#module-spockaddons","text":"Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\" Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md \"\"\" __all__ = [\"s3\", \"tune\"]","title":"Module spock.addons"},{"location":"reference/spock/addons/#sub-modules","text":"spock.addons.s3 spock.addons.tune","title":"Sub-modules"},{"location":"reference/spock/addons/s3/","text":"Module spock.addons.s3 Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\" Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md \"\"\" from spock.addons.s3.configs import S3Config, S3DownloadConfig, S3UploadConfig __all__ = [\"configs\", \"utils\", \"S3Config\", \"S3DownloadConfig\", \"S3UploadConfig\"] Sub-modules spock.addons.s3.configs spock.addons.s3.utils Variables configs Classes S3Config class S3Config ( session : boto3 . session . Session , s3_session : Union [ botocore . client . BaseClient , NoneType ] = None , temp_folder : Union [ str , NoneType ] = '/tmp/' , download_config : spock . addons . s3 . configs . S3DownloadConfig = S3DownloadConfig ( VersionId = None , SSECustomerAlgorithm = None , SSECustomerKey = None , SSECustomerKeyMD5 = None , RequestPayer = None , ExpectedBucketOwner = None ), upload_config : spock . addons . s3 . configs . S3UploadConfig = S3UploadConfig ( ACL = None , CacheControl = None , ContentDisposition = None , ContentEncoding = None , ContentLanguage = None , ContentType = None , ExpectedBucketOwner = None , Expires = None , GrantFullControl = None , GrantRead = None , GrantReadACP = None , GrantWriteACP = None , Metadata = None , RequestPayer = None , ServerSideEncryption = None , StorageClass = None , SSECustomerAlgorithm = None , SSECustomerKey = None , SSECustomerKeyMD5 = None , SSEKMSKeyId = None , SSEKMSEncryptionContext = None , Tagging = None , WebsiteRedirectLocation = None ) ) ??? example \"View Source\" class S3Config: \"\"\"Configuration class for S3 support *Attributes*: session: instantiated boto3 session object s3_session: automatically generated s3 client from the boto3 session if not provided kms_arn: AWS KMS key ARN (optional) temp_folder: temporary working folder to write/read spock configuration(s) (optional: defaults to /tmp) download_config: S3DownloadConfig for extra download configs (optional) upload_config: S3UploadConfig for extra upload configs (optional) \"\"\" session: boto3.Session # s3_session: BaseClient = attr.ib(init=False) s3_session: Optional[BaseClient] = None temp_folder: Optional[str] = \"/tmp/\" download_config: S3DownloadConfig = S3DownloadConfig() upload_config: S3UploadConfig = S3UploadConfig() def __attrs_post_init__(self): if self.s3_session is None: self.s3_session = self.session.client(\"s3\") S3DownloadConfig class S3DownloadConfig ( * , VersionId : str = None , SSECustomerAlgorithm : str = None , SSECustomerKey : str = None , SSECustomerKeyMD5 : str = None , RequestPayer : str = None , ExpectedBucketOwner : str = None ) S3UploadConfig class S3UploadConfig ( * , ACL : str = None , CacheControl : str = None , ContentDisposition : str = None , ContentEncoding : str = None , ContentLanguage : str = None , ContentType : str = None , ExpectedBucketOwner : str = None , Expires : str = None , GrantFullControl : str = None , GrantRead : str = None , GrantReadACP : str = None , GrantWriteACP : str = None , Metadata : str = None , RequestPayer : str = None , ServerSideEncryption : str = None , StorageClass : str = None , SSECustomerAlgorithm : str = None , SSECustomerKey : str = None , SSECustomerKeyMD5 : str = None , SSEKMSKeyId : str = None , SSEKMSEncryptionContext : str = None , Tagging : str = None , WebsiteRedirectLocation : str = None )","title":"Index"},{"location":"reference/spock/addons/s3/#module-spockaddonss3","text":"Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\" Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md \"\"\" from spock.addons.s3.configs import S3Config, S3DownloadConfig, S3UploadConfig __all__ = [\"configs\", \"utils\", \"S3Config\", \"S3DownloadConfig\", \"S3UploadConfig\"]","title":"Module spock.addons.s3"},{"location":"reference/spock/addons/s3/#sub-modules","text":"spock.addons.s3.configs spock.addons.s3.utils","title":"Sub-modules"},{"location":"reference/spock/addons/s3/#variables","text":"configs","title":"Variables"},{"location":"reference/spock/addons/s3/#classes","text":"","title":"Classes"},{"location":"reference/spock/addons/s3/#s3config","text":"class S3Config ( session : boto3 . session . Session , s3_session : Union [ botocore . client . BaseClient , NoneType ] = None , temp_folder : Union [ str , NoneType ] = '/tmp/' , download_config : spock . addons . s3 . configs . S3DownloadConfig = S3DownloadConfig ( VersionId = None , SSECustomerAlgorithm = None , SSECustomerKey = None , SSECustomerKeyMD5 = None , RequestPayer = None , ExpectedBucketOwner = None ), upload_config : spock . addons . s3 . configs . S3UploadConfig = S3UploadConfig ( ACL = None , CacheControl = None , ContentDisposition = None , ContentEncoding = None , ContentLanguage = None , ContentType = None , ExpectedBucketOwner = None , Expires = None , GrantFullControl = None , GrantRead = None , GrantReadACP = None , GrantWriteACP = None , Metadata = None , RequestPayer = None , ServerSideEncryption = None , StorageClass = None , SSECustomerAlgorithm = None , SSECustomerKey = None , SSECustomerKeyMD5 = None , SSEKMSKeyId = None , SSEKMSEncryptionContext = None , Tagging = None , WebsiteRedirectLocation = None ) ) ??? example \"View Source\" class S3Config: \"\"\"Configuration class for S3 support *Attributes*: session: instantiated boto3 session object s3_session: automatically generated s3 client from the boto3 session if not provided kms_arn: AWS KMS key ARN (optional) temp_folder: temporary working folder to write/read spock configuration(s) (optional: defaults to /tmp) download_config: S3DownloadConfig for extra download configs (optional) upload_config: S3UploadConfig for extra upload configs (optional) \"\"\" session: boto3.Session # s3_session: BaseClient = attr.ib(init=False) s3_session: Optional[BaseClient] = None temp_folder: Optional[str] = \"/tmp/\" download_config: S3DownloadConfig = S3DownloadConfig() upload_config: S3UploadConfig = S3UploadConfig() def __attrs_post_init__(self): if self.s3_session is None: self.s3_session = self.session.client(\"s3\")","title":"S3Config"},{"location":"reference/spock/addons/s3/#s3downloadconfig","text":"class S3DownloadConfig ( * , VersionId : str = None , SSECustomerAlgorithm : str = None , SSECustomerKey : str = None , SSECustomerKeyMD5 : str = None , RequestPayer : str = None , ExpectedBucketOwner : str = None )","title":"S3DownloadConfig"},{"location":"reference/spock/addons/s3/#s3uploadconfig","text":"class S3UploadConfig ( * , ACL : str = None , CacheControl : str = None , ContentDisposition : str = None , ContentEncoding : str = None , ContentLanguage : str = None , ContentType : str = None , ExpectedBucketOwner : str = None , Expires : str = None , GrantFullControl : str = None , GrantRead : str = None , GrantReadACP : str = None , GrantWriteACP : str = None , Metadata : str = None , RequestPayer : str = None , ServerSideEncryption : str = None , StorageClass : str = None , SSECustomerAlgorithm : str = None , SSECustomerKey : str = None , SSECustomerKeyMD5 : str = None , SSEKMSKeyId : str = None , SSEKMSEncryptionContext : str = None , Tagging : str = None , WebsiteRedirectLocation : str = None )","title":"S3UploadConfig"},{"location":"reference/spock/addons/s3/configs/","text":"Module spock.addons.s3.configs Handles all S3 related configurations None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles all S3 related configurations\"\"\" import attr try: import boto3 from botocore.client import BaseClient from s3transfer.manager import TransferManager except ImportError: print( \"Missing libraries to support S3 functionality. Please re-install spock with the extra s3 dependencies -- \" \"pip install spock-config[s3]\" ) from typing import Optional # Iterate through the allowed download args for S3 and map into optional attr.ib download_attrs = { val: attr.ib( default=None, type=str, validator=attr.validators.optional(attr.validators.instance_of(str)), ) for val in TransferManager.ALLOWED_DOWNLOAD_ARGS } # Make the class dynamically S3DownloadConfig = attr.make_class( name=\"S3DownloadConfig\", attrs=download_attrs, kw_only=True, frozen=True ) # Iterate through the allowed upload args for S3 and map into optional attr.ib upload_attrs = { val: attr.ib( default=None, type=str, validator=attr.validators.optional(attr.validators.instance_of(str)), ) for val in TransferManager.ALLOWED_UPLOAD_ARGS } # Make the class dynamically S3UploadConfig = attr.make_class( name=\"S3UploadConfig\", attrs=upload_attrs, kw_only=True, frozen=True ) @attr.s(auto_attribs=True) class S3Config: \"\"\"Configuration class for S3 support *Attributes*: session: instantiated boto3 session object s3_session: automatically generated s3 client from the boto3 session if not provided kms_arn: AWS KMS key ARN (optional) temp_folder: temporary working folder to write/read spock configuration(s) (optional: defaults to /tmp) download_config: S3DownloadConfig for extra download configs (optional) upload_config: S3UploadConfig for extra upload configs (optional) \"\"\" session: boto3.Session # s3_session: BaseClient = attr.ib(init=False) s3_session: Optional[BaseClient] = None temp_folder: Optional[str] = \"/tmp/\" download_config: S3DownloadConfig = S3DownloadConfig() upload_config: S3UploadConfig = S3UploadConfig() def __attrs_post_init__(self): if self.s3_session is None: self.s3_session = self.session.client(\"s3\") Variables download_attrs upload_attrs Classes S3Config class S3Config ( session : boto3 . session . Session , s3_session : Union [ botocore . client . BaseClient , NoneType ] = None , temp_folder : Union [ str , NoneType ] = '/tmp/' , download_config : spock . addons . s3 . configs . S3DownloadConfig = S3DownloadConfig ( VersionId = None , SSECustomerAlgorithm = None , SSECustomerKey = None , SSECustomerKeyMD5 = None , RequestPayer = None , ExpectedBucketOwner = None ), upload_config : spock . addons . s3 . configs . S3UploadConfig = S3UploadConfig ( ACL = None , CacheControl = None , ContentDisposition = None , ContentEncoding = None , ContentLanguage = None , ContentType = None , ExpectedBucketOwner = None , Expires = None , GrantFullControl = None , GrantRead = None , GrantReadACP = None , GrantWriteACP = None , Metadata = None , RequestPayer = None , ServerSideEncryption = None , StorageClass = None , SSECustomerAlgorithm = None , SSECustomerKey = None , SSECustomerKeyMD5 = None , SSEKMSKeyId = None , SSEKMSEncryptionContext = None , Tagging = None , WebsiteRedirectLocation = None ) ) ??? example \"View Source\" class S3Config: \"\"\"Configuration class for S3 support *Attributes*: session: instantiated boto3 session object s3_session: automatically generated s3 client from the boto3 session if not provided kms_arn: AWS KMS key ARN (optional) temp_folder: temporary working folder to write/read spock configuration(s) (optional: defaults to /tmp) download_config: S3DownloadConfig for extra download configs (optional) upload_config: S3UploadConfig for extra upload configs (optional) \"\"\" session: boto3.Session # s3_session: BaseClient = attr.ib(init=False) s3_session: Optional[BaseClient] = None temp_folder: Optional[str] = \"/tmp/\" download_config: S3DownloadConfig = S3DownloadConfig() upload_config: S3UploadConfig = S3UploadConfig() def __attrs_post_init__(self): if self.s3_session is None: self.s3_session = self.session.client(\"s3\") S3DownloadConfig class S3DownloadConfig ( * , VersionId : str = None , SSECustomerAlgorithm : str = None , SSECustomerKey : str = None , SSECustomerKeyMD5 : str = None , RequestPayer : str = None , ExpectedBucketOwner : str = None ) S3UploadConfig class S3UploadConfig ( * , ACL : str = None , CacheControl : str = None , ContentDisposition : str = None , ContentEncoding : str = None , ContentLanguage : str = None , ContentType : str = None , ExpectedBucketOwner : str = None , Expires : str = None , GrantFullControl : str = None , GrantRead : str = None , GrantReadACP : str = None , GrantWriteACP : str = None , Metadata : str = None , RequestPayer : str = None , ServerSideEncryption : str = None , StorageClass : str = None , SSECustomerAlgorithm : str = None , SSECustomerKey : str = None , SSECustomerKeyMD5 : str = None , SSEKMSKeyId : str = None , SSEKMSEncryptionContext : str = None , Tagging : str = None , WebsiteRedirectLocation : str = None )","title":"Configs"},{"location":"reference/spock/addons/s3/configs/#module-spockaddonss3configs","text":"Handles all S3 related configurations None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles all S3 related configurations\"\"\" import attr try: import boto3 from botocore.client import BaseClient from s3transfer.manager import TransferManager except ImportError: print( \"Missing libraries to support S3 functionality. Please re-install spock with the extra s3 dependencies -- \" \"pip install spock-config[s3]\" ) from typing import Optional # Iterate through the allowed download args for S3 and map into optional attr.ib download_attrs = { val: attr.ib( default=None, type=str, validator=attr.validators.optional(attr.validators.instance_of(str)), ) for val in TransferManager.ALLOWED_DOWNLOAD_ARGS } # Make the class dynamically S3DownloadConfig = attr.make_class( name=\"S3DownloadConfig\", attrs=download_attrs, kw_only=True, frozen=True ) # Iterate through the allowed upload args for S3 and map into optional attr.ib upload_attrs = { val: attr.ib( default=None, type=str, validator=attr.validators.optional(attr.validators.instance_of(str)), ) for val in TransferManager.ALLOWED_UPLOAD_ARGS } # Make the class dynamically S3UploadConfig = attr.make_class( name=\"S3UploadConfig\", attrs=upload_attrs, kw_only=True, frozen=True ) @attr.s(auto_attribs=True) class S3Config: \"\"\"Configuration class for S3 support *Attributes*: session: instantiated boto3 session object s3_session: automatically generated s3 client from the boto3 session if not provided kms_arn: AWS KMS key ARN (optional) temp_folder: temporary working folder to write/read spock configuration(s) (optional: defaults to /tmp) download_config: S3DownloadConfig for extra download configs (optional) upload_config: S3UploadConfig for extra upload configs (optional) \"\"\" session: boto3.Session # s3_session: BaseClient = attr.ib(init=False) s3_session: Optional[BaseClient] = None temp_folder: Optional[str] = \"/tmp/\" download_config: S3DownloadConfig = S3DownloadConfig() upload_config: S3UploadConfig = S3UploadConfig() def __attrs_post_init__(self): if self.s3_session is None: self.s3_session = self.session.client(\"s3\")","title":"Module spock.addons.s3.configs"},{"location":"reference/spock/addons/s3/configs/#variables","text":"download_attrs upload_attrs","title":"Variables"},{"location":"reference/spock/addons/s3/configs/#classes","text":"","title":"Classes"},{"location":"reference/spock/addons/s3/configs/#s3config","text":"class S3Config ( session : boto3 . session . Session , s3_session : Union [ botocore . client . BaseClient , NoneType ] = None , temp_folder : Union [ str , NoneType ] = '/tmp/' , download_config : spock . addons . s3 . configs . S3DownloadConfig = S3DownloadConfig ( VersionId = None , SSECustomerAlgorithm = None , SSECustomerKey = None , SSECustomerKeyMD5 = None , RequestPayer = None , ExpectedBucketOwner = None ), upload_config : spock . addons . s3 . configs . S3UploadConfig = S3UploadConfig ( ACL = None , CacheControl = None , ContentDisposition = None , ContentEncoding = None , ContentLanguage = None , ContentType = None , ExpectedBucketOwner = None , Expires = None , GrantFullControl = None , GrantRead = None , GrantReadACP = None , GrantWriteACP = None , Metadata = None , RequestPayer = None , ServerSideEncryption = None , StorageClass = None , SSECustomerAlgorithm = None , SSECustomerKey = None , SSECustomerKeyMD5 = None , SSEKMSKeyId = None , SSEKMSEncryptionContext = None , Tagging = None , WebsiteRedirectLocation = None ) ) ??? example \"View Source\" class S3Config: \"\"\"Configuration class for S3 support *Attributes*: session: instantiated boto3 session object s3_session: automatically generated s3 client from the boto3 session if not provided kms_arn: AWS KMS key ARN (optional) temp_folder: temporary working folder to write/read spock configuration(s) (optional: defaults to /tmp) download_config: S3DownloadConfig for extra download configs (optional) upload_config: S3UploadConfig for extra upload configs (optional) \"\"\" session: boto3.Session # s3_session: BaseClient = attr.ib(init=False) s3_session: Optional[BaseClient] = None temp_folder: Optional[str] = \"/tmp/\" download_config: S3DownloadConfig = S3DownloadConfig() upload_config: S3UploadConfig = S3UploadConfig() def __attrs_post_init__(self): if self.s3_session is None: self.s3_session = self.session.client(\"s3\")","title":"S3Config"},{"location":"reference/spock/addons/s3/configs/#s3downloadconfig","text":"class S3DownloadConfig ( * , VersionId : str = None , SSECustomerAlgorithm : str = None , SSECustomerKey : str = None , SSECustomerKeyMD5 : str = None , RequestPayer : str = None , ExpectedBucketOwner : str = None )","title":"S3DownloadConfig"},{"location":"reference/spock/addons/s3/configs/#s3uploadconfig","text":"class S3UploadConfig ( * , ACL : str = None , CacheControl : str = None , ContentDisposition : str = None , ContentEncoding : str = None , ContentLanguage : str = None , ContentType : str = None , ExpectedBucketOwner : str = None , Expires : str = None , GrantFullControl : str = None , GrantRead : str = None , GrantReadACP : str = None , GrantWriteACP : str = None , Metadata : str = None , RequestPayer : str = None , ServerSideEncryption : str = None , StorageClass : str = None , SSECustomerAlgorithm : str = None , SSECustomerKey : str = None , SSECustomerKeyMD5 : str = None , SSEKMSKeyId : str = None , SSEKMSEncryptionContext : str = None , Tagging : str = None , WebsiteRedirectLocation : str = None )","title":"S3UploadConfig"},{"location":"reference/spock/addons/s3/utils/","text":"Module spock.addons.s3.utils Handles all S3 related ops -- allows for s3 functionality to be optional to keep req deps light None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles all S3 related ops -- allows for s3 functionality to be optional to keep req deps light\"\"\" import attr try: import boto3 from botocore.client import BaseClient except ImportError: print( \"Missing libraries to support S3 functionality. Please re-install spock with the extra s3 dependencies -- \" \"pip install spock-config[s3]\" ) import os import sys import typing from urllib.parse import urlparse from hurry.filesize import size from spock.addons.s3.configs import S3Config, S3DownloadConfig, S3UploadConfig def handle_s3_load_path(path: str, s3_config: S3Config) -> str: \"\"\"Handles loading from S3 uri Handles downloading file from a given s3 uri to a local temp location and passing the path back to the handler load call *Args*: path: s3 uri path s3_config: s3_config object *Returns*: temp_path: the temporary path of the config file downloaded from s3 \"\"\" if s3_config is None: raise ValueError( \"Load from S3 -- Missing S3Config object which is necessary to handle S3 style paths\" ) bucket, obj, fid = get_s3_bucket_object_name(s3_path=path) # Construct the full temp path temp_path = f\"{s3_config.temp_folder}/{fid}\" # Strip double slashes if exist temp_path = temp_path.replace(r\"//\", r\"/\") temp_path = download_s3( bucket=bucket, obj=obj, temp_path=temp_path, s3_session=s3_config.s3_session, download_config=s3_config.download_config, ) return temp_path def handle_s3_save_path(temp_path: str, s3_path: str, name: str, s3_config: S3Config): \"\"\"Handles saving to S3 uri Points to the local spock configuration file and handles getting it up to S3 *Args*: temp_path: the temporary path the spock config was written out to locally s3_path: base s3 uri name: spock generated filename s3_config: s3_config object *Returns*: \"\"\" if s3_config is None: raise ValueError( \"Save to S3 -- Missing S3Config object which is necessary to handle S3 style paths\" ) # Fix posix strip s3_path = s3_path.replace(\"s3:/\", \"s3://\") bucket, obj, fid = get_s3_bucket_object_name(f\"{s3_path}/{name}\") upload_s3( bucket=bucket, obj=obj, temp_path=temp_path, s3_session=s3_config.s3_session, upload_config=s3_config.upload_config, ) def get_s3_bucket_object_name(s3_path: str) -> typing.Tuple[str, str, str]: \"\"\"Splits a S3 uri into bucket, object, name *Args*: s3_path: s3 uri *Returns*: bucket object name \"\"\" parsed = urlparse(s3_path) return parsed.netloc, parsed.path.lstrip(\"/\"), os.path.basename(parsed.path) def download_s3( bucket: str, obj: str, temp_path: str, s3_session: BaseClient, download_config: S3DownloadConfig, ) -> str: \"\"\"Attempts to download the file from the S3 uri to a temp location using any extra arguments to the download *Args*: bucket: s3 bucket obj: s3 object temp_path: local temporary path to write file s3_session: current s3 session download_config: S3DownloadConfig with extra options for the file transfer *Returns*: temp_path: the temporary path of the config file downloaded from s3 \"\"\" try: # Unroll the extra options for those values that are not None extra_options = { k: v for k, v in attr.asdict(download_config).items() if v is not None } file_size = s3_session.head_object(Bucket=bucket, Key=obj, **extra_options)[ \"ContentLength\" ] print(f\"Attempting to download s3://{bucket}/{obj} (size: {size(file_size)})\") current_progress = 0 n_ticks = 50 def _s3_progress_bar(chunk): nonlocal current_progress # Increment progress current_progress += chunk done = int(n_ticks * (current_progress / file_size)) sys.stdout.write( f\"\\r[%s%s] \" f\"{int(current_progress/file_size) * 100}%%\" % (\"=\" * done, \" \" * (n_ticks - done)) ) sys.stdout.flush() sys.stdout.write(\"\\n\\n\") # Download with the progress callback s3_session.download_file( bucket, obj, temp_path, Callback=_s3_progress_bar, ExtraArgs=extra_options ) return temp_path except IOError: print( f\"Failed to download file from S3 \" f\"(bucket: {bucket}, object: {obj}) \" f\"and write to {temp_path}\" ) def upload_s3( bucket: str, obj: str, temp_path: str, s3_session: BaseClient, upload_config: S3UploadConfig, ): \"\"\"Attempts to upload the local file to the S3 uri using any extra arguments to the upload *Args*: bucket: s3 bucket obj: s3 object temp_path: temporary path of the config file s3_session: current s3 session upload_config: S3UploadConfig with extra options for the file transfer *Returns*: \"\"\" try: # Unroll the extra options for those values that are not None extra_options = { k: v for k, v in attr.asdict(upload_config).items() if v is not None } file_size = os.path.getsize(temp_path) print(f\"Attempting to upload s3://{bucket}/{obj} (size: {size(file_size)})\") current_progress = 0 n_ticks = 50 def _s3_progress_bar(chunk): nonlocal current_progress # Increment progress current_progress += chunk done = int(n_ticks * (current_progress / file_size)) sys.stdout.write( f\"\\r[%s%s] \" f\"{int(current_progress/file_size) * 100}%%\" % (\"=\" * done, \" \" * (n_ticks - done)) ) sys.stdout.flush() sys.stdout.write(\"\\n\\n\") # Upload with progress callback s3_session.upload_file( temp_path, bucket, obj, Callback=_s3_progress_bar, ExtraArgs=extra_options ) except IOError: print( f\"Failed to upload file to S3 \" f\"(bucket: {bucket}, object: {obj}) \" f\"from {temp_path}\" ) Functions download_s3 def download_s3 ( bucket : str , obj : str , temp_path : str , s3_session : botocore . client . BaseClient , download_config : spock . addons . s3 . configs . S3DownloadConfig ) -> str Attempts to download the file from the S3 uri to a temp location using any extra arguments to the download Args : bucket: s3 bucket obj: s3 object temp_path: local temporary path to write file s3_session: current s3 session download_config: S3DownloadConfig with extra options for the file transfer Returns : temp_path: the temporary path of the config file downloaded from s3 ??? example \"View Source\" def download_s3( bucket: str, obj: str, temp_path: str, s3_session: BaseClient, download_config: S3DownloadConfig, ) -> str: \"\"\"Attempts to download the file from the S3 uri to a temp location using any extra arguments to the download *Args*: bucket: s3 bucket obj: s3 object temp_path: local temporary path to write file s3_session: current s3 session download_config: S3DownloadConfig with extra options for the file transfer *Returns*: temp_path: the temporary path of the config file downloaded from s3 \"\"\" try: # Unroll the extra options for those values that are not None extra_options = { k: v for k, v in attr.asdict(download_config).items() if v is not None } file_size = s3_session.head_object(Bucket=bucket, Key=obj, **extra_options)[ \"ContentLength\" ] print(f\"Attempting to download s3://{bucket}/{obj} (size: {size(file_size)})\") current_progress = 0 n_ticks = 50 def _s3_progress_bar(chunk): nonlocal current_progress # Increment progress current_progress += chunk done = int(n_ticks * (current_progress / file_size)) sys.stdout.write( f\"\\r[%s%s] \" f\"{int(current_progress/file_size) * 100}%%\" % (\"=\" * done, \" \" * (n_ticks - done)) ) sys.stdout.flush() sys.stdout.write(\"\\n\\n\") # Download with the progress callback s3_session.download_file( bucket, obj, temp_path, Callback=_s3_progress_bar, ExtraArgs=extra_options ) return temp_path except IOError: print( f\"Failed to download file from S3 \" f\"(bucket: {bucket}, object: {obj}) \" f\"and write to {temp_path}\" ) get_s3_bucket_object_name def get_s3_bucket_object_name ( s3_path : str ) -> Tuple [ str , str , str ] Splits a S3 uri into bucket, object, name Args : s3_path: s3 uri Returns : bucket object name ??? example \"View Source\" def get_s3_bucket_object_name(s3_path: str) -> typing.Tuple[str, str, str]: \"\"\"Splits a S3 uri into bucket, object, name *Args*: s3_path: s3 uri *Returns*: bucket object name \"\"\" parsed = urlparse(s3_path) return parsed.netloc, parsed.path.lstrip(\"/\"), os.path.basename(parsed.path) handle_s3_load_path def handle_s3_load_path ( path : str , s3_config : spock . addons . s3 . configs . S3Config ) -> str Handles loading from S3 uri Handles downloading file from a given s3 uri to a local temp location and passing the path back to the handler load call Args : path: s3 uri path s3_config: s3_config object Returns : temp_path: the temporary path of the config file downloaded from s3 ??? example \"View Source\" def handle_s3_load_path(path: str, s3_config: S3Config) -> str: \"\"\"Handles loading from S3 uri Handles downloading file from a given s3 uri to a local temp location and passing the path back to the handler load call *Args*: path: s3 uri path s3_config: s3_config object *Returns*: temp_path: the temporary path of the config file downloaded from s3 \"\"\" if s3_config is None: raise ValueError( \"Load from S3 -- Missing S3Config object which is necessary to handle S3 style paths\" ) bucket, obj, fid = get_s3_bucket_object_name(s3_path=path) # Construct the full temp path temp_path = f\"{s3_config.temp_folder}/{fid}\" # Strip double slashes if exist temp_path = temp_path.replace(r\"//\", r\"/\") temp_path = download_s3( bucket=bucket, obj=obj, temp_path=temp_path, s3_session=s3_config.s3_session, download_config=s3_config.download_config, ) return temp_path handle_s3_save_path def handle_s3_save_path ( temp_path : str , s3_path : str , name : str , s3_config : spock . addons . s3 . configs . S3Config ) Handles saving to S3 uri Points to the local spock configuration file and handles getting it up to S3 Args : temp_path: the temporary path the spock config was written out to locally s3_path: base s3 uri name: spock generated filename s3_config: s3_config object Returns : ??? example \"View Source\" def handle_s3_save_path(temp_path: str, s3_path: str, name: str, s3_config: S3Config): \"\"\"Handles saving to S3 uri Points to the local spock configuration file and handles getting it up to S3 *Args*: temp_path: the temporary path the spock config was written out to locally s3_path: base s3 uri name: spock generated filename s3_config: s3_config object *Returns*: \"\"\" if s3_config is None: raise ValueError( \"Save to S3 -- Missing S3Config object which is necessary to handle S3 style paths\" ) # Fix posix strip s3_path = s3_path.replace(\"s3:/\", \"s3://\") bucket, obj, fid = get_s3_bucket_object_name(f\"{s3_path}/{name}\") upload_s3( bucket=bucket, obj=obj, temp_path=temp_path, s3_session=s3_config.s3_session, upload_config=s3_config.upload_config, ) upload_s3 def upload_s3 ( bucket : str , obj : str , temp_path : str , s3_session : botocore . client . BaseClient , upload_config : spock . addons . s3 . configs . S3UploadConfig ) Attempts to upload the local file to the S3 uri using any extra arguments to the upload Args : bucket: s3 bucket obj: s3 object temp_path: temporary path of the config file s3_session: current s3 session upload_config: S3UploadConfig with extra options for the file transfer Returns : ??? example \"View Source\" def upload_s3( bucket: str, obj: str, temp_path: str, s3_session: BaseClient, upload_config: S3UploadConfig, ): \"\"\"Attempts to upload the local file to the S3 uri using any extra arguments to the upload *Args*: bucket: s3 bucket obj: s3 object temp_path: temporary path of the config file s3_session: current s3 session upload_config: S3UploadConfig with extra options for the file transfer *Returns*: \"\"\" try: # Unroll the extra options for those values that are not None extra_options = { k: v for k, v in attr.asdict(upload_config).items() if v is not None } file_size = os.path.getsize(temp_path) print(f\"Attempting to upload s3://{bucket}/{obj} (size: {size(file_size)})\") current_progress = 0 n_ticks = 50 def _s3_progress_bar(chunk): nonlocal current_progress # Increment progress current_progress += chunk done = int(n_ticks * (current_progress / file_size)) sys.stdout.write( f\"\\r[%s%s] \" f\"{int(current_progress/file_size) * 100}%%\" % (\"=\" * done, \" \" * (n_ticks - done)) ) sys.stdout.flush() sys.stdout.write(\"\\n\\n\") # Upload with progress callback s3_session.upload_file( temp_path, bucket, obj, Callback=_s3_progress_bar, ExtraArgs=extra_options ) except IOError: print( f\"Failed to upload file to S3 \" f\"(bucket: {bucket}, object: {obj}) \" f\"from {temp_path}\" )","title":"Utils"},{"location":"reference/spock/addons/s3/utils/#module-spockaddonss3utils","text":"Handles all S3 related ops -- allows for s3 functionality to be optional to keep req deps light None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles all S3 related ops -- allows for s3 functionality to be optional to keep req deps light\"\"\" import attr try: import boto3 from botocore.client import BaseClient except ImportError: print( \"Missing libraries to support S3 functionality. Please re-install spock with the extra s3 dependencies -- \" \"pip install spock-config[s3]\" ) import os import sys import typing from urllib.parse import urlparse from hurry.filesize import size from spock.addons.s3.configs import S3Config, S3DownloadConfig, S3UploadConfig def handle_s3_load_path(path: str, s3_config: S3Config) -> str: \"\"\"Handles loading from S3 uri Handles downloading file from a given s3 uri to a local temp location and passing the path back to the handler load call *Args*: path: s3 uri path s3_config: s3_config object *Returns*: temp_path: the temporary path of the config file downloaded from s3 \"\"\" if s3_config is None: raise ValueError( \"Load from S3 -- Missing S3Config object which is necessary to handle S3 style paths\" ) bucket, obj, fid = get_s3_bucket_object_name(s3_path=path) # Construct the full temp path temp_path = f\"{s3_config.temp_folder}/{fid}\" # Strip double slashes if exist temp_path = temp_path.replace(r\"//\", r\"/\") temp_path = download_s3( bucket=bucket, obj=obj, temp_path=temp_path, s3_session=s3_config.s3_session, download_config=s3_config.download_config, ) return temp_path def handle_s3_save_path(temp_path: str, s3_path: str, name: str, s3_config: S3Config): \"\"\"Handles saving to S3 uri Points to the local spock configuration file and handles getting it up to S3 *Args*: temp_path: the temporary path the spock config was written out to locally s3_path: base s3 uri name: spock generated filename s3_config: s3_config object *Returns*: \"\"\" if s3_config is None: raise ValueError( \"Save to S3 -- Missing S3Config object which is necessary to handle S3 style paths\" ) # Fix posix strip s3_path = s3_path.replace(\"s3:/\", \"s3://\") bucket, obj, fid = get_s3_bucket_object_name(f\"{s3_path}/{name}\") upload_s3( bucket=bucket, obj=obj, temp_path=temp_path, s3_session=s3_config.s3_session, upload_config=s3_config.upload_config, ) def get_s3_bucket_object_name(s3_path: str) -> typing.Tuple[str, str, str]: \"\"\"Splits a S3 uri into bucket, object, name *Args*: s3_path: s3 uri *Returns*: bucket object name \"\"\" parsed = urlparse(s3_path) return parsed.netloc, parsed.path.lstrip(\"/\"), os.path.basename(parsed.path) def download_s3( bucket: str, obj: str, temp_path: str, s3_session: BaseClient, download_config: S3DownloadConfig, ) -> str: \"\"\"Attempts to download the file from the S3 uri to a temp location using any extra arguments to the download *Args*: bucket: s3 bucket obj: s3 object temp_path: local temporary path to write file s3_session: current s3 session download_config: S3DownloadConfig with extra options for the file transfer *Returns*: temp_path: the temporary path of the config file downloaded from s3 \"\"\" try: # Unroll the extra options for those values that are not None extra_options = { k: v for k, v in attr.asdict(download_config).items() if v is not None } file_size = s3_session.head_object(Bucket=bucket, Key=obj, **extra_options)[ \"ContentLength\" ] print(f\"Attempting to download s3://{bucket}/{obj} (size: {size(file_size)})\") current_progress = 0 n_ticks = 50 def _s3_progress_bar(chunk): nonlocal current_progress # Increment progress current_progress += chunk done = int(n_ticks * (current_progress / file_size)) sys.stdout.write( f\"\\r[%s%s] \" f\"{int(current_progress/file_size) * 100}%%\" % (\"=\" * done, \" \" * (n_ticks - done)) ) sys.stdout.flush() sys.stdout.write(\"\\n\\n\") # Download with the progress callback s3_session.download_file( bucket, obj, temp_path, Callback=_s3_progress_bar, ExtraArgs=extra_options ) return temp_path except IOError: print( f\"Failed to download file from S3 \" f\"(bucket: {bucket}, object: {obj}) \" f\"and write to {temp_path}\" ) def upload_s3( bucket: str, obj: str, temp_path: str, s3_session: BaseClient, upload_config: S3UploadConfig, ): \"\"\"Attempts to upload the local file to the S3 uri using any extra arguments to the upload *Args*: bucket: s3 bucket obj: s3 object temp_path: temporary path of the config file s3_session: current s3 session upload_config: S3UploadConfig with extra options for the file transfer *Returns*: \"\"\" try: # Unroll the extra options for those values that are not None extra_options = { k: v for k, v in attr.asdict(upload_config).items() if v is not None } file_size = os.path.getsize(temp_path) print(f\"Attempting to upload s3://{bucket}/{obj} (size: {size(file_size)})\") current_progress = 0 n_ticks = 50 def _s3_progress_bar(chunk): nonlocal current_progress # Increment progress current_progress += chunk done = int(n_ticks * (current_progress / file_size)) sys.stdout.write( f\"\\r[%s%s] \" f\"{int(current_progress/file_size) * 100}%%\" % (\"=\" * done, \" \" * (n_ticks - done)) ) sys.stdout.flush() sys.stdout.write(\"\\n\\n\") # Upload with progress callback s3_session.upload_file( temp_path, bucket, obj, Callback=_s3_progress_bar, ExtraArgs=extra_options ) except IOError: print( f\"Failed to upload file to S3 \" f\"(bucket: {bucket}, object: {obj}) \" f\"from {temp_path}\" )","title":"Module spock.addons.s3.utils"},{"location":"reference/spock/addons/s3/utils/#functions","text":"","title":"Functions"},{"location":"reference/spock/addons/s3/utils/#download_s3","text":"def download_s3 ( bucket : str , obj : str , temp_path : str , s3_session : botocore . client . BaseClient , download_config : spock . addons . s3 . configs . S3DownloadConfig ) -> str Attempts to download the file from the S3 uri to a temp location using any extra arguments to the download Args : bucket: s3 bucket obj: s3 object temp_path: local temporary path to write file s3_session: current s3 session download_config: S3DownloadConfig with extra options for the file transfer Returns : temp_path: the temporary path of the config file downloaded from s3 ??? example \"View Source\" def download_s3( bucket: str, obj: str, temp_path: str, s3_session: BaseClient, download_config: S3DownloadConfig, ) -> str: \"\"\"Attempts to download the file from the S3 uri to a temp location using any extra arguments to the download *Args*: bucket: s3 bucket obj: s3 object temp_path: local temporary path to write file s3_session: current s3 session download_config: S3DownloadConfig with extra options for the file transfer *Returns*: temp_path: the temporary path of the config file downloaded from s3 \"\"\" try: # Unroll the extra options for those values that are not None extra_options = { k: v for k, v in attr.asdict(download_config).items() if v is not None } file_size = s3_session.head_object(Bucket=bucket, Key=obj, **extra_options)[ \"ContentLength\" ] print(f\"Attempting to download s3://{bucket}/{obj} (size: {size(file_size)})\") current_progress = 0 n_ticks = 50 def _s3_progress_bar(chunk): nonlocal current_progress # Increment progress current_progress += chunk done = int(n_ticks * (current_progress / file_size)) sys.stdout.write( f\"\\r[%s%s] \" f\"{int(current_progress/file_size) * 100}%%\" % (\"=\" * done, \" \" * (n_ticks - done)) ) sys.stdout.flush() sys.stdout.write(\"\\n\\n\") # Download with the progress callback s3_session.download_file( bucket, obj, temp_path, Callback=_s3_progress_bar, ExtraArgs=extra_options ) return temp_path except IOError: print( f\"Failed to download file from S3 \" f\"(bucket: {bucket}, object: {obj}) \" f\"and write to {temp_path}\" )","title":"download_s3"},{"location":"reference/spock/addons/s3/utils/#get_s3_bucket_object_name","text":"def get_s3_bucket_object_name ( s3_path : str ) -> Tuple [ str , str , str ] Splits a S3 uri into bucket, object, name Args : s3_path: s3 uri Returns : bucket object name ??? example \"View Source\" def get_s3_bucket_object_name(s3_path: str) -> typing.Tuple[str, str, str]: \"\"\"Splits a S3 uri into bucket, object, name *Args*: s3_path: s3 uri *Returns*: bucket object name \"\"\" parsed = urlparse(s3_path) return parsed.netloc, parsed.path.lstrip(\"/\"), os.path.basename(parsed.path)","title":"get_s3_bucket_object_name"},{"location":"reference/spock/addons/s3/utils/#handle_s3_load_path","text":"def handle_s3_load_path ( path : str , s3_config : spock . addons . s3 . configs . S3Config ) -> str Handles loading from S3 uri Handles downloading file from a given s3 uri to a local temp location and passing the path back to the handler load call Args : path: s3 uri path s3_config: s3_config object Returns : temp_path: the temporary path of the config file downloaded from s3 ??? example \"View Source\" def handle_s3_load_path(path: str, s3_config: S3Config) -> str: \"\"\"Handles loading from S3 uri Handles downloading file from a given s3 uri to a local temp location and passing the path back to the handler load call *Args*: path: s3 uri path s3_config: s3_config object *Returns*: temp_path: the temporary path of the config file downloaded from s3 \"\"\" if s3_config is None: raise ValueError( \"Load from S3 -- Missing S3Config object which is necessary to handle S3 style paths\" ) bucket, obj, fid = get_s3_bucket_object_name(s3_path=path) # Construct the full temp path temp_path = f\"{s3_config.temp_folder}/{fid}\" # Strip double slashes if exist temp_path = temp_path.replace(r\"//\", r\"/\") temp_path = download_s3( bucket=bucket, obj=obj, temp_path=temp_path, s3_session=s3_config.s3_session, download_config=s3_config.download_config, ) return temp_path","title":"handle_s3_load_path"},{"location":"reference/spock/addons/s3/utils/#handle_s3_save_path","text":"def handle_s3_save_path ( temp_path : str , s3_path : str , name : str , s3_config : spock . addons . s3 . configs . S3Config ) Handles saving to S3 uri Points to the local spock configuration file and handles getting it up to S3 Args : temp_path: the temporary path the spock config was written out to locally s3_path: base s3 uri name: spock generated filename s3_config: s3_config object Returns : ??? example \"View Source\" def handle_s3_save_path(temp_path: str, s3_path: str, name: str, s3_config: S3Config): \"\"\"Handles saving to S3 uri Points to the local spock configuration file and handles getting it up to S3 *Args*: temp_path: the temporary path the spock config was written out to locally s3_path: base s3 uri name: spock generated filename s3_config: s3_config object *Returns*: \"\"\" if s3_config is None: raise ValueError( \"Save to S3 -- Missing S3Config object which is necessary to handle S3 style paths\" ) # Fix posix strip s3_path = s3_path.replace(\"s3:/\", \"s3://\") bucket, obj, fid = get_s3_bucket_object_name(f\"{s3_path}/{name}\") upload_s3( bucket=bucket, obj=obj, temp_path=temp_path, s3_session=s3_config.s3_session, upload_config=s3_config.upload_config, )","title":"handle_s3_save_path"},{"location":"reference/spock/addons/s3/utils/#upload_s3","text":"def upload_s3 ( bucket : str , obj : str , temp_path : str , s3_session : botocore . client . BaseClient , upload_config : spock . addons . s3 . configs . S3UploadConfig ) Attempts to upload the local file to the S3 uri using any extra arguments to the upload Args : bucket: s3 bucket obj: s3 object temp_path: temporary path of the config file s3_session: current s3 session upload_config: S3UploadConfig with extra options for the file transfer Returns : ??? example \"View Source\" def upload_s3( bucket: str, obj: str, temp_path: str, s3_session: BaseClient, upload_config: S3UploadConfig, ): \"\"\"Attempts to upload the local file to the S3 uri using any extra arguments to the upload *Args*: bucket: s3 bucket obj: s3 object temp_path: temporary path of the config file s3_session: current s3 session upload_config: S3UploadConfig with extra options for the file transfer *Returns*: \"\"\" try: # Unroll the extra options for those values that are not None extra_options = { k: v for k, v in attr.asdict(upload_config).items() if v is not None } file_size = os.path.getsize(temp_path) print(f\"Attempting to upload s3://{bucket}/{obj} (size: {size(file_size)})\") current_progress = 0 n_ticks = 50 def _s3_progress_bar(chunk): nonlocal current_progress # Increment progress current_progress += chunk done = int(n_ticks * (current_progress / file_size)) sys.stdout.write( f\"\\r[%s%s] \" f\"{int(current_progress/file_size) * 100}%%\" % (\"=\" * done, \" \" * (n_ticks - done)) ) sys.stdout.flush() sys.stdout.write(\"\\n\\n\") # Upload with progress callback s3_session.upload_file( temp_path, bucket, obj, Callback=_s3_progress_bar, ExtraArgs=extra_options ) except IOError: print( f\"Failed to upload file to S3 \" f\"(bucket: {bucket}, object: {obj}) \" f\"from {temp_path}\" )","title":"upload_s3"},{"location":"reference/spock/addons/tune/","text":"Module spock.addons.tune Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\" Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md \"\"\" from spock.addons.tune.config import ( AxTunerConfig, ChoiceHyperParameter, OptunaTunerConfig, RangeHyperParameter, spockTuner, ) __all__ = [ \"builder\", \"config\", \"spockTuner\", \"AxTunerConfig\", \"RangeHyperParameter\", \"ChoiceHyperParameter\", \"OptunaTunerConfig\", ] Sub-modules spock.addons.tune.ax spock.addons.tune.builder spock.addons.tune.config spock.addons.tune.interface spock.addons.tune.optuna spock.addons.tune.payload spock.addons.tune.tuner Variables config Functions spockTuner def spockTuner ( cls ) Ovverides basic spock_attr decorator with another name Using a different name allows spock to easily determine which parameters are normal and which are meant to be used in a hyper-parameter tuning backend Args : cls: basic class def Returns : cls: slotted attrs class that is frozen and kw only ??? example \"View Source\" def _spock_tune(cls): \"\"\"Ovverides basic spock_attr decorator with another name Using a different name allows spock to easily determine which parameters are normal and which are meant to be used in a hyper-parameter tuning backend *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" bases, attrs_dict = _base_attr(cls) # Dynamically make an attr class obj = attr.make_class( name=cls.__name__, bases=bases, attrs=attrs_dict, kw_only=True, frozen=True ) # For each class we dynamically create we need to register it within the system modules for pickle to work setattr(sys.modules[\"spock\"].addons.tune.config, obj.__name__, obj) # Swap the __doc__ string from cls to obj obj.__doc__ = cls.__doc__ return obj Classes AxTunerConfig class AxTunerConfig ( objective_name : str , tracking_metric_names : Union [ List [ str ], NoneType ] = None , name : Union [ str , NoneType ] = 'spock_ax_40b1f198-742e-401a-8b3c-0f7e01994f87' , minimize : bool = True , parameter_constraints : Union [ List [ str ], NoneType ] = None , outcome_constraints : Union [ List [ str ], NoneType ] = None , support_intermediate_data : bool = False , overwrite_existing_experiment : bool = False , immutable_search_space_and_opt_config : bool = True , is_test : bool = False , generation_strategy : Union [ ax . modelbridge . generation_strategy . GenerationStrategy , NoneType ] = None , enforce_sequential_optimization : bool = True , random_seed : Union [ int , NoneType ] = None , verbose_logging : bool = True ) ??? example \"View Source\" class AxTunerConfig: objective_name: str tracking_metric_names: Optional[List[str]] = None name: Optional[str] = f\"spock_ax_{uuid4()}\" minimize: bool = True parameter_constraints: Optional[List[str]] = None outcome_constraints: Optional[List[str]] = None support_intermediate_data: bool = False overwrite_existing_experiment: bool = False immutable_search_space_and_opt_config: bool = True is_test: bool = False generation_strategy: Optional[GenerationStrategy] = None enforce_sequential_optimization: bool = True random_seed: Optional[int] = None verbose_logging: bool = True ChoiceHyperParameter class ChoiceHyperParameter ( type : str , choices : Union [ List [ str ], List [ int ], List [ float ], List [ bool ]] ) Attributes Name Type Description Default type None type of the hyper-parameter -- (note: spock will attempt to autocast into this type) None choices None list of variable length that contains all the possible choices to select from None ??? example \"View Source\" class ChoiceHyperParameter: \"\"\"Choice based hyper-parameter that is sampled uniformly Attributes: type: type of the hyper-parameter -- (note: spock will attempt to autocast into this type) choices: list of variable length that contains all the possible choices to select from \"\"\" type = attr.ib( type=str, validator=[ attr.validators.instance_of(str), attr.validators.in_([\"float\", \"int\", \"str\", \"bool\"]), ], ) choices = attr.ib( type=Union[List[str], List[int], List[float], List[bool]], validator=attr.validators.deep_iterable( member_validator=attr.validators.instance_of((float, int, bool, str)), iterable_validator=attr.validators.instance_of(list), ), ) Class variables choices type OptunaTunerConfig class OptunaTunerConfig ( storage : Union [ str , optuna . storages . _base . BaseStorage , NoneType ] = None , sampler : Union [ optuna . samplers . _base . BaseSampler , NoneType ] = None , pruner : Union [ optuna . pruners . _base . BasePruner , NoneType ] = None , study_name : Union [ str , NoneType ] = 'spock_optuna_5966f43f-329f-4d58-ab83-c4257b667d63' , direction : Union [ str , optuna . study . _study_direction . StudyDirection , NoneType ] = None , load_if_exists : bool = False , directions : Union [ Sequence [ Union [ str , optuna . study . _study_direction . StudyDirection ]], NoneType ] = None ) ??? example \"View Source\" class OptunaTunerConfig: storage: Optional[Union[str, optuna.storages.BaseStorage]] = None sampler: Optional[optuna.samplers.BaseSampler] = None pruner: Optional[optuna.pruners.BasePruner] = None study_name: Optional[str] = f\"spock_optuna_{uuid4()}\" direction: Optional[Union[str, optuna.study.StudyDirection]] = None load_if_exists: bool = False directions: Optional[Sequence[Union[str, optuna.study.StudyDirection]]] = None RangeHyperParameter class RangeHyperParameter ( type : str , bounds : Union [ Tuple [ float , float ], Tuple [ int , int ]], log_scale : bool ) Attributes Name Type Description Default type None type of the hyper-parameter (note: spock will attempt to autocast into this type) None bounds None min and max of the hyper-parameter range None log_scale None log scale the values before sampling None ??? example \"View Source\" class RangeHyperParameter: \"\"\"Range based hyper-parameter that is sampled uniformly Attributes: type: type of the hyper-parameter (note: spock will attempt to autocast into this type) bounds: min and max of the hyper-parameter range log_scale: log scale the values before sampling \"\"\" type = attr.ib( type=str, validator=[ attr.validators.instance_of(str), attr.validators.in_([\"float\", \"int\"]), ], ) bounds = attr.ib( type=Union[Tuple[float, float], Tuple[int, int]], validator=attr.validators.deep_iterable( member_validator=attr.validators.instance_of((float, int)), iterable_validator=attr.validators.instance_of(tuple), ), ) log_scale = attr.ib(type=bool, validator=attr.validators.instance_of(bool)) Class variables bounds log_scale type","title":"Index"},{"location":"reference/spock/addons/tune/#module-spockaddonstune","text":"Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\" Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md \"\"\" from spock.addons.tune.config import ( AxTunerConfig, ChoiceHyperParameter, OptunaTunerConfig, RangeHyperParameter, spockTuner, ) __all__ = [ \"builder\", \"config\", \"spockTuner\", \"AxTunerConfig\", \"RangeHyperParameter\", \"ChoiceHyperParameter\", \"OptunaTunerConfig\", ]","title":"Module spock.addons.tune"},{"location":"reference/spock/addons/tune/#sub-modules","text":"spock.addons.tune.ax spock.addons.tune.builder spock.addons.tune.config spock.addons.tune.interface spock.addons.tune.optuna spock.addons.tune.payload spock.addons.tune.tuner","title":"Sub-modules"},{"location":"reference/spock/addons/tune/#variables","text":"config","title":"Variables"},{"location":"reference/spock/addons/tune/#functions","text":"","title":"Functions"},{"location":"reference/spock/addons/tune/#spocktuner","text":"def spockTuner ( cls ) Ovverides basic spock_attr decorator with another name Using a different name allows spock to easily determine which parameters are normal and which are meant to be used in a hyper-parameter tuning backend Args : cls: basic class def Returns : cls: slotted attrs class that is frozen and kw only ??? example \"View Source\" def _spock_tune(cls): \"\"\"Ovverides basic spock_attr decorator with another name Using a different name allows spock to easily determine which parameters are normal and which are meant to be used in a hyper-parameter tuning backend *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" bases, attrs_dict = _base_attr(cls) # Dynamically make an attr class obj = attr.make_class( name=cls.__name__, bases=bases, attrs=attrs_dict, kw_only=True, frozen=True ) # For each class we dynamically create we need to register it within the system modules for pickle to work setattr(sys.modules[\"spock\"].addons.tune.config, obj.__name__, obj) # Swap the __doc__ string from cls to obj obj.__doc__ = cls.__doc__ return obj","title":"spockTuner"},{"location":"reference/spock/addons/tune/#classes","text":"","title":"Classes"},{"location":"reference/spock/addons/tune/#axtunerconfig","text":"class AxTunerConfig ( objective_name : str , tracking_metric_names : Union [ List [ str ], NoneType ] = None , name : Union [ str , NoneType ] = 'spock_ax_40b1f198-742e-401a-8b3c-0f7e01994f87' , minimize : bool = True , parameter_constraints : Union [ List [ str ], NoneType ] = None , outcome_constraints : Union [ List [ str ], NoneType ] = None , support_intermediate_data : bool = False , overwrite_existing_experiment : bool = False , immutable_search_space_and_opt_config : bool = True , is_test : bool = False , generation_strategy : Union [ ax . modelbridge . generation_strategy . GenerationStrategy , NoneType ] = None , enforce_sequential_optimization : bool = True , random_seed : Union [ int , NoneType ] = None , verbose_logging : bool = True ) ??? example \"View Source\" class AxTunerConfig: objective_name: str tracking_metric_names: Optional[List[str]] = None name: Optional[str] = f\"spock_ax_{uuid4()}\" minimize: bool = True parameter_constraints: Optional[List[str]] = None outcome_constraints: Optional[List[str]] = None support_intermediate_data: bool = False overwrite_existing_experiment: bool = False immutable_search_space_and_opt_config: bool = True is_test: bool = False generation_strategy: Optional[GenerationStrategy] = None enforce_sequential_optimization: bool = True random_seed: Optional[int] = None verbose_logging: bool = True","title":"AxTunerConfig"},{"location":"reference/spock/addons/tune/#choicehyperparameter","text":"class ChoiceHyperParameter ( type : str , choices : Union [ List [ str ], List [ int ], List [ float ], List [ bool ]] )","title":"ChoiceHyperParameter"},{"location":"reference/spock/addons/tune/#attributes","text":"Name Type Description Default type None type of the hyper-parameter -- (note: spock will attempt to autocast into this type) None choices None list of variable length that contains all the possible choices to select from None ??? example \"View Source\" class ChoiceHyperParameter: \"\"\"Choice based hyper-parameter that is sampled uniformly Attributes: type: type of the hyper-parameter -- (note: spock will attempt to autocast into this type) choices: list of variable length that contains all the possible choices to select from \"\"\" type = attr.ib( type=str, validator=[ attr.validators.instance_of(str), attr.validators.in_([\"float\", \"int\", \"str\", \"bool\"]), ], ) choices = attr.ib( type=Union[List[str], List[int], List[float], List[bool]], validator=attr.validators.deep_iterable( member_validator=attr.validators.instance_of((float, int, bool, str)), iterable_validator=attr.validators.instance_of(list), ), )","title":"Attributes"},{"location":"reference/spock/addons/tune/#class-variables","text":"choices type","title":"Class variables"},{"location":"reference/spock/addons/tune/#optunatunerconfig","text":"class OptunaTunerConfig ( storage : Union [ str , optuna . storages . _base . BaseStorage , NoneType ] = None , sampler : Union [ optuna . samplers . _base . BaseSampler , NoneType ] = None , pruner : Union [ optuna . pruners . _base . BasePruner , NoneType ] = None , study_name : Union [ str , NoneType ] = 'spock_optuna_5966f43f-329f-4d58-ab83-c4257b667d63' , direction : Union [ str , optuna . study . _study_direction . StudyDirection , NoneType ] = None , load_if_exists : bool = False , directions : Union [ Sequence [ Union [ str , optuna . study . _study_direction . StudyDirection ]], NoneType ] = None ) ??? example \"View Source\" class OptunaTunerConfig: storage: Optional[Union[str, optuna.storages.BaseStorage]] = None sampler: Optional[optuna.samplers.BaseSampler] = None pruner: Optional[optuna.pruners.BasePruner] = None study_name: Optional[str] = f\"spock_optuna_{uuid4()}\" direction: Optional[Union[str, optuna.study.StudyDirection]] = None load_if_exists: bool = False directions: Optional[Sequence[Union[str, optuna.study.StudyDirection]]] = None","title":"OptunaTunerConfig"},{"location":"reference/spock/addons/tune/#rangehyperparameter","text":"class RangeHyperParameter ( type : str , bounds : Union [ Tuple [ float , float ], Tuple [ int , int ]], log_scale : bool )","title":"RangeHyperParameter"},{"location":"reference/spock/addons/tune/#attributes_1","text":"Name Type Description Default type None type of the hyper-parameter (note: spock will attempt to autocast into this type) None bounds None min and max of the hyper-parameter range None log_scale None log scale the values before sampling None ??? example \"View Source\" class RangeHyperParameter: \"\"\"Range based hyper-parameter that is sampled uniformly Attributes: type: type of the hyper-parameter (note: spock will attempt to autocast into this type) bounds: min and max of the hyper-parameter range log_scale: log scale the values before sampling \"\"\" type = attr.ib( type=str, validator=[ attr.validators.instance_of(str), attr.validators.in_([\"float\", \"int\"]), ], ) bounds = attr.ib( type=Union[Tuple[float, float], Tuple[int, int]], validator=attr.validators.deep_iterable( member_validator=attr.validators.instance_of((float, int)), iterable_validator=attr.validators.instance_of(tuple), ), ) log_scale = attr.ib(type=bool, validator=attr.validators.instance_of(bool))","title":"Attributes"},{"location":"reference/spock/addons/tune/#class-variables_1","text":"bounds log_scale type","title":"Class variables"},{"location":"reference/spock/addons/tune/ax/","text":"Module spock.addons.tune.ax Handles the ax backend None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the ax backend\"\"\" from ax.service.ax_client import AxClient from spock.addons.tune.config import AxTunerConfig from spock.addons.tune.interface import BaseInterface try: from typing import TypedDict except ImportError: from mypy_extensions import TypedDict class AxTunerStatus(TypedDict): \"\"\"Tuner status return object for Ax -- supports the service style API from Ax *Attributes*: client: current AxClient instance trial_index: current trial index \"\"\" client: AxClient trial_index: int class AxInterface(BaseInterface): \"\"\"Specific override to support the Ax backend -- supports the service style API from Ax\"\"\" def __init__(self, tuner_config: AxTunerConfig, tuner_namespace): \"\"\"AxInterface init call that maps variables, creates a map to fnc calls, and constructs the necessary underlying objects *Args*: tuner_config: configuration object for the ax backend tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" super(AxInterface, self).__init__(tuner_config, tuner_namespace) self._tuner_obj = AxClient( generation_strategy=self._tuner_config.generation_strategy, enforce_sequential_optimization=self._tuner_config.enforce_sequential_optimization, random_seed=self._tuner_config.random_seed, verbose_logging=self._tuner_config.verbose_logging, ) # Some variables to use later self._trial_index = None self._sample_hash = None # Mapping spock underlying classes to ax distributions (search space) self._map_type = { \"RangeHyperParameter\": { \"int\": self._ax_range, \"float\": self._ax_range, }, \"ChoiceHyperParameter\": { \"int\": self._ax_choice, \"float\": self._ax_choice, \"str\": self._ax_choice, \"bool\": self._ax_choice, }, } # Build the correct underlying dictionary object for Ax client create experiment self._param_obj = self._construct() # Create the AxClient experiment self._tuner_obj.create_experiment( parameters=self._param_obj, name=self._tuner_config.name, objective_name=self._tuner_config.objective_name, minimize=self._tuner_config.minimize, parameter_constraints=self._tuner_config.parameter_constraints, outcome_constraints=self._tuner_config.outcome_constraints, overwrite_existing_experiment=self._tuner_config.overwrite_existing_experiment, tracking_metric_names=self._tuner_config.tracking_metric_names, immutable_search_space_and_opt_config=self._tuner_config.immutable_search_space_and_opt_config, is_test=self._tuner_config.is_test, ) @property def tuner_status(self) -> AxTunerStatus: return AxTunerStatus(client=self._tuner_obj, trial_index=self._trial_index) @property def best(self): best_obj = self._tuner_obj.get_best_parameters() rollup_dict, _ = self._sample_rollup(best_obj[0]) return ( self._gen_spockspace(rollup_dict), best_obj[1][0][self._tuner_obj.objective_name], ) @property def _get_sample(self): return self._tuner_obj.get_next_trial() def sample(self): parameters, self._trial_index = self._get_sample # Roll this back out into a Spockspace so it can be merged into the fixed parameter Spockspace # Also need to un-dot the param names to rebuild the nested structure rollup_dict, sample_hash = self._sample_rollup(parameters) self._sample_hash = sample_hash return self._gen_spockspace(rollup_dict) def _construct(self): param_list = [] # These will only be nested one level deep given the tuner syntax for k, v in vars(self._tuner_namespace).items(): for ik, iv in vars(v).items(): param_fn = self._map_type[type(iv).__name__][iv.type] param_list.append(param_fn(name=f\"{k}.{ik}\", val=iv)) return param_list def _ax_range(self, name, val): \"\"\"Assemble the dictionary for ax range parameters *Args*: name: parameter name val: current attr val *Returns*: dictionary that can be added to a parameter list \"\"\" low, high = self._try_range_cast(val, type_string=\"RangeHyperParameter\") return { \"name\": name, \"type\": \"range\", \"bounds\": [low, high], \"value_type\": val.type, \"log_scale\": val.log_scale, } def _ax_choice(self, name, val): \"\"\"Assemble the dictionary for ax choice parameters *Args*: name: parameter name val: current attr val *Returns*: dictionary that can be added to a parameter list \"\"\" val = self._try_choice_cast(val, type_string=\"ChoiceHyperParameter\") return { \"name\": name, \"type\": \"choice\", \"values\": val.choices, \"value_type\": val.type, } Classes AxInterface class AxInterface ( tuner_config : spock . addons . tune . config . AxTunerConfig , tuner_namespace ) ??? example \"View Source\" class AxInterface(BaseInterface): \"\"\"Specific override to support the Ax backend -- supports the service style API from Ax\"\"\" def __init__(self, tuner_config: AxTunerConfig, tuner_namespace): \"\"\"AxInterface init call that maps variables, creates a map to fnc calls, and constructs the necessary underlying objects *Args*: tuner_config: configuration object for the ax backend tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" super(AxInterface, self).__init__(tuner_config, tuner_namespace) self._tuner_obj = AxClient( generation_strategy=self._tuner_config.generation_strategy, enforce_sequential_optimization=self._tuner_config.enforce_sequential_optimization, random_seed=self._tuner_config.random_seed, verbose_logging=self._tuner_config.verbose_logging, ) # Some variables to use later self._trial_index = None self._sample_hash = None # Mapping spock underlying classes to ax distributions (search space) self._map_type = { \"RangeHyperParameter\": { \"int\": self._ax_range, \"float\": self._ax_range, }, \"ChoiceHyperParameter\": { \"int\": self._ax_choice, \"float\": self._ax_choice, \"str\": self._ax_choice, \"bool\": self._ax_choice, }, } # Build the correct underlying dictionary object for Ax client create experiment self._param_obj = self._construct() # Create the AxClient experiment self._tuner_obj.create_experiment( parameters=self._param_obj, name=self._tuner_config.name, objective_name=self._tuner_config.objective_name, minimize=self._tuner_config.minimize, parameter_constraints=self._tuner_config.parameter_constraints, outcome_constraints=self._tuner_config.outcome_constraints, overwrite_existing_experiment=self._tuner_config.overwrite_existing_experiment, tracking_metric_names=self._tuner_config.tracking_metric_names, immutable_search_space_and_opt_config=self._tuner_config.immutable_search_space_and_opt_config, is_test=self._tuner_config.is_test, ) @property def tuner_status(self) -> AxTunerStatus: return AxTunerStatus(client=self._tuner_obj, trial_index=self._trial_index) @property def best(self): best_obj = self._tuner_obj.get_best_parameters() rollup_dict, _ = self._sample_rollup(best_obj[0]) return ( self._gen_spockspace(rollup_dict), best_obj[1][0][self._tuner_obj.objective_name], ) @property def _get_sample(self): return self._tuner_obj.get_next_trial() def sample(self): parameters, self._trial_index = self._get_sample # Roll this back out into a Spockspace so it can be merged into the fixed parameter Spockspace # Also need to un-dot the param names to rebuild the nested structure rollup_dict, sample_hash = self._sample_rollup(parameters) self._sample_hash = sample_hash return self._gen_spockspace(rollup_dict) def _construct(self): param_list = [] # These will only be nested one level deep given the tuner syntax for k, v in vars(self._tuner_namespace).items(): for ik, iv in vars(v).items(): param_fn = self._map_type[type(iv).__name__][iv.type] param_list.append(param_fn(name=f\"{k}.{ik}\", val=iv)) return param_list def _ax_range(self, name, val): \"\"\"Assemble the dictionary for ax range parameters *Args*: name: parameter name val: current attr val *Returns*: dictionary that can be added to a parameter list \"\"\" low, high = self._try_range_cast(val, type_string=\"RangeHyperParameter\") return { \"name\": name, \"type\": \"range\", \"bounds\": [low, high], \"value_type\": val.type, \"log_scale\": val.log_scale, } def _ax_choice(self, name, val): \"\"\"Assemble the dictionary for ax choice parameters *Args*: name: parameter name val: current attr val *Returns*: dictionary that can be added to a parameter list \"\"\" val = self._try_choice_cast(val, type_string=\"ChoiceHyperParameter\") return { \"name\": name, \"type\": \"choice\", \"values\": val.choices, \"value_type\": val.type, } Ancestors (in MRO) spock.addons.tune.interface.BaseInterface abc.ABC Instance variables best tuner_status Methods sample def sample ( self ) Calls the underlying library sample to get a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) Returns : Spockspace of the current hyper-parameter draw ??? example \"View Source\" def sample(self): parameters, self._trial_index = self._get_sample # Roll this back out into a Spockspace so it can be merged into the fixed parameter Spockspace # Also need to un-dot the param names to rebuild the nested structure rollup_dict, sample_hash = self._sample_rollup(parameters) self._sample_hash = sample_hash return self._gen_spockspace(rollup_dict) AxTunerStatus class AxTunerStatus ( / , * args , ** kwargs ) ??? example \"View Source\" class AxTunerStatus(TypedDict): \"\"\"Tuner status return object for Ax -- supports the service style API from Ax *Attributes*: client: current AxClient instance trial_index: current trial index \"\"\" client: AxClient trial_index: int Ancestors (in MRO) builtins.dict Methods clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] values def values ( ... ) D.values() -> an object providing a view on D's values","title":"Ax"},{"location":"reference/spock/addons/tune/ax/#module-spockaddonstuneax","text":"Handles the ax backend None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the ax backend\"\"\" from ax.service.ax_client import AxClient from spock.addons.tune.config import AxTunerConfig from spock.addons.tune.interface import BaseInterface try: from typing import TypedDict except ImportError: from mypy_extensions import TypedDict class AxTunerStatus(TypedDict): \"\"\"Tuner status return object for Ax -- supports the service style API from Ax *Attributes*: client: current AxClient instance trial_index: current trial index \"\"\" client: AxClient trial_index: int class AxInterface(BaseInterface): \"\"\"Specific override to support the Ax backend -- supports the service style API from Ax\"\"\" def __init__(self, tuner_config: AxTunerConfig, tuner_namespace): \"\"\"AxInterface init call that maps variables, creates a map to fnc calls, and constructs the necessary underlying objects *Args*: tuner_config: configuration object for the ax backend tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" super(AxInterface, self).__init__(tuner_config, tuner_namespace) self._tuner_obj = AxClient( generation_strategy=self._tuner_config.generation_strategy, enforce_sequential_optimization=self._tuner_config.enforce_sequential_optimization, random_seed=self._tuner_config.random_seed, verbose_logging=self._tuner_config.verbose_logging, ) # Some variables to use later self._trial_index = None self._sample_hash = None # Mapping spock underlying classes to ax distributions (search space) self._map_type = { \"RangeHyperParameter\": { \"int\": self._ax_range, \"float\": self._ax_range, }, \"ChoiceHyperParameter\": { \"int\": self._ax_choice, \"float\": self._ax_choice, \"str\": self._ax_choice, \"bool\": self._ax_choice, }, } # Build the correct underlying dictionary object for Ax client create experiment self._param_obj = self._construct() # Create the AxClient experiment self._tuner_obj.create_experiment( parameters=self._param_obj, name=self._tuner_config.name, objective_name=self._tuner_config.objective_name, minimize=self._tuner_config.minimize, parameter_constraints=self._tuner_config.parameter_constraints, outcome_constraints=self._tuner_config.outcome_constraints, overwrite_existing_experiment=self._tuner_config.overwrite_existing_experiment, tracking_metric_names=self._tuner_config.tracking_metric_names, immutable_search_space_and_opt_config=self._tuner_config.immutable_search_space_and_opt_config, is_test=self._tuner_config.is_test, ) @property def tuner_status(self) -> AxTunerStatus: return AxTunerStatus(client=self._tuner_obj, trial_index=self._trial_index) @property def best(self): best_obj = self._tuner_obj.get_best_parameters() rollup_dict, _ = self._sample_rollup(best_obj[0]) return ( self._gen_spockspace(rollup_dict), best_obj[1][0][self._tuner_obj.objective_name], ) @property def _get_sample(self): return self._tuner_obj.get_next_trial() def sample(self): parameters, self._trial_index = self._get_sample # Roll this back out into a Spockspace so it can be merged into the fixed parameter Spockspace # Also need to un-dot the param names to rebuild the nested structure rollup_dict, sample_hash = self._sample_rollup(parameters) self._sample_hash = sample_hash return self._gen_spockspace(rollup_dict) def _construct(self): param_list = [] # These will only be nested one level deep given the tuner syntax for k, v in vars(self._tuner_namespace).items(): for ik, iv in vars(v).items(): param_fn = self._map_type[type(iv).__name__][iv.type] param_list.append(param_fn(name=f\"{k}.{ik}\", val=iv)) return param_list def _ax_range(self, name, val): \"\"\"Assemble the dictionary for ax range parameters *Args*: name: parameter name val: current attr val *Returns*: dictionary that can be added to a parameter list \"\"\" low, high = self._try_range_cast(val, type_string=\"RangeHyperParameter\") return { \"name\": name, \"type\": \"range\", \"bounds\": [low, high], \"value_type\": val.type, \"log_scale\": val.log_scale, } def _ax_choice(self, name, val): \"\"\"Assemble the dictionary for ax choice parameters *Args*: name: parameter name val: current attr val *Returns*: dictionary that can be added to a parameter list \"\"\" val = self._try_choice_cast(val, type_string=\"ChoiceHyperParameter\") return { \"name\": name, \"type\": \"choice\", \"values\": val.choices, \"value_type\": val.type, }","title":"Module spock.addons.tune.ax"},{"location":"reference/spock/addons/tune/ax/#classes","text":"","title":"Classes"},{"location":"reference/spock/addons/tune/ax/#axinterface","text":"class AxInterface ( tuner_config : spock . addons . tune . config . AxTunerConfig , tuner_namespace ) ??? example \"View Source\" class AxInterface(BaseInterface): \"\"\"Specific override to support the Ax backend -- supports the service style API from Ax\"\"\" def __init__(self, tuner_config: AxTunerConfig, tuner_namespace): \"\"\"AxInterface init call that maps variables, creates a map to fnc calls, and constructs the necessary underlying objects *Args*: tuner_config: configuration object for the ax backend tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" super(AxInterface, self).__init__(tuner_config, tuner_namespace) self._tuner_obj = AxClient( generation_strategy=self._tuner_config.generation_strategy, enforce_sequential_optimization=self._tuner_config.enforce_sequential_optimization, random_seed=self._tuner_config.random_seed, verbose_logging=self._tuner_config.verbose_logging, ) # Some variables to use later self._trial_index = None self._sample_hash = None # Mapping spock underlying classes to ax distributions (search space) self._map_type = { \"RangeHyperParameter\": { \"int\": self._ax_range, \"float\": self._ax_range, }, \"ChoiceHyperParameter\": { \"int\": self._ax_choice, \"float\": self._ax_choice, \"str\": self._ax_choice, \"bool\": self._ax_choice, }, } # Build the correct underlying dictionary object for Ax client create experiment self._param_obj = self._construct() # Create the AxClient experiment self._tuner_obj.create_experiment( parameters=self._param_obj, name=self._tuner_config.name, objective_name=self._tuner_config.objective_name, minimize=self._tuner_config.minimize, parameter_constraints=self._tuner_config.parameter_constraints, outcome_constraints=self._tuner_config.outcome_constraints, overwrite_existing_experiment=self._tuner_config.overwrite_existing_experiment, tracking_metric_names=self._tuner_config.tracking_metric_names, immutable_search_space_and_opt_config=self._tuner_config.immutable_search_space_and_opt_config, is_test=self._tuner_config.is_test, ) @property def tuner_status(self) -> AxTunerStatus: return AxTunerStatus(client=self._tuner_obj, trial_index=self._trial_index) @property def best(self): best_obj = self._tuner_obj.get_best_parameters() rollup_dict, _ = self._sample_rollup(best_obj[0]) return ( self._gen_spockspace(rollup_dict), best_obj[1][0][self._tuner_obj.objective_name], ) @property def _get_sample(self): return self._tuner_obj.get_next_trial() def sample(self): parameters, self._trial_index = self._get_sample # Roll this back out into a Spockspace so it can be merged into the fixed parameter Spockspace # Also need to un-dot the param names to rebuild the nested structure rollup_dict, sample_hash = self._sample_rollup(parameters) self._sample_hash = sample_hash return self._gen_spockspace(rollup_dict) def _construct(self): param_list = [] # These will only be nested one level deep given the tuner syntax for k, v in vars(self._tuner_namespace).items(): for ik, iv in vars(v).items(): param_fn = self._map_type[type(iv).__name__][iv.type] param_list.append(param_fn(name=f\"{k}.{ik}\", val=iv)) return param_list def _ax_range(self, name, val): \"\"\"Assemble the dictionary for ax range parameters *Args*: name: parameter name val: current attr val *Returns*: dictionary that can be added to a parameter list \"\"\" low, high = self._try_range_cast(val, type_string=\"RangeHyperParameter\") return { \"name\": name, \"type\": \"range\", \"bounds\": [low, high], \"value_type\": val.type, \"log_scale\": val.log_scale, } def _ax_choice(self, name, val): \"\"\"Assemble the dictionary for ax choice parameters *Args*: name: parameter name val: current attr val *Returns*: dictionary that can be added to a parameter list \"\"\" val = self._try_choice_cast(val, type_string=\"ChoiceHyperParameter\") return { \"name\": name, \"type\": \"choice\", \"values\": val.choices, \"value_type\": val.type, }","title":"AxInterface"},{"location":"reference/spock/addons/tune/ax/#ancestors-in-mro","text":"spock.addons.tune.interface.BaseInterface abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/addons/tune/ax/#instance-variables","text":"best tuner_status","title":"Instance variables"},{"location":"reference/spock/addons/tune/ax/#methods","text":"","title":"Methods"},{"location":"reference/spock/addons/tune/ax/#sample","text":"def sample ( self ) Calls the underlying library sample to get a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) Returns : Spockspace of the current hyper-parameter draw ??? example \"View Source\" def sample(self): parameters, self._trial_index = self._get_sample # Roll this back out into a Spockspace so it can be merged into the fixed parameter Spockspace # Also need to un-dot the param names to rebuild the nested structure rollup_dict, sample_hash = self._sample_rollup(parameters) self._sample_hash = sample_hash return self._gen_spockspace(rollup_dict)","title":"sample"},{"location":"reference/spock/addons/tune/ax/#axtunerstatus","text":"class AxTunerStatus ( / , * args , ** kwargs ) ??? example \"View Source\" class AxTunerStatus(TypedDict): \"\"\"Tuner status return object for Ax -- supports the service style API from Ax *Attributes*: client: current AxClient instance trial_index: current trial index \"\"\" client: AxClient trial_index: int","title":"AxTunerStatus"},{"location":"reference/spock/addons/tune/ax/#ancestors-in-mro_1","text":"builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/spock/addons/tune/ax/#methods_1","text":"","title":"Methods"},{"location":"reference/spock/addons/tune/ax/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/spock/addons/tune/ax/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/spock/addons/tune/ax/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/spock/addons/tune/ax/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/spock/addons/tune/ax/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/spock/addons/tune/ax/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/spock/addons/tune/ax/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/spock/addons/tune/ax/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/spock/addons/tune/ax/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/spock/addons/tune/ax/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/spock/addons/tune/ax/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/spock/addons/tune/builder/","text":"Module spock.addons.tune.builder Handles the tuner builder backend None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the tuner builder backend\"\"\" from spock.backend.builder import BaseBuilder from spock.utils import make_argument class TunerBuilder(BaseBuilder): def __init__(self, *args, **kwargs): \"\"\"TunerBuilder init Args: *args: list of input classes that link to a backend configs: None or List of configs to read from desc: description for the arg parser no_cmd_line: flag to force no command line reads **kwargs: any extra keyword args \"\"\" super().__init__(*args, module_name=\"spock.addons.tune.config\", **kwargs) def _handle_arguments(self, args, class_obj): \"\"\"Ovverides base -- Handles all argument mapping Creates a dictionary of named parameters that are mapped to the final type of object *Args*: args: read file arguments class_obj: instance of a class obj *Returns*: fields: dictionary of mapped parameters \"\"\" attr_name = class_obj.__name__ fields = { val.name: val.type(**args[attr_name][val.name]) for val in class_obj.__attrs_attrs__ } return fields @staticmethod def _make_group_override_parser(parser, class_obj, class_name): \"\"\"Makes a name specific override parser for a given class obj Takes a class object of the backend and adds a new argument group with argument names given with name Class.val.(unrolled config parameters) so that individual parameters specific to a class can be overridden. *Args*: parser: argument parser class_obj: instance of a backend class class_name: used for module matching *Returns*: parser: argument parser with new class specific overrides \"\"\" attr_name = class_obj.__name__ group_parser = parser.add_argument_group( title=str(attr_name) + \" Specific Overrides\" ) for val in class_obj.__attrs_attrs__: val_type = val.metadata[\"type\"] if \"type\" in val.metadata else val.type for arg in val_type.__attrs_attrs__: arg_name = f\"--{str(attr_name)}.{val.name}.{arg.name}\" group_parser = make_argument(arg_name, arg.type, group_parser) return parser def _extract_fnc(self, val, module_name): return self._extract_other_types(val.type, module_name) Classes TunerBuilder class TunerBuilder ( * args , ** kwargs ) ??? example \"View Source\" class TunerBuilder(BaseBuilder): def __init__(self, *args, **kwargs): \"\"\"TunerBuilder init Args: *args: list of input classes that link to a backend configs: None or List of configs to read from desc: description for the arg parser no_cmd_line: flag to force no command line reads **kwargs: any extra keyword args \"\"\" super().__init__(*args, module_name=\"spock.addons.tune.config\", **kwargs) def _handle_arguments(self, args, class_obj): \"\"\"Ovverides base -- Handles all argument mapping Creates a dictionary of named parameters that are mapped to the final type of object *Args*: args: read file arguments class_obj: instance of a class obj *Returns*: fields: dictionary of mapped parameters \"\"\" attr_name = class_obj.__name__ fields = { val.name: val.type(**args[attr_name][val.name]) for val in class_obj.__attrs_attrs__ } return fields @staticmethod def _make_group_override_parser(parser, class_obj, class_name): \"\"\"Makes a name specific override parser for a given class obj Takes a class object of the backend and adds a new argument group with argument names given with name Class.val.(unrolled config parameters) so that individual parameters specific to a class can be overridden. *Args*: parser: argument parser class_obj: instance of a backend class class_name: used for module matching *Returns*: parser: argument parser with new class specific overrides \"\"\" attr_name = class_obj.__name__ group_parser = parser.add_argument_group( title=str(attr_name) + \" Specific Overrides\" ) for val in class_obj.__attrs_attrs__: val_type = val.metadata[\"type\"] if \"type\" in val.metadata else val.type for arg in val_type.__attrs_attrs__: arg_name = f\"--{str(attr_name)}.{val.name}.{arg.name}\" group_parser = make_argument(arg_name, arg.type, group_parser) return parser def _extract_fnc(self, val, module_name): return self._extract_other_types(val.type, module_name) Ancestors (in MRO) spock.backend.builder.BaseBuilder abc.ABC Methods build_override_parsers def build_override_parsers ( self , parser ) Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers Args : parser: argument parser Returns : parser: argument parser with new class specific overrides ??? example \"View Source\" def build_override_parsers(self, parser): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers *Args*: parser: argument parser *Returns*: parser: argument parser with new class specific overrides \"\"\" # Build out each class override specific parser for val in self.input_classes: parser = self._make_group_override_parser( parser=parser, class_obj=val, class_name=self._module_name ) return parser generate def generate ( self , dict_args ) Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values Args : dict_args: dictionary of arguments from the configs Returns : namespace containing automatically generated instances of the classes ??? example \"View Source\" def generate(self, dict_args): \"\"\"Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values *Args*: dict_args: dictionary of arguments from the configs *Returns*: namespace containing automatically generated instances of the classes \"\"\" auto_dict = {} for attr_classes in self.input_classes: attr_build = self._auto_generate(dict_args, attr_classes) if isinstance(attr_build, list): class_name = list({type(val).__name__ for val in attr_build}) if len(class_name) > 1: raise ValueError(\"Repeated class has more than one unique name\") auto_dict.update({class_name[0]: attr_build}) else: auto_dict.update({type(attr_build).__name__: attr_build}) return Spockspace(**auto_dict) handle_help_info def handle_help_info ( self ) Handles walking through classes to get help info For each class this function will search doc and attempt to pull out help information for both the class itself and each attribute within the class Returns : None ??? example \"View Source\" def handle_help_info(self): \"\"\"Handles walking through classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class *Returns*: None \"\"\" self._attrs_help(self.input_classes, self._module_name)","title":"Builder"},{"location":"reference/spock/addons/tune/builder/#module-spockaddonstunebuilder","text":"Handles the tuner builder backend None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the tuner builder backend\"\"\" from spock.backend.builder import BaseBuilder from spock.utils import make_argument class TunerBuilder(BaseBuilder): def __init__(self, *args, **kwargs): \"\"\"TunerBuilder init Args: *args: list of input classes that link to a backend configs: None or List of configs to read from desc: description for the arg parser no_cmd_line: flag to force no command line reads **kwargs: any extra keyword args \"\"\" super().__init__(*args, module_name=\"spock.addons.tune.config\", **kwargs) def _handle_arguments(self, args, class_obj): \"\"\"Ovverides base -- Handles all argument mapping Creates a dictionary of named parameters that are mapped to the final type of object *Args*: args: read file arguments class_obj: instance of a class obj *Returns*: fields: dictionary of mapped parameters \"\"\" attr_name = class_obj.__name__ fields = { val.name: val.type(**args[attr_name][val.name]) for val in class_obj.__attrs_attrs__ } return fields @staticmethod def _make_group_override_parser(parser, class_obj, class_name): \"\"\"Makes a name specific override parser for a given class obj Takes a class object of the backend and adds a new argument group with argument names given with name Class.val.(unrolled config parameters) so that individual parameters specific to a class can be overridden. *Args*: parser: argument parser class_obj: instance of a backend class class_name: used for module matching *Returns*: parser: argument parser with new class specific overrides \"\"\" attr_name = class_obj.__name__ group_parser = parser.add_argument_group( title=str(attr_name) + \" Specific Overrides\" ) for val in class_obj.__attrs_attrs__: val_type = val.metadata[\"type\"] if \"type\" in val.metadata else val.type for arg in val_type.__attrs_attrs__: arg_name = f\"--{str(attr_name)}.{val.name}.{arg.name}\" group_parser = make_argument(arg_name, arg.type, group_parser) return parser def _extract_fnc(self, val, module_name): return self._extract_other_types(val.type, module_name)","title":"Module spock.addons.tune.builder"},{"location":"reference/spock/addons/tune/builder/#classes","text":"","title":"Classes"},{"location":"reference/spock/addons/tune/builder/#tunerbuilder","text":"class TunerBuilder ( * args , ** kwargs ) ??? example \"View Source\" class TunerBuilder(BaseBuilder): def __init__(self, *args, **kwargs): \"\"\"TunerBuilder init Args: *args: list of input classes that link to a backend configs: None or List of configs to read from desc: description for the arg parser no_cmd_line: flag to force no command line reads **kwargs: any extra keyword args \"\"\" super().__init__(*args, module_name=\"spock.addons.tune.config\", **kwargs) def _handle_arguments(self, args, class_obj): \"\"\"Ovverides base -- Handles all argument mapping Creates a dictionary of named parameters that are mapped to the final type of object *Args*: args: read file arguments class_obj: instance of a class obj *Returns*: fields: dictionary of mapped parameters \"\"\" attr_name = class_obj.__name__ fields = { val.name: val.type(**args[attr_name][val.name]) for val in class_obj.__attrs_attrs__ } return fields @staticmethod def _make_group_override_parser(parser, class_obj, class_name): \"\"\"Makes a name specific override parser for a given class obj Takes a class object of the backend and adds a new argument group with argument names given with name Class.val.(unrolled config parameters) so that individual parameters specific to a class can be overridden. *Args*: parser: argument parser class_obj: instance of a backend class class_name: used for module matching *Returns*: parser: argument parser with new class specific overrides \"\"\" attr_name = class_obj.__name__ group_parser = parser.add_argument_group( title=str(attr_name) + \" Specific Overrides\" ) for val in class_obj.__attrs_attrs__: val_type = val.metadata[\"type\"] if \"type\" in val.metadata else val.type for arg in val_type.__attrs_attrs__: arg_name = f\"--{str(attr_name)}.{val.name}.{arg.name}\" group_parser = make_argument(arg_name, arg.type, group_parser) return parser def _extract_fnc(self, val, module_name): return self._extract_other_types(val.type, module_name)","title":"TunerBuilder"},{"location":"reference/spock/addons/tune/builder/#ancestors-in-mro","text":"spock.backend.builder.BaseBuilder abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/addons/tune/builder/#methods","text":"","title":"Methods"},{"location":"reference/spock/addons/tune/builder/#build_override_parsers","text":"def build_override_parsers ( self , parser ) Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers Args : parser: argument parser Returns : parser: argument parser with new class specific overrides ??? example \"View Source\" def build_override_parsers(self, parser): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers *Args*: parser: argument parser *Returns*: parser: argument parser with new class specific overrides \"\"\" # Build out each class override specific parser for val in self.input_classes: parser = self._make_group_override_parser( parser=parser, class_obj=val, class_name=self._module_name ) return parser","title":"build_override_parsers"},{"location":"reference/spock/addons/tune/builder/#generate","text":"def generate ( self , dict_args ) Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values Args : dict_args: dictionary of arguments from the configs Returns : namespace containing automatically generated instances of the classes ??? example \"View Source\" def generate(self, dict_args): \"\"\"Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values *Args*: dict_args: dictionary of arguments from the configs *Returns*: namespace containing automatically generated instances of the classes \"\"\" auto_dict = {} for attr_classes in self.input_classes: attr_build = self._auto_generate(dict_args, attr_classes) if isinstance(attr_build, list): class_name = list({type(val).__name__ for val in attr_build}) if len(class_name) > 1: raise ValueError(\"Repeated class has more than one unique name\") auto_dict.update({class_name[0]: attr_build}) else: auto_dict.update({type(attr_build).__name__: attr_build}) return Spockspace(**auto_dict)","title":"generate"},{"location":"reference/spock/addons/tune/builder/#handle_help_info","text":"def handle_help_info ( self ) Handles walking through classes to get help info For each class this function will search doc and attempt to pull out help information for both the class itself and each attribute within the class Returns : None ??? example \"View Source\" def handle_help_info(self): \"\"\"Handles walking through classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class *Returns*: None \"\"\" self._attrs_help(self.input_classes, self._module_name)","title":"handle_help_info"},{"location":"reference/spock/addons/tune/config/","text":"Module spock.addons.tune.config Creates the spock config interface that wraps attr -- tune version for hyper-parameters None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Creates the spock config interface that wraps attr -- tune version for hyper-parameters\"\"\" import sys from typing import List, Optional, Sequence, Tuple, Union from uuid import uuid4 import attr import optuna from ax.modelbridge.generation_strategy import GenerationStrategy from spock.backend.config import _base_attr @attr.s(auto_attribs=True) class AxTunerConfig: objective_name: str tracking_metric_names: Optional[List[str]] = None name: Optional[str] = f\"spock_ax_{uuid4()}\" minimize: bool = True parameter_constraints: Optional[List[str]] = None outcome_constraints: Optional[List[str]] = None support_intermediate_data: bool = False overwrite_existing_experiment: bool = False immutable_search_space_and_opt_config: bool = True is_test: bool = False generation_strategy: Optional[GenerationStrategy] = None enforce_sequential_optimization: bool = True random_seed: Optional[int] = None verbose_logging: bool = True @attr.s(auto_attribs=True) class OptunaTunerConfig: storage: Optional[Union[str, optuna.storages.BaseStorage]] = None sampler: Optional[optuna.samplers.BaseSampler] = None pruner: Optional[optuna.pruners.BasePruner] = None study_name: Optional[str] = f\"spock_optuna_{uuid4()}\" direction: Optional[Union[str, optuna.study.StudyDirection]] = None load_if_exists: bool = False directions: Optional[Sequence[Union[str, optuna.study.StudyDirection]]] = None def _spock_tune(cls): \"\"\"Ovverides basic spock_attr decorator with another name Using a different name allows spock to easily determine which parameters are normal and which are meant to be used in a hyper-parameter tuning backend *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" bases, attrs_dict = _base_attr(cls) # Dynamically make an attr class obj = attr.make_class( name=cls.__name__, bases=bases, attrs=attrs_dict, kw_only=True, frozen=True ) # For each class we dynamically create we need to register it within the system modules for pickle to work setattr(sys.modules[\"spock\"].addons.tune.config, obj.__name__, obj) # Swap the __doc__ string from cls to obj obj.__doc__ = cls.__doc__ return obj # Make the alias for the decorator spockTuner = _spock_tune @attr.s class RangeHyperParameter: \"\"\"Range based hyper-parameter that is sampled uniformly Attributes: type: type of the hyper-parameter (note: spock will attempt to autocast into this type) bounds: min and max of the hyper-parameter range log_scale: log scale the values before sampling \"\"\" type = attr.ib( type=str, validator=[ attr.validators.instance_of(str), attr.validators.in_([\"float\", \"int\"]), ], ) bounds = attr.ib( type=Union[Tuple[float, float], Tuple[int, int]], validator=attr.validators.deep_iterable( member_validator=attr.validators.instance_of((float, int)), iterable_validator=attr.validators.instance_of(tuple), ), ) log_scale = attr.ib(type=bool, validator=attr.validators.instance_of(bool)) @attr.s class ChoiceHyperParameter: \"\"\"Choice based hyper-parameter that is sampled uniformly Attributes: type: type of the hyper-parameter -- (note: spock will attempt to autocast into this type) choices: list of variable length that contains all the possible choices to select from \"\"\" type = attr.ib( type=str, validator=[ attr.validators.instance_of(str), attr.validators.in_([\"float\", \"int\", \"str\", \"bool\"]), ], ) choices = attr.ib( type=Union[List[str], List[int], List[float], List[bool]], validator=attr.validators.deep_iterable( member_validator=attr.validators.instance_of((float, int, bool, str)), iterable_validator=attr.validators.instance_of(list), ), ) Functions spockTuner def spockTuner ( cls ) Ovverides basic spock_attr decorator with another name Using a different name allows spock to easily determine which parameters are normal and which are meant to be used in a hyper-parameter tuning backend Args : cls: basic class def Returns : cls: slotted attrs class that is frozen and kw only ??? example \"View Source\" def _spock_tune(cls): \"\"\"Ovverides basic spock_attr decorator with another name Using a different name allows spock to easily determine which parameters are normal and which are meant to be used in a hyper-parameter tuning backend *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" bases, attrs_dict = _base_attr(cls) # Dynamically make an attr class obj = attr.make_class( name=cls.__name__, bases=bases, attrs=attrs_dict, kw_only=True, frozen=True ) # For each class we dynamically create we need to register it within the system modules for pickle to work setattr(sys.modules[\"spock\"].addons.tune.config, obj.__name__, obj) # Swap the __doc__ string from cls to obj obj.__doc__ = cls.__doc__ return obj Classes AxTunerConfig class AxTunerConfig ( objective_name : str , tracking_metric_names : Union [ List [ str ], NoneType ] = None , name : Union [ str , NoneType ] = 'spock_ax_40b1f198-742e-401a-8b3c-0f7e01994f87' , minimize : bool = True , parameter_constraints : Union [ List [ str ], NoneType ] = None , outcome_constraints : Union [ List [ str ], NoneType ] = None , support_intermediate_data : bool = False , overwrite_existing_experiment : bool = False , immutable_search_space_and_opt_config : bool = True , is_test : bool = False , generation_strategy : Union [ ax . modelbridge . generation_strategy . GenerationStrategy , NoneType ] = None , enforce_sequential_optimization : bool = True , random_seed : Union [ int , NoneType ] = None , verbose_logging : bool = True ) ??? example \"View Source\" class AxTunerConfig: objective_name: str tracking_metric_names: Optional[List[str]] = None name: Optional[str] = f\"spock_ax_{uuid4()}\" minimize: bool = True parameter_constraints: Optional[List[str]] = None outcome_constraints: Optional[List[str]] = None support_intermediate_data: bool = False overwrite_existing_experiment: bool = False immutable_search_space_and_opt_config: bool = True is_test: bool = False generation_strategy: Optional[GenerationStrategy] = None enforce_sequential_optimization: bool = True random_seed: Optional[int] = None verbose_logging: bool = True ChoiceHyperParameter class ChoiceHyperParameter ( type : str , choices : Union [ List [ str ], List [ int ], List [ float ], List [ bool ]] ) Attributes Name Type Description Default type None type of the hyper-parameter -- (note: spock will attempt to autocast into this type) None choices None list of variable length that contains all the possible choices to select from None ??? example \"View Source\" class ChoiceHyperParameter: \"\"\"Choice based hyper-parameter that is sampled uniformly Attributes: type: type of the hyper-parameter -- (note: spock will attempt to autocast into this type) choices: list of variable length that contains all the possible choices to select from \"\"\" type = attr.ib( type=str, validator=[ attr.validators.instance_of(str), attr.validators.in_([\"float\", \"int\", \"str\", \"bool\"]), ], ) choices = attr.ib( type=Union[List[str], List[int], List[float], List[bool]], validator=attr.validators.deep_iterable( member_validator=attr.validators.instance_of((float, int, bool, str)), iterable_validator=attr.validators.instance_of(list), ), ) Class variables choices type OptunaTunerConfig class OptunaTunerConfig ( storage : Union [ str , optuna . storages . _base . BaseStorage , NoneType ] = None , sampler : Union [ optuna . samplers . _base . BaseSampler , NoneType ] = None , pruner : Union [ optuna . pruners . _base . BasePruner , NoneType ] = None , study_name : Union [ str , NoneType ] = 'spock_optuna_5966f43f-329f-4d58-ab83-c4257b667d63' , direction : Union [ str , optuna . study . _study_direction . StudyDirection , NoneType ] = None , load_if_exists : bool = False , directions : Union [ Sequence [ Union [ str , optuna . study . _study_direction . StudyDirection ]], NoneType ] = None ) ??? example \"View Source\" class OptunaTunerConfig: storage: Optional[Union[str, optuna.storages.BaseStorage]] = None sampler: Optional[optuna.samplers.BaseSampler] = None pruner: Optional[optuna.pruners.BasePruner] = None study_name: Optional[str] = f\"spock_optuna_{uuid4()}\" direction: Optional[Union[str, optuna.study.StudyDirection]] = None load_if_exists: bool = False directions: Optional[Sequence[Union[str, optuna.study.StudyDirection]]] = None RangeHyperParameter class RangeHyperParameter ( type : str , bounds : Union [ Tuple [ float , float ], Tuple [ int , int ]], log_scale : bool ) Attributes Name Type Description Default type None type of the hyper-parameter (note: spock will attempt to autocast into this type) None bounds None min and max of the hyper-parameter range None log_scale None log scale the values before sampling None ??? example \"View Source\" class RangeHyperParameter: \"\"\"Range based hyper-parameter that is sampled uniformly Attributes: type: type of the hyper-parameter (note: spock will attempt to autocast into this type) bounds: min and max of the hyper-parameter range log_scale: log scale the values before sampling \"\"\" type = attr.ib( type=str, validator=[ attr.validators.instance_of(str), attr.validators.in_([\"float\", \"int\"]), ], ) bounds = attr.ib( type=Union[Tuple[float, float], Tuple[int, int]], validator=attr.validators.deep_iterable( member_validator=attr.validators.instance_of((float, int)), iterable_validator=attr.validators.instance_of(tuple), ), ) log_scale = attr.ib(type=bool, validator=attr.validators.instance_of(bool)) Class variables bounds log_scale type","title":"Config"},{"location":"reference/spock/addons/tune/config/#module-spockaddonstuneconfig","text":"Creates the spock config interface that wraps attr -- tune version for hyper-parameters None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Creates the spock config interface that wraps attr -- tune version for hyper-parameters\"\"\" import sys from typing import List, Optional, Sequence, Tuple, Union from uuid import uuid4 import attr import optuna from ax.modelbridge.generation_strategy import GenerationStrategy from spock.backend.config import _base_attr @attr.s(auto_attribs=True) class AxTunerConfig: objective_name: str tracking_metric_names: Optional[List[str]] = None name: Optional[str] = f\"spock_ax_{uuid4()}\" minimize: bool = True parameter_constraints: Optional[List[str]] = None outcome_constraints: Optional[List[str]] = None support_intermediate_data: bool = False overwrite_existing_experiment: bool = False immutable_search_space_and_opt_config: bool = True is_test: bool = False generation_strategy: Optional[GenerationStrategy] = None enforce_sequential_optimization: bool = True random_seed: Optional[int] = None verbose_logging: bool = True @attr.s(auto_attribs=True) class OptunaTunerConfig: storage: Optional[Union[str, optuna.storages.BaseStorage]] = None sampler: Optional[optuna.samplers.BaseSampler] = None pruner: Optional[optuna.pruners.BasePruner] = None study_name: Optional[str] = f\"spock_optuna_{uuid4()}\" direction: Optional[Union[str, optuna.study.StudyDirection]] = None load_if_exists: bool = False directions: Optional[Sequence[Union[str, optuna.study.StudyDirection]]] = None def _spock_tune(cls): \"\"\"Ovverides basic spock_attr decorator with another name Using a different name allows spock to easily determine which parameters are normal and which are meant to be used in a hyper-parameter tuning backend *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" bases, attrs_dict = _base_attr(cls) # Dynamically make an attr class obj = attr.make_class( name=cls.__name__, bases=bases, attrs=attrs_dict, kw_only=True, frozen=True ) # For each class we dynamically create we need to register it within the system modules for pickle to work setattr(sys.modules[\"spock\"].addons.tune.config, obj.__name__, obj) # Swap the __doc__ string from cls to obj obj.__doc__ = cls.__doc__ return obj # Make the alias for the decorator spockTuner = _spock_tune @attr.s class RangeHyperParameter: \"\"\"Range based hyper-parameter that is sampled uniformly Attributes: type: type of the hyper-parameter (note: spock will attempt to autocast into this type) bounds: min and max of the hyper-parameter range log_scale: log scale the values before sampling \"\"\" type = attr.ib( type=str, validator=[ attr.validators.instance_of(str), attr.validators.in_([\"float\", \"int\"]), ], ) bounds = attr.ib( type=Union[Tuple[float, float], Tuple[int, int]], validator=attr.validators.deep_iterable( member_validator=attr.validators.instance_of((float, int)), iterable_validator=attr.validators.instance_of(tuple), ), ) log_scale = attr.ib(type=bool, validator=attr.validators.instance_of(bool)) @attr.s class ChoiceHyperParameter: \"\"\"Choice based hyper-parameter that is sampled uniformly Attributes: type: type of the hyper-parameter -- (note: spock will attempt to autocast into this type) choices: list of variable length that contains all the possible choices to select from \"\"\" type = attr.ib( type=str, validator=[ attr.validators.instance_of(str), attr.validators.in_([\"float\", \"int\", \"str\", \"bool\"]), ], ) choices = attr.ib( type=Union[List[str], List[int], List[float], List[bool]], validator=attr.validators.deep_iterable( member_validator=attr.validators.instance_of((float, int, bool, str)), iterable_validator=attr.validators.instance_of(list), ), )","title":"Module spock.addons.tune.config"},{"location":"reference/spock/addons/tune/config/#functions","text":"","title":"Functions"},{"location":"reference/spock/addons/tune/config/#spocktuner","text":"def spockTuner ( cls ) Ovverides basic spock_attr decorator with another name Using a different name allows spock to easily determine which parameters are normal and which are meant to be used in a hyper-parameter tuning backend Args : cls: basic class def Returns : cls: slotted attrs class that is frozen and kw only ??? example \"View Source\" def _spock_tune(cls): \"\"\"Ovverides basic spock_attr decorator with another name Using a different name allows spock to easily determine which parameters are normal and which are meant to be used in a hyper-parameter tuning backend *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" bases, attrs_dict = _base_attr(cls) # Dynamically make an attr class obj = attr.make_class( name=cls.__name__, bases=bases, attrs=attrs_dict, kw_only=True, frozen=True ) # For each class we dynamically create we need to register it within the system modules for pickle to work setattr(sys.modules[\"spock\"].addons.tune.config, obj.__name__, obj) # Swap the __doc__ string from cls to obj obj.__doc__ = cls.__doc__ return obj","title":"spockTuner"},{"location":"reference/spock/addons/tune/config/#classes","text":"","title":"Classes"},{"location":"reference/spock/addons/tune/config/#axtunerconfig","text":"class AxTunerConfig ( objective_name : str , tracking_metric_names : Union [ List [ str ], NoneType ] = None , name : Union [ str , NoneType ] = 'spock_ax_40b1f198-742e-401a-8b3c-0f7e01994f87' , minimize : bool = True , parameter_constraints : Union [ List [ str ], NoneType ] = None , outcome_constraints : Union [ List [ str ], NoneType ] = None , support_intermediate_data : bool = False , overwrite_existing_experiment : bool = False , immutable_search_space_and_opt_config : bool = True , is_test : bool = False , generation_strategy : Union [ ax . modelbridge . generation_strategy . GenerationStrategy , NoneType ] = None , enforce_sequential_optimization : bool = True , random_seed : Union [ int , NoneType ] = None , verbose_logging : bool = True ) ??? example \"View Source\" class AxTunerConfig: objective_name: str tracking_metric_names: Optional[List[str]] = None name: Optional[str] = f\"spock_ax_{uuid4()}\" minimize: bool = True parameter_constraints: Optional[List[str]] = None outcome_constraints: Optional[List[str]] = None support_intermediate_data: bool = False overwrite_existing_experiment: bool = False immutable_search_space_and_opt_config: bool = True is_test: bool = False generation_strategy: Optional[GenerationStrategy] = None enforce_sequential_optimization: bool = True random_seed: Optional[int] = None verbose_logging: bool = True","title":"AxTunerConfig"},{"location":"reference/spock/addons/tune/config/#choicehyperparameter","text":"class ChoiceHyperParameter ( type : str , choices : Union [ List [ str ], List [ int ], List [ float ], List [ bool ]] )","title":"ChoiceHyperParameter"},{"location":"reference/spock/addons/tune/config/#attributes","text":"Name Type Description Default type None type of the hyper-parameter -- (note: spock will attempt to autocast into this type) None choices None list of variable length that contains all the possible choices to select from None ??? example \"View Source\" class ChoiceHyperParameter: \"\"\"Choice based hyper-parameter that is sampled uniformly Attributes: type: type of the hyper-parameter -- (note: spock will attempt to autocast into this type) choices: list of variable length that contains all the possible choices to select from \"\"\" type = attr.ib( type=str, validator=[ attr.validators.instance_of(str), attr.validators.in_([\"float\", \"int\", \"str\", \"bool\"]), ], ) choices = attr.ib( type=Union[List[str], List[int], List[float], List[bool]], validator=attr.validators.deep_iterable( member_validator=attr.validators.instance_of((float, int, bool, str)), iterable_validator=attr.validators.instance_of(list), ), )","title":"Attributes"},{"location":"reference/spock/addons/tune/config/#class-variables","text":"choices type","title":"Class variables"},{"location":"reference/spock/addons/tune/config/#optunatunerconfig","text":"class OptunaTunerConfig ( storage : Union [ str , optuna . storages . _base . BaseStorage , NoneType ] = None , sampler : Union [ optuna . samplers . _base . BaseSampler , NoneType ] = None , pruner : Union [ optuna . pruners . _base . BasePruner , NoneType ] = None , study_name : Union [ str , NoneType ] = 'spock_optuna_5966f43f-329f-4d58-ab83-c4257b667d63' , direction : Union [ str , optuna . study . _study_direction . StudyDirection , NoneType ] = None , load_if_exists : bool = False , directions : Union [ Sequence [ Union [ str , optuna . study . _study_direction . StudyDirection ]], NoneType ] = None ) ??? example \"View Source\" class OptunaTunerConfig: storage: Optional[Union[str, optuna.storages.BaseStorage]] = None sampler: Optional[optuna.samplers.BaseSampler] = None pruner: Optional[optuna.pruners.BasePruner] = None study_name: Optional[str] = f\"spock_optuna_{uuid4()}\" direction: Optional[Union[str, optuna.study.StudyDirection]] = None load_if_exists: bool = False directions: Optional[Sequence[Union[str, optuna.study.StudyDirection]]] = None","title":"OptunaTunerConfig"},{"location":"reference/spock/addons/tune/config/#rangehyperparameter","text":"class RangeHyperParameter ( type : str , bounds : Union [ Tuple [ float , float ], Tuple [ int , int ]], log_scale : bool )","title":"RangeHyperParameter"},{"location":"reference/spock/addons/tune/config/#attributes_1","text":"Name Type Description Default type None type of the hyper-parameter (note: spock will attempt to autocast into this type) None bounds None min and max of the hyper-parameter range None log_scale None log scale the values before sampling None ??? example \"View Source\" class RangeHyperParameter: \"\"\"Range based hyper-parameter that is sampled uniformly Attributes: type: type of the hyper-parameter (note: spock will attempt to autocast into this type) bounds: min and max of the hyper-parameter range log_scale: log scale the values before sampling \"\"\" type = attr.ib( type=str, validator=[ attr.validators.instance_of(str), attr.validators.in_([\"float\", \"int\"]), ], ) bounds = attr.ib( type=Union[Tuple[float, float], Tuple[int, int]], validator=attr.validators.deep_iterable( member_validator=attr.validators.instance_of((float, int)), iterable_validator=attr.validators.instance_of(tuple), ), ) log_scale = attr.ib(type=bool, validator=attr.validators.instance_of(bool))","title":"Attributes"},{"location":"reference/spock/addons/tune/config/#class-variables_1","text":"bounds log_scale type","title":"Class variables"},{"location":"reference/spock/addons/tune/interface/","text":"Module spock.addons.tune.interface Handles the base interface None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the base interface\"\"\" import hashlib import json from abc import ABC, abstractmethod from typing import Dict, Union import attr from spock.addons.tune.config import AxTunerConfig, OptunaTunerConfig from spock.backend.wrappers import Spockspace class BaseInterface(ABC): \"\"\"Base interface for the various hyper-parameter tuner backends *Attributes* _tuner_config: spock version of the tuner configuration _tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" def __init__(self, tuner_config, tuner_namespace: Spockspace): \"\"\"Base init call that maps a few variables *Args*: tuner_config: necessary dict object to determine the interface and sample correctly from the underlying library tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" self._tuner_config = tuner_config self._tuner_namespace = tuner_namespace @abstractmethod def sample(self): \"\"\"Calls the underlying library sample to get a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) *Returns*: Spockspace of the current hyper-parameter draw \"\"\" pass @abstractmethod def _construct(self): \"\"\"Constructs the base object needed by the underlying library to construct the correct object that allows for hyper-parameter sampling *Returns*: flat dictionary of all hyper-parameters named with dot notation (class.param_name) \"\"\" pass @property @abstractmethod def _get_sample(self): \"\"\"Gets the sample parameter dictionary from the underlying backend\"\"\" pass @property @abstractmethod def tuner_status(self): \"\"\"Returns a dictionary of all the necessary underlying tuner internals to report the result\"\"\" pass @property @abstractmethod def best(self): \"\"\"Returns a Spockspace of the best hyper-parameter config and the associated metric value\"\"\" @staticmethod def _sample_rollup(params): \"\"\"Rollup the sample draw into a dictionary that can be converted to a spockspace with the correct names and roots -- un-dots the name structure *Args*: params: current parameter dictionary -- named by dot notation *Returns*: dictionary of rolled up sampled parameters md5 hash of the dictionary contents \"\"\" key_set = {k.split(\".\")[0] for k in params.keys()} rollup_dict = {val: {} for val in key_set} for k, v in params.items(): split_names = k.split(\".\") rollup_dict[split_names[0]].update({split_names[1]: v}) dict_hash = hashlib.md5( json.dumps(rollup_dict, sort_keys=True).encode(\"utf-8\") ).digest() return rollup_dict, dict_hash def _gen_spockspace(self, tune_dict: Dict): \"\"\"Converts a dictionary of dictionaries of parameters into a valid Spockspace *Args*: tune_dict: dictionary of current parameters Returns: tune_dict: Spockspace \"\"\" for k, v in tune_dict.items(): attrs_dict = { ik: attr.ib( validator=attr.validators.instance_of(type(iv)), type=type(iv) ) for ik, iv in v.items() } obj = attr.make_class(name=k, attrs=attrs_dict, kw_only=True, frozen=True) tune_dict.update({k: obj(**v)}) return self._to_spockspace(tune_dict) @staticmethod def _config_to_dict(tuner_config: Union[OptunaTunerConfig, AxTunerConfig]): \"\"\"Turns an attrs config object into a dictionary *Args*: tuner_config: attrs config object *Returns*: dictionary of the attrs config object \"\"\" return {k: v for k, v in attr.asdict(tuner_config).items() if v is not None} @staticmethod def _to_spockspace(tune_dict: Dict): \"\"\"Converts a dict to a Spockspace *Args*: tune_dict: current dictionary *Returns*: Spockspace of dict \"\"\" return Spockspace(**tune_dict) @staticmethod def _get_caster(val): \"\"\"Gets a callable type object from a string type *Args*: val: current attr val: *Returns*: type class object \"\"\" return __builtins__[val.type] def _try_choice_cast(self, val, type_string: str): \"\"\"Try/except for casting choice parameters *Args*: val: current attr val type_string: spock hyper-parameter type name *Returns*: val: updated attr val \"\"\" caster = self._get_caster(val) # Just attempt to cast in a try except try: val.choices = [caster(v) for v in val.choices] return val except TypeError: print( f\"Attempted to cast into type: {val.type} but failed -- check the inputs to {type_string}\" ) def _try_range_cast(self, val, type_string: str): \"\"\"Try/except for casting range parameters *Args*: val: current attr val type_string: spock hyper-parameter type name *Returns*: low: low bound high: high bound \"\"\" caster = self._get_caster(val) try: low = caster(val.bounds[0]) high = caster(val.bounds[1]) return low, high except TypeError: print( f\"Attempted to cast into type: {val.type} but failed -- check the inputs to {type_string}\" ) Classes BaseInterface class BaseInterface ( tuner_config , tuner_namespace : spock . backend . wrappers . Spockspace ) ??? example \"View Source\" class BaseInterface(ABC): \"\"\"Base interface for the various hyper-parameter tuner backends *Attributes* _tuner_config: spock version of the tuner configuration _tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" def __init__(self, tuner_config, tuner_namespace: Spockspace): \"\"\"Base init call that maps a few variables *Args*: tuner_config: necessary dict object to determine the interface and sample correctly from the underlying library tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" self._tuner_config = tuner_config self._tuner_namespace = tuner_namespace @abstractmethod def sample(self): \"\"\"Calls the underlying library sample to get a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) *Returns*: Spockspace of the current hyper-parameter draw \"\"\" pass @abstractmethod def _construct(self): \"\"\"Constructs the base object needed by the underlying library to construct the correct object that allows for hyper-parameter sampling *Returns*: flat dictionary of all hyper-parameters named with dot notation (class.param_name) \"\"\" pass @property @abstractmethod def _get_sample(self): \"\"\"Gets the sample parameter dictionary from the underlying backend\"\"\" pass @property @abstractmethod def tuner_status(self): \"\"\"Returns a dictionary of all the necessary underlying tuner internals to report the result\"\"\" pass @property @abstractmethod def best(self): \"\"\"Returns a Spockspace of the best hyper-parameter config and the associated metric value\"\"\" @staticmethod def _sample_rollup(params): \"\"\"Rollup the sample draw into a dictionary that can be converted to a spockspace with the correct names and roots -- un-dots the name structure *Args*: params: current parameter dictionary -- named by dot notation *Returns*: dictionary of rolled up sampled parameters md5 hash of the dictionary contents \"\"\" key_set = {k.split(\".\")[0] for k in params.keys()} rollup_dict = {val: {} for val in key_set} for k, v in params.items(): split_names = k.split(\".\") rollup_dict[split_names[0]].update({split_names[1]: v}) dict_hash = hashlib.md5( json.dumps(rollup_dict, sort_keys=True).encode(\"utf-8\") ).digest() return rollup_dict, dict_hash def _gen_spockspace(self, tune_dict: Dict): \"\"\"Converts a dictionary of dictionaries of parameters into a valid Spockspace *Args*: tune_dict: dictionary of current parameters Returns: tune_dict: Spockspace \"\"\" for k, v in tune_dict.items(): attrs_dict = { ik: attr.ib( validator=attr.validators.instance_of(type(iv)), type=type(iv) ) for ik, iv in v.items() } obj = attr.make_class(name=k, attrs=attrs_dict, kw_only=True, frozen=True) tune_dict.update({k: obj(**v)}) return self._to_spockspace(tune_dict) @staticmethod def _config_to_dict(tuner_config: Union[OptunaTunerConfig, AxTunerConfig]): \"\"\"Turns an attrs config object into a dictionary *Args*: tuner_config: attrs config object *Returns*: dictionary of the attrs config object \"\"\" return {k: v for k, v in attr.asdict(tuner_config).items() if v is not None} @staticmethod def _to_spockspace(tune_dict: Dict): \"\"\"Converts a dict to a Spockspace *Args*: tune_dict: current dictionary *Returns*: Spockspace of dict \"\"\" return Spockspace(**tune_dict) @staticmethod def _get_caster(val): \"\"\"Gets a callable type object from a string type *Args*: val: current attr val: *Returns*: type class object \"\"\" return __builtins__[val.type] def _try_choice_cast(self, val, type_string: str): \"\"\"Try/except for casting choice parameters *Args*: val: current attr val type_string: spock hyper-parameter type name *Returns*: val: updated attr val \"\"\" caster = self._get_caster(val) # Just attempt to cast in a try except try: val.choices = [caster(v) for v in val.choices] return val except TypeError: print( f\"Attempted to cast into type: {val.type} but failed -- check the inputs to {type_string}\" ) def _try_range_cast(self, val, type_string: str): \"\"\"Try/except for casting range parameters *Args*: val: current attr val type_string: spock hyper-parameter type name *Returns*: low: low bound high: high bound \"\"\" caster = self._get_caster(val) try: low = caster(val.bounds[0]) high = caster(val.bounds[1]) return low, high except TypeError: print( f\"Attempted to cast into type: {val.type} but failed -- check the inputs to {type_string}\" ) Ancestors (in MRO) abc.ABC Descendants spock.addons.tune.ax.AxInterface spock.addons.tune.optuna.OptunaInterface Instance variables best Returns a Spockspace of the best hyper-parameter config and the associated metric value tuner_status Returns a dictionary of all the necessary underlying tuner internals to report the result Methods sample def sample ( self ) Calls the underlying library sample to get a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) Returns : Spockspace of the current hyper-parameter draw ??? example \"View Source\" @abstractmethod def sample(self): \"\"\"Calls the underlying library sample to get a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) *Returns*: Spockspace of the current hyper-parameter draw \"\"\" pass","title":"Interface"},{"location":"reference/spock/addons/tune/interface/#module-spockaddonstuneinterface","text":"Handles the base interface None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the base interface\"\"\" import hashlib import json from abc import ABC, abstractmethod from typing import Dict, Union import attr from spock.addons.tune.config import AxTunerConfig, OptunaTunerConfig from spock.backend.wrappers import Spockspace class BaseInterface(ABC): \"\"\"Base interface for the various hyper-parameter tuner backends *Attributes* _tuner_config: spock version of the tuner configuration _tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" def __init__(self, tuner_config, tuner_namespace: Spockspace): \"\"\"Base init call that maps a few variables *Args*: tuner_config: necessary dict object to determine the interface and sample correctly from the underlying library tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" self._tuner_config = tuner_config self._tuner_namespace = tuner_namespace @abstractmethod def sample(self): \"\"\"Calls the underlying library sample to get a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) *Returns*: Spockspace of the current hyper-parameter draw \"\"\" pass @abstractmethod def _construct(self): \"\"\"Constructs the base object needed by the underlying library to construct the correct object that allows for hyper-parameter sampling *Returns*: flat dictionary of all hyper-parameters named with dot notation (class.param_name) \"\"\" pass @property @abstractmethod def _get_sample(self): \"\"\"Gets the sample parameter dictionary from the underlying backend\"\"\" pass @property @abstractmethod def tuner_status(self): \"\"\"Returns a dictionary of all the necessary underlying tuner internals to report the result\"\"\" pass @property @abstractmethod def best(self): \"\"\"Returns a Spockspace of the best hyper-parameter config and the associated metric value\"\"\" @staticmethod def _sample_rollup(params): \"\"\"Rollup the sample draw into a dictionary that can be converted to a spockspace with the correct names and roots -- un-dots the name structure *Args*: params: current parameter dictionary -- named by dot notation *Returns*: dictionary of rolled up sampled parameters md5 hash of the dictionary contents \"\"\" key_set = {k.split(\".\")[0] for k in params.keys()} rollup_dict = {val: {} for val in key_set} for k, v in params.items(): split_names = k.split(\".\") rollup_dict[split_names[0]].update({split_names[1]: v}) dict_hash = hashlib.md5( json.dumps(rollup_dict, sort_keys=True).encode(\"utf-8\") ).digest() return rollup_dict, dict_hash def _gen_spockspace(self, tune_dict: Dict): \"\"\"Converts a dictionary of dictionaries of parameters into a valid Spockspace *Args*: tune_dict: dictionary of current parameters Returns: tune_dict: Spockspace \"\"\" for k, v in tune_dict.items(): attrs_dict = { ik: attr.ib( validator=attr.validators.instance_of(type(iv)), type=type(iv) ) for ik, iv in v.items() } obj = attr.make_class(name=k, attrs=attrs_dict, kw_only=True, frozen=True) tune_dict.update({k: obj(**v)}) return self._to_spockspace(tune_dict) @staticmethod def _config_to_dict(tuner_config: Union[OptunaTunerConfig, AxTunerConfig]): \"\"\"Turns an attrs config object into a dictionary *Args*: tuner_config: attrs config object *Returns*: dictionary of the attrs config object \"\"\" return {k: v for k, v in attr.asdict(tuner_config).items() if v is not None} @staticmethod def _to_spockspace(tune_dict: Dict): \"\"\"Converts a dict to a Spockspace *Args*: tune_dict: current dictionary *Returns*: Spockspace of dict \"\"\" return Spockspace(**tune_dict) @staticmethod def _get_caster(val): \"\"\"Gets a callable type object from a string type *Args*: val: current attr val: *Returns*: type class object \"\"\" return __builtins__[val.type] def _try_choice_cast(self, val, type_string: str): \"\"\"Try/except for casting choice parameters *Args*: val: current attr val type_string: spock hyper-parameter type name *Returns*: val: updated attr val \"\"\" caster = self._get_caster(val) # Just attempt to cast in a try except try: val.choices = [caster(v) for v in val.choices] return val except TypeError: print( f\"Attempted to cast into type: {val.type} but failed -- check the inputs to {type_string}\" ) def _try_range_cast(self, val, type_string: str): \"\"\"Try/except for casting range parameters *Args*: val: current attr val type_string: spock hyper-parameter type name *Returns*: low: low bound high: high bound \"\"\" caster = self._get_caster(val) try: low = caster(val.bounds[0]) high = caster(val.bounds[1]) return low, high except TypeError: print( f\"Attempted to cast into type: {val.type} but failed -- check the inputs to {type_string}\" )","title":"Module spock.addons.tune.interface"},{"location":"reference/spock/addons/tune/interface/#classes","text":"","title":"Classes"},{"location":"reference/spock/addons/tune/interface/#baseinterface","text":"class BaseInterface ( tuner_config , tuner_namespace : spock . backend . wrappers . Spockspace ) ??? example \"View Source\" class BaseInterface(ABC): \"\"\"Base interface for the various hyper-parameter tuner backends *Attributes* _tuner_config: spock version of the tuner configuration _tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" def __init__(self, tuner_config, tuner_namespace: Spockspace): \"\"\"Base init call that maps a few variables *Args*: tuner_config: necessary dict object to determine the interface and sample correctly from the underlying library tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" self._tuner_config = tuner_config self._tuner_namespace = tuner_namespace @abstractmethod def sample(self): \"\"\"Calls the underlying library sample to get a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) *Returns*: Spockspace of the current hyper-parameter draw \"\"\" pass @abstractmethod def _construct(self): \"\"\"Constructs the base object needed by the underlying library to construct the correct object that allows for hyper-parameter sampling *Returns*: flat dictionary of all hyper-parameters named with dot notation (class.param_name) \"\"\" pass @property @abstractmethod def _get_sample(self): \"\"\"Gets the sample parameter dictionary from the underlying backend\"\"\" pass @property @abstractmethod def tuner_status(self): \"\"\"Returns a dictionary of all the necessary underlying tuner internals to report the result\"\"\" pass @property @abstractmethod def best(self): \"\"\"Returns a Spockspace of the best hyper-parameter config and the associated metric value\"\"\" @staticmethod def _sample_rollup(params): \"\"\"Rollup the sample draw into a dictionary that can be converted to a spockspace with the correct names and roots -- un-dots the name structure *Args*: params: current parameter dictionary -- named by dot notation *Returns*: dictionary of rolled up sampled parameters md5 hash of the dictionary contents \"\"\" key_set = {k.split(\".\")[0] for k in params.keys()} rollup_dict = {val: {} for val in key_set} for k, v in params.items(): split_names = k.split(\".\") rollup_dict[split_names[0]].update({split_names[1]: v}) dict_hash = hashlib.md5( json.dumps(rollup_dict, sort_keys=True).encode(\"utf-8\") ).digest() return rollup_dict, dict_hash def _gen_spockspace(self, tune_dict: Dict): \"\"\"Converts a dictionary of dictionaries of parameters into a valid Spockspace *Args*: tune_dict: dictionary of current parameters Returns: tune_dict: Spockspace \"\"\" for k, v in tune_dict.items(): attrs_dict = { ik: attr.ib( validator=attr.validators.instance_of(type(iv)), type=type(iv) ) for ik, iv in v.items() } obj = attr.make_class(name=k, attrs=attrs_dict, kw_only=True, frozen=True) tune_dict.update({k: obj(**v)}) return self._to_spockspace(tune_dict) @staticmethod def _config_to_dict(tuner_config: Union[OptunaTunerConfig, AxTunerConfig]): \"\"\"Turns an attrs config object into a dictionary *Args*: tuner_config: attrs config object *Returns*: dictionary of the attrs config object \"\"\" return {k: v for k, v in attr.asdict(tuner_config).items() if v is not None} @staticmethod def _to_spockspace(tune_dict: Dict): \"\"\"Converts a dict to a Spockspace *Args*: tune_dict: current dictionary *Returns*: Spockspace of dict \"\"\" return Spockspace(**tune_dict) @staticmethod def _get_caster(val): \"\"\"Gets a callable type object from a string type *Args*: val: current attr val: *Returns*: type class object \"\"\" return __builtins__[val.type] def _try_choice_cast(self, val, type_string: str): \"\"\"Try/except for casting choice parameters *Args*: val: current attr val type_string: spock hyper-parameter type name *Returns*: val: updated attr val \"\"\" caster = self._get_caster(val) # Just attempt to cast in a try except try: val.choices = [caster(v) for v in val.choices] return val except TypeError: print( f\"Attempted to cast into type: {val.type} but failed -- check the inputs to {type_string}\" ) def _try_range_cast(self, val, type_string: str): \"\"\"Try/except for casting range parameters *Args*: val: current attr val type_string: spock hyper-parameter type name *Returns*: low: low bound high: high bound \"\"\" caster = self._get_caster(val) try: low = caster(val.bounds[0]) high = caster(val.bounds[1]) return low, high except TypeError: print( f\"Attempted to cast into type: {val.type} but failed -- check the inputs to {type_string}\" )","title":"BaseInterface"},{"location":"reference/spock/addons/tune/interface/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/addons/tune/interface/#descendants","text":"spock.addons.tune.ax.AxInterface spock.addons.tune.optuna.OptunaInterface","title":"Descendants"},{"location":"reference/spock/addons/tune/interface/#instance-variables","text":"best Returns a Spockspace of the best hyper-parameter config and the associated metric value tuner_status Returns a dictionary of all the necessary underlying tuner internals to report the result","title":"Instance variables"},{"location":"reference/spock/addons/tune/interface/#methods","text":"","title":"Methods"},{"location":"reference/spock/addons/tune/interface/#sample","text":"def sample ( self ) Calls the underlying library sample to get a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) Returns : Spockspace of the current hyper-parameter draw ??? example \"View Source\" @abstractmethod def sample(self): \"\"\"Calls the underlying library sample to get a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) *Returns*: Spockspace of the current hyper-parameter draw \"\"\" pass","title":"sample"},{"location":"reference/spock/addons/tune/optuna/","text":"Module spock.addons.tune.optuna Handles the optuna backend None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the optuna backend\"\"\" import optuna from spock.addons.tune.config import OptunaTunerConfig from spock.addons.tune.interface import BaseInterface try: from typing import TypedDict except ImportError: from mypy_extensions import TypedDict class OptunaTunerStatus(TypedDict): \"\"\"Tuner status return object for Optuna -- supports the define-and-run style interface from Optuna *Attributes*: trial: current ask trial sample study: current optuna study object \"\"\" trial: optuna.Trial study: optuna.Study class OptunaInterface(BaseInterface): \"\"\"Specific override to support the optuna backend -- supports the define-and-run style interface from Optuna *Attributes*: _map_type: dictionary that maps class names and types to fns that create optuna distributions _trial: current trial object from the optuna backend _tuner_obj: underlying optuna study object _tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types _param_obj: underlying object that optuna study can sample from (flat dictionary) _sample_hash: hash of the most recent sample draw \"\"\" def __init__(self, tuner_config: OptunaTunerConfig, tuner_namespace): \"\"\"OptunaInterface init call that maps variables, creates a map to fnc calls, and constructs the necessary underlying objects *Args*: tuner_config: configuration object for the optuna backend tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" super(OptunaInterface, self).__init__(tuner_config, tuner_namespace) self._tuner_obj = optuna.create_study( **self._config_to_dict(self._tuner_config) ) # Some variables to use later self._trial = None self._sample_hash = None # Mapping spock underlying classes to optuna distributions (define-and-run interface) self._map_type = { \"RangeHyperParameter\": { \"int\": self._uniform_int_dist, \"float\": self._uniform_float_dist, }, \"ChoiceHyperParameter\": { \"int\": self._categorical_dist, \"float\": self._categorical_dist, \"str\": self._categorical_dist, \"bool\": self._categorical_dist, }, } # Build the correct underlying dictionary object for Optuna self._param_obj = self._construct() @property def tuner_status(self) -> OptunaTunerStatus: return OptunaTunerStatus(trial=self._trial, study=self._tuner_obj) @property def best(self): rollup_dict, _ = self._sample_rollup(self._tuner_obj.best_trial.params) return ( self._gen_spockspace(rollup_dict), self._tuner_obj.best_value, ) @property def _get_sample(self): return self._tuner_obj.ask(self._param_obj) def sample(self): self._trial = self._get_sample # Roll this back out into a Spockspace so it can be merged into the fixed parameter Spockspace # Also need to un-dot the param names to rebuild the nested structure rollup_dict, sample_hash = self._sample_rollup(self._trial.params) self._sample_hash = sample_hash return self._gen_spockspace(rollup_dict) def _construct(self): optuna_dict = {} # These will only be nested one level deep given the tuner syntax for k, v in vars(self._tuner_namespace).items(): for ik, iv in vars(v).items(): param_fn = self._map_type[type(iv).__name__][iv.type] optuna_dict.update({f\"{k}.{ik}\": param_fn(iv)}) return optuna_dict def _uniform_float_dist(self, val): \"\"\"Assemble the optuna.distributions.(Log)UniformDistribution object https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.UniformDistribution.html https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.LogUniformDistribution.html *Args*: val: current attr val *Returns*: optuna.distributions.UniformDistribution or optuna.distributions.LogUniformDistribution \"\"\" low, high = self._try_range_cast(val, type_string=\"RangeHyperParameter\") log_scale = val.log_scale return ( optuna.distributions.LogUniformDistribution(low=low, high=high) if log_scale else optuna.distributions.UniformDistribution(low=low, high=high) ) def _uniform_int_dist(self, val): \"\"\"Assemble the optuna.distributions.Int(Log)UniformDistribution object https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.IntUniformDistribution.html https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.IntLogUniformDistribution.html *Args*: val: current attr val *Returns*: optuna.distributions.IntUniformDistribution or optuna.distributions.IntLogUniformDistribution \"\"\" low, high = self._try_range_cast(val, type_string=\"RangeHyperParameter\") log_scale = val.log_scale return ( optuna.distributions.IntLogUniformDistribution(low=low, high=high) if log_scale else optuna.distributions.IntUniformDistribution(low=low, high=high) ) def _categorical_dist(self, val): \"\"\"Assemble the optuna.distributions.CategoricalDistribution object https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.CategoricalDistribution.html *Args*: val: current attr val *Returns*: optuna.distributions.CategoricalDistribution \"\"\" val = self._try_choice_cast(val, type_string=\"ChoiceHyperParameter\") return optuna.distributions.CategoricalDistribution(choices=val.choices) Classes OptunaInterface class OptunaInterface ( tuner_config : spock . addons . tune . config . OptunaTunerConfig , tuner_namespace ) ??? example \"View Source\" class OptunaInterface(BaseInterface): \"\"\"Specific override to support the optuna backend -- supports the define-and-run style interface from Optuna *Attributes*: _map_type: dictionary that maps class names and types to fns that create optuna distributions _trial: current trial object from the optuna backend _tuner_obj: underlying optuna study object _tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types _param_obj: underlying object that optuna study can sample from (flat dictionary) _sample_hash: hash of the most recent sample draw \"\"\" def __init__(self, tuner_config: OptunaTunerConfig, tuner_namespace): \"\"\"OptunaInterface init call that maps variables, creates a map to fnc calls, and constructs the necessary underlying objects *Args*: tuner_config: configuration object for the optuna backend tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" super(OptunaInterface, self).__init__(tuner_config, tuner_namespace) self._tuner_obj = optuna.create_study( **self._config_to_dict(self._tuner_config) ) # Some variables to use later self._trial = None self._sample_hash = None # Mapping spock underlying classes to optuna distributions (define-and-run interface) self._map_type = { \"RangeHyperParameter\": { \"int\": self._uniform_int_dist, \"float\": self._uniform_float_dist, }, \"ChoiceHyperParameter\": { \"int\": self._categorical_dist, \"float\": self._categorical_dist, \"str\": self._categorical_dist, \"bool\": self._categorical_dist, }, } # Build the correct underlying dictionary object for Optuna self._param_obj = self._construct() @property def tuner_status(self) -> OptunaTunerStatus: return OptunaTunerStatus(trial=self._trial, study=self._tuner_obj) @property def best(self): rollup_dict, _ = self._sample_rollup(self._tuner_obj.best_trial.params) return ( self._gen_spockspace(rollup_dict), self._tuner_obj.best_value, ) @property def _get_sample(self): return self._tuner_obj.ask(self._param_obj) def sample(self): self._trial = self._get_sample # Roll this back out into a Spockspace so it can be merged into the fixed parameter Spockspace # Also need to un-dot the param names to rebuild the nested structure rollup_dict, sample_hash = self._sample_rollup(self._trial.params) self._sample_hash = sample_hash return self._gen_spockspace(rollup_dict) def _construct(self): optuna_dict = {} # These will only be nested one level deep given the tuner syntax for k, v in vars(self._tuner_namespace).items(): for ik, iv in vars(v).items(): param_fn = self._map_type[type(iv).__name__][iv.type] optuna_dict.update({f\"{k}.{ik}\": param_fn(iv)}) return optuna_dict def _uniform_float_dist(self, val): \"\"\"Assemble the optuna.distributions.(Log)UniformDistribution object https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.UniformDistribution.html https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.LogUniformDistribution.html *Args*: val: current attr val *Returns*: optuna.distributions.UniformDistribution or optuna.distributions.LogUniformDistribution \"\"\" low, high = self._try_range_cast(val, type_string=\"RangeHyperParameter\") log_scale = val.log_scale return ( optuna.distributions.LogUniformDistribution(low=low, high=high) if log_scale else optuna.distributions.UniformDistribution(low=low, high=high) ) def _uniform_int_dist(self, val): \"\"\"Assemble the optuna.distributions.Int(Log)UniformDistribution object https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.IntUniformDistribution.html https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.IntLogUniformDistribution.html *Args*: val: current attr val *Returns*: optuna.distributions.IntUniformDistribution or optuna.distributions.IntLogUniformDistribution \"\"\" low, high = self._try_range_cast(val, type_string=\"RangeHyperParameter\") log_scale = val.log_scale return ( optuna.distributions.IntLogUniformDistribution(low=low, high=high) if log_scale else optuna.distributions.IntUniformDistribution(low=low, high=high) ) def _categorical_dist(self, val): \"\"\"Assemble the optuna.distributions.CategoricalDistribution object https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.CategoricalDistribution.html *Args*: val: current attr val *Returns*: optuna.distributions.CategoricalDistribution \"\"\" val = self._try_choice_cast(val, type_string=\"ChoiceHyperParameter\") return optuna.distributions.CategoricalDistribution(choices=val.choices) Ancestors (in MRO) spock.addons.tune.interface.BaseInterface abc.ABC Instance variables best tuner_status Methods sample def sample ( self ) Calls the underlying library sample to get a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) Returns : Spockspace of the current hyper-parameter draw ??? example \"View Source\" def sample(self): self._trial = self._get_sample # Roll this back out into a Spockspace so it can be merged into the fixed parameter Spockspace # Also need to un-dot the param names to rebuild the nested structure rollup_dict, sample_hash = self._sample_rollup(self._trial.params) self._sample_hash = sample_hash return self._gen_spockspace(rollup_dict) OptunaTunerStatus class OptunaTunerStatus ( / , * args , ** kwargs ) ??? example \"View Source\" class OptunaTunerStatus(TypedDict): \"\"\"Tuner status return object for Optuna -- supports the define-and-run style interface from Optuna *Attributes*: trial: current ask trial sample study: current optuna study object \"\"\" trial: optuna.Trial study: optuna.Study Ancestors (in MRO) builtins.dict Methods clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] values def values ( ... ) D.values() -> an object providing a view on D's values","title":"Optuna"},{"location":"reference/spock/addons/tune/optuna/#module-spockaddonstuneoptuna","text":"Handles the optuna backend None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the optuna backend\"\"\" import optuna from spock.addons.tune.config import OptunaTunerConfig from spock.addons.tune.interface import BaseInterface try: from typing import TypedDict except ImportError: from mypy_extensions import TypedDict class OptunaTunerStatus(TypedDict): \"\"\"Tuner status return object for Optuna -- supports the define-and-run style interface from Optuna *Attributes*: trial: current ask trial sample study: current optuna study object \"\"\" trial: optuna.Trial study: optuna.Study class OptunaInterface(BaseInterface): \"\"\"Specific override to support the optuna backend -- supports the define-and-run style interface from Optuna *Attributes*: _map_type: dictionary that maps class names and types to fns that create optuna distributions _trial: current trial object from the optuna backend _tuner_obj: underlying optuna study object _tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types _param_obj: underlying object that optuna study can sample from (flat dictionary) _sample_hash: hash of the most recent sample draw \"\"\" def __init__(self, tuner_config: OptunaTunerConfig, tuner_namespace): \"\"\"OptunaInterface init call that maps variables, creates a map to fnc calls, and constructs the necessary underlying objects *Args*: tuner_config: configuration object for the optuna backend tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" super(OptunaInterface, self).__init__(tuner_config, tuner_namespace) self._tuner_obj = optuna.create_study( **self._config_to_dict(self._tuner_config) ) # Some variables to use later self._trial = None self._sample_hash = None # Mapping spock underlying classes to optuna distributions (define-and-run interface) self._map_type = { \"RangeHyperParameter\": { \"int\": self._uniform_int_dist, \"float\": self._uniform_float_dist, }, \"ChoiceHyperParameter\": { \"int\": self._categorical_dist, \"float\": self._categorical_dist, \"str\": self._categorical_dist, \"bool\": self._categorical_dist, }, } # Build the correct underlying dictionary object for Optuna self._param_obj = self._construct() @property def tuner_status(self) -> OptunaTunerStatus: return OptunaTunerStatus(trial=self._trial, study=self._tuner_obj) @property def best(self): rollup_dict, _ = self._sample_rollup(self._tuner_obj.best_trial.params) return ( self._gen_spockspace(rollup_dict), self._tuner_obj.best_value, ) @property def _get_sample(self): return self._tuner_obj.ask(self._param_obj) def sample(self): self._trial = self._get_sample # Roll this back out into a Spockspace so it can be merged into the fixed parameter Spockspace # Also need to un-dot the param names to rebuild the nested structure rollup_dict, sample_hash = self._sample_rollup(self._trial.params) self._sample_hash = sample_hash return self._gen_spockspace(rollup_dict) def _construct(self): optuna_dict = {} # These will only be nested one level deep given the tuner syntax for k, v in vars(self._tuner_namespace).items(): for ik, iv in vars(v).items(): param_fn = self._map_type[type(iv).__name__][iv.type] optuna_dict.update({f\"{k}.{ik}\": param_fn(iv)}) return optuna_dict def _uniform_float_dist(self, val): \"\"\"Assemble the optuna.distributions.(Log)UniformDistribution object https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.UniformDistribution.html https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.LogUniformDistribution.html *Args*: val: current attr val *Returns*: optuna.distributions.UniformDistribution or optuna.distributions.LogUniformDistribution \"\"\" low, high = self._try_range_cast(val, type_string=\"RangeHyperParameter\") log_scale = val.log_scale return ( optuna.distributions.LogUniformDistribution(low=low, high=high) if log_scale else optuna.distributions.UniformDistribution(low=low, high=high) ) def _uniform_int_dist(self, val): \"\"\"Assemble the optuna.distributions.Int(Log)UniformDistribution object https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.IntUniformDistribution.html https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.IntLogUniformDistribution.html *Args*: val: current attr val *Returns*: optuna.distributions.IntUniformDistribution or optuna.distributions.IntLogUniformDistribution \"\"\" low, high = self._try_range_cast(val, type_string=\"RangeHyperParameter\") log_scale = val.log_scale return ( optuna.distributions.IntLogUniformDistribution(low=low, high=high) if log_scale else optuna.distributions.IntUniformDistribution(low=low, high=high) ) def _categorical_dist(self, val): \"\"\"Assemble the optuna.distributions.CategoricalDistribution object https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.CategoricalDistribution.html *Args*: val: current attr val *Returns*: optuna.distributions.CategoricalDistribution \"\"\" val = self._try_choice_cast(val, type_string=\"ChoiceHyperParameter\") return optuna.distributions.CategoricalDistribution(choices=val.choices)","title":"Module spock.addons.tune.optuna"},{"location":"reference/spock/addons/tune/optuna/#classes","text":"","title":"Classes"},{"location":"reference/spock/addons/tune/optuna/#optunainterface","text":"class OptunaInterface ( tuner_config : spock . addons . tune . config . OptunaTunerConfig , tuner_namespace ) ??? example \"View Source\" class OptunaInterface(BaseInterface): \"\"\"Specific override to support the optuna backend -- supports the define-and-run style interface from Optuna *Attributes*: _map_type: dictionary that maps class names and types to fns that create optuna distributions _trial: current trial object from the optuna backend _tuner_obj: underlying optuna study object _tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types _param_obj: underlying object that optuna study can sample from (flat dictionary) _sample_hash: hash of the most recent sample draw \"\"\" def __init__(self, tuner_config: OptunaTunerConfig, tuner_namespace): \"\"\"OptunaInterface init call that maps variables, creates a map to fnc calls, and constructs the necessary underlying objects *Args*: tuner_config: configuration object for the optuna backend tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types \"\"\" super(OptunaInterface, self).__init__(tuner_config, tuner_namespace) self._tuner_obj = optuna.create_study( **self._config_to_dict(self._tuner_config) ) # Some variables to use later self._trial = None self._sample_hash = None # Mapping spock underlying classes to optuna distributions (define-and-run interface) self._map_type = { \"RangeHyperParameter\": { \"int\": self._uniform_int_dist, \"float\": self._uniform_float_dist, }, \"ChoiceHyperParameter\": { \"int\": self._categorical_dist, \"float\": self._categorical_dist, \"str\": self._categorical_dist, \"bool\": self._categorical_dist, }, } # Build the correct underlying dictionary object for Optuna self._param_obj = self._construct() @property def tuner_status(self) -> OptunaTunerStatus: return OptunaTunerStatus(trial=self._trial, study=self._tuner_obj) @property def best(self): rollup_dict, _ = self._sample_rollup(self._tuner_obj.best_trial.params) return ( self._gen_spockspace(rollup_dict), self._tuner_obj.best_value, ) @property def _get_sample(self): return self._tuner_obj.ask(self._param_obj) def sample(self): self._trial = self._get_sample # Roll this back out into a Spockspace so it can be merged into the fixed parameter Spockspace # Also need to un-dot the param names to rebuild the nested structure rollup_dict, sample_hash = self._sample_rollup(self._trial.params) self._sample_hash = sample_hash return self._gen_spockspace(rollup_dict) def _construct(self): optuna_dict = {} # These will only be nested one level deep given the tuner syntax for k, v in vars(self._tuner_namespace).items(): for ik, iv in vars(v).items(): param_fn = self._map_type[type(iv).__name__][iv.type] optuna_dict.update({f\"{k}.{ik}\": param_fn(iv)}) return optuna_dict def _uniform_float_dist(self, val): \"\"\"Assemble the optuna.distributions.(Log)UniformDistribution object https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.UniformDistribution.html https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.LogUniformDistribution.html *Args*: val: current attr val *Returns*: optuna.distributions.UniformDistribution or optuna.distributions.LogUniformDistribution \"\"\" low, high = self._try_range_cast(val, type_string=\"RangeHyperParameter\") log_scale = val.log_scale return ( optuna.distributions.LogUniformDistribution(low=low, high=high) if log_scale else optuna.distributions.UniformDistribution(low=low, high=high) ) def _uniform_int_dist(self, val): \"\"\"Assemble the optuna.distributions.Int(Log)UniformDistribution object https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.IntUniformDistribution.html https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.IntLogUniformDistribution.html *Args*: val: current attr val *Returns*: optuna.distributions.IntUniformDistribution or optuna.distributions.IntLogUniformDistribution \"\"\" low, high = self._try_range_cast(val, type_string=\"RangeHyperParameter\") log_scale = val.log_scale return ( optuna.distributions.IntLogUniformDistribution(low=low, high=high) if log_scale else optuna.distributions.IntUniformDistribution(low=low, high=high) ) def _categorical_dist(self, val): \"\"\"Assemble the optuna.distributions.CategoricalDistribution object https://optuna.readthedocs.io/en/stable/reference/generated/optuna.distributions.CategoricalDistribution.html *Args*: val: current attr val *Returns*: optuna.distributions.CategoricalDistribution \"\"\" val = self._try_choice_cast(val, type_string=\"ChoiceHyperParameter\") return optuna.distributions.CategoricalDistribution(choices=val.choices)","title":"OptunaInterface"},{"location":"reference/spock/addons/tune/optuna/#ancestors-in-mro","text":"spock.addons.tune.interface.BaseInterface abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/addons/tune/optuna/#instance-variables","text":"best tuner_status","title":"Instance variables"},{"location":"reference/spock/addons/tune/optuna/#methods","text":"","title":"Methods"},{"location":"reference/spock/addons/tune/optuna/#sample","text":"def sample ( self ) Calls the underlying library sample to get a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) Returns : Spockspace of the current hyper-parameter draw ??? example \"View Source\" def sample(self): self._trial = self._get_sample # Roll this back out into a Spockspace so it can be merged into the fixed parameter Spockspace # Also need to un-dot the param names to rebuild the nested structure rollup_dict, sample_hash = self._sample_rollup(self._trial.params) self._sample_hash = sample_hash return self._gen_spockspace(rollup_dict)","title":"sample"},{"location":"reference/spock/addons/tune/optuna/#optunatunerstatus","text":"class OptunaTunerStatus ( / , * args , ** kwargs ) ??? example \"View Source\" class OptunaTunerStatus(TypedDict): \"\"\"Tuner status return object for Optuna -- supports the define-and-run style interface from Optuna *Attributes*: trial: current ask trial sample study: current optuna study object \"\"\" trial: optuna.Trial study: optuna.Study","title":"OptunaTunerStatus"},{"location":"reference/spock/addons/tune/optuna/#ancestors-in-mro_1","text":"builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/spock/addons/tune/optuna/#methods_1","text":"","title":"Methods"},{"location":"reference/spock/addons/tune/optuna/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/spock/addons/tune/optuna/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/spock/addons/tune/optuna/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/spock/addons/tune/optuna/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/spock/addons/tune/optuna/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/spock/addons/tune/optuna/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/spock/addons/tune/optuna/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/spock/addons/tune/optuna/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/spock/addons/tune/optuna/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/spock/addons/tune/optuna/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/spock/addons/tune/optuna/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/spock/addons/tune/payload/","text":"Module spock.addons.tune.payload Handles the tuner payload backend None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the tuner payload backend\"\"\" from spock.backend.payload import BasePayload from spock.backend.utils import get_attr_fields class TunerPayload(BasePayload): \"\"\"Handles building the payload for tuners This class builds out the payload from config files of multiple types. It handles various file types and also composition of config files via a recursive calls *Attributes*: _loaders: maps of each file extension to the loader class \"\"\" def __init__(self, s3_config=None): \"\"\"Init for TunerPayload *Args*: s3_config: optional S3 config object \"\"\" super().__init__(s3_config=s3_config) def __call__(self, *args, **kwargs): \"\"\"Call to allow self chaining *Args*: *args: **kwargs: *Returns*: Payload: instance of self \"\"\" return TunerPayload(*args, **kwargs) @staticmethod def _update_payload(base_payload, input_classes, ignore_classes, payload): # Get basic args attr_fields = get_attr_fields(input_classes=input_classes) # Get the ignore fields ignore_fields = get_attr_fields(input_classes=ignore_classes) for k, v in base_payload.items(): if k not in ignore_fields: if k != \"config\": # Dict infers that we are overriding a global setting in a specific config if isinstance(v, dict): # we're in a namespace # Check for incorrect specific override of global def if k not in attr_fields: raise TypeError( f\"Referring to a class space {k} that is undefined\" ) for i_keys in v.keys(): if i_keys not in attr_fields[k]: raise ValueError( f\"Provided an unknown argument named {k}.{i_keys}\" ) if k in payload and isinstance(v, dict): payload[k].update(v) else: payload[k] = v # Handle tuple conversion here -- lazily for ik, iv in v.items(): if \"bounds\" in iv: iv[\"bounds\"] = tuple(iv[\"bounds\"]) return payload @staticmethod def _handle_payload_override(payload, key, value): key_split = key.split(\".\") curr_ref = payload for idx, split in enumerate(key_split): # If the root isn't in the payload then it needs to be added but only for the first key split if idx == 0 and (split not in payload): payload.update({split: {}}) # Check if it's the last value and figure out the override if idx == (len(key_split) - 1): # Handle bool(s) a bit differently as they are store_true if isinstance(curr_ref, dict) and isinstance(value, bool): if value is not False: curr_ref[split] = value # If we are at the dictionary level we should be able to just payload override elif isinstance(curr_ref, dict) and not isinstance(value, bool): curr_ref[split] = value else: raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Failed to find key {split} within lowest level Dict\" ) # If it's not keep walking the current payload else: curr_ref = curr_ref[split] return payload Classes TunerPayload class TunerPayload ( s3_config = None ) ??? example \"View Source\" class TunerPayload(BasePayload): \"\"\"Handles building the payload for tuners This class builds out the payload from config files of multiple types. It handles various file types and also composition of config files via a recursive calls *Attributes*: _loaders: maps of each file extension to the loader class \"\"\" def __init__(self, s3_config=None): \"\"\"Init for TunerPayload *Args*: s3_config: optional S3 config object \"\"\" super().__init__(s3_config=s3_config) def __call__(self, *args, **kwargs): \"\"\"Call to allow self chaining *Args*: *args: **kwargs: *Returns*: Payload: instance of self \"\"\" return TunerPayload(*args, **kwargs) @staticmethod def _update_payload(base_payload, input_classes, ignore_classes, payload): # Get basic args attr_fields = get_attr_fields(input_classes=input_classes) # Get the ignore fields ignore_fields = get_attr_fields(input_classes=ignore_classes) for k, v in base_payload.items(): if k not in ignore_fields: if k != \"config\": # Dict infers that we are overriding a global setting in a specific config if isinstance(v, dict): # we're in a namespace # Check for incorrect specific override of global def if k not in attr_fields: raise TypeError( f\"Referring to a class space {k} that is undefined\" ) for i_keys in v.keys(): if i_keys not in attr_fields[k]: raise ValueError( f\"Provided an unknown argument named {k}.{i_keys}\" ) if k in payload and isinstance(v, dict): payload[k].update(v) else: payload[k] = v # Handle tuple conversion here -- lazily for ik, iv in v.items(): if \"bounds\" in iv: iv[\"bounds\"] = tuple(iv[\"bounds\"]) return payload @staticmethod def _handle_payload_override(payload, key, value): key_split = key.split(\".\") curr_ref = payload for idx, split in enumerate(key_split): # If the root isn't in the payload then it needs to be added but only for the first key split if idx == 0 and (split not in payload): payload.update({split: {}}) # Check if it's the last value and figure out the override if idx == (len(key_split) - 1): # Handle bool(s) a bit differently as they are store_true if isinstance(curr_ref, dict) and isinstance(value, bool): if value is not False: curr_ref[split] = value # If we are at the dictionary level we should be able to just payload override elif isinstance(curr_ref, dict) and not isinstance(value, bool): curr_ref[split] = value else: raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Failed to find key {split} within lowest level Dict\" ) # If it's not keep walking the current payload else: curr_ref = curr_ref[split] return payload Ancestors (in MRO) spock.backend.payload.BasePayload spock.backend.handler.BaseHandler abc.ABC Methods payload def payload ( self , input_classes , ignore_classes , path , cmd_args , deps ) Builds the payload from config files Public exposed call to build the payload and set any command line overrides Args : input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies Returns : payload: dictionary of all mapped parameters ??? example \"View Source\" def payload(self, input_classes, ignore_classes, path, cmd_args, deps): \"\"\"Builds the payload from config files Public exposed call to build the payload and set any command line overrides *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" payload = self._payload(input_classes, ignore_classes, path, deps, root=True) payload = self._handle_overrides(payload, ignore_classes, cmd_args) return payload","title":"Payload"},{"location":"reference/spock/addons/tune/payload/#module-spockaddonstunepayload","text":"Handles the tuner payload backend None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the tuner payload backend\"\"\" from spock.backend.payload import BasePayload from spock.backend.utils import get_attr_fields class TunerPayload(BasePayload): \"\"\"Handles building the payload for tuners This class builds out the payload from config files of multiple types. It handles various file types and also composition of config files via a recursive calls *Attributes*: _loaders: maps of each file extension to the loader class \"\"\" def __init__(self, s3_config=None): \"\"\"Init for TunerPayload *Args*: s3_config: optional S3 config object \"\"\" super().__init__(s3_config=s3_config) def __call__(self, *args, **kwargs): \"\"\"Call to allow self chaining *Args*: *args: **kwargs: *Returns*: Payload: instance of self \"\"\" return TunerPayload(*args, **kwargs) @staticmethod def _update_payload(base_payload, input_classes, ignore_classes, payload): # Get basic args attr_fields = get_attr_fields(input_classes=input_classes) # Get the ignore fields ignore_fields = get_attr_fields(input_classes=ignore_classes) for k, v in base_payload.items(): if k not in ignore_fields: if k != \"config\": # Dict infers that we are overriding a global setting in a specific config if isinstance(v, dict): # we're in a namespace # Check for incorrect specific override of global def if k not in attr_fields: raise TypeError( f\"Referring to a class space {k} that is undefined\" ) for i_keys in v.keys(): if i_keys not in attr_fields[k]: raise ValueError( f\"Provided an unknown argument named {k}.{i_keys}\" ) if k in payload and isinstance(v, dict): payload[k].update(v) else: payload[k] = v # Handle tuple conversion here -- lazily for ik, iv in v.items(): if \"bounds\" in iv: iv[\"bounds\"] = tuple(iv[\"bounds\"]) return payload @staticmethod def _handle_payload_override(payload, key, value): key_split = key.split(\".\") curr_ref = payload for idx, split in enumerate(key_split): # If the root isn't in the payload then it needs to be added but only for the first key split if idx == 0 and (split not in payload): payload.update({split: {}}) # Check if it's the last value and figure out the override if idx == (len(key_split) - 1): # Handle bool(s) a bit differently as they are store_true if isinstance(curr_ref, dict) and isinstance(value, bool): if value is not False: curr_ref[split] = value # If we are at the dictionary level we should be able to just payload override elif isinstance(curr_ref, dict) and not isinstance(value, bool): curr_ref[split] = value else: raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Failed to find key {split} within lowest level Dict\" ) # If it's not keep walking the current payload else: curr_ref = curr_ref[split] return payload","title":"Module spock.addons.tune.payload"},{"location":"reference/spock/addons/tune/payload/#classes","text":"","title":"Classes"},{"location":"reference/spock/addons/tune/payload/#tunerpayload","text":"class TunerPayload ( s3_config = None ) ??? example \"View Source\" class TunerPayload(BasePayload): \"\"\"Handles building the payload for tuners This class builds out the payload from config files of multiple types. It handles various file types and also composition of config files via a recursive calls *Attributes*: _loaders: maps of each file extension to the loader class \"\"\" def __init__(self, s3_config=None): \"\"\"Init for TunerPayload *Args*: s3_config: optional S3 config object \"\"\" super().__init__(s3_config=s3_config) def __call__(self, *args, **kwargs): \"\"\"Call to allow self chaining *Args*: *args: **kwargs: *Returns*: Payload: instance of self \"\"\" return TunerPayload(*args, **kwargs) @staticmethod def _update_payload(base_payload, input_classes, ignore_classes, payload): # Get basic args attr_fields = get_attr_fields(input_classes=input_classes) # Get the ignore fields ignore_fields = get_attr_fields(input_classes=ignore_classes) for k, v in base_payload.items(): if k not in ignore_fields: if k != \"config\": # Dict infers that we are overriding a global setting in a specific config if isinstance(v, dict): # we're in a namespace # Check for incorrect specific override of global def if k not in attr_fields: raise TypeError( f\"Referring to a class space {k} that is undefined\" ) for i_keys in v.keys(): if i_keys not in attr_fields[k]: raise ValueError( f\"Provided an unknown argument named {k}.{i_keys}\" ) if k in payload and isinstance(v, dict): payload[k].update(v) else: payload[k] = v # Handle tuple conversion here -- lazily for ik, iv in v.items(): if \"bounds\" in iv: iv[\"bounds\"] = tuple(iv[\"bounds\"]) return payload @staticmethod def _handle_payload_override(payload, key, value): key_split = key.split(\".\") curr_ref = payload for idx, split in enumerate(key_split): # If the root isn't in the payload then it needs to be added but only for the first key split if idx == 0 and (split not in payload): payload.update({split: {}}) # Check if it's the last value and figure out the override if idx == (len(key_split) - 1): # Handle bool(s) a bit differently as they are store_true if isinstance(curr_ref, dict) and isinstance(value, bool): if value is not False: curr_ref[split] = value # If we are at the dictionary level we should be able to just payload override elif isinstance(curr_ref, dict) and not isinstance(value, bool): curr_ref[split] = value else: raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Failed to find key {split} within lowest level Dict\" ) # If it's not keep walking the current payload else: curr_ref = curr_ref[split] return payload","title":"TunerPayload"},{"location":"reference/spock/addons/tune/payload/#ancestors-in-mro","text":"spock.backend.payload.BasePayload spock.backend.handler.BaseHandler abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/addons/tune/payload/#methods","text":"","title":"Methods"},{"location":"reference/spock/addons/tune/payload/#payload","text":"def payload ( self , input_classes , ignore_classes , path , cmd_args , deps ) Builds the payload from config files Public exposed call to build the payload and set any command line overrides Args : input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies Returns : payload: dictionary of all mapped parameters ??? example \"View Source\" def payload(self, input_classes, ignore_classes, path, cmd_args, deps): \"\"\"Builds the payload from config files Public exposed call to build the payload and set any command line overrides *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" payload = self._payload(input_classes, ignore_classes, path, deps, root=True) payload = self._handle_overrides(payload, ignore_classes, cmd_args) return payload","title":"payload"},{"location":"reference/spock/addons/tune/tuner/","text":"Module spock.addons.tune.tuner Handles the tuner interface interface None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the tuner interface interface\"\"\" from typing import Union from spock.addons.tune.ax import AxInterface from spock.addons.tune.config import AxTunerConfig, OptunaTunerConfig from spock.addons.tune.optuna import OptunaInterface from spock.backend.wrappers import Spockspace class TunerInterface: \"\"\"Handles the general tuner interface by creating the necessary underlying tuner class and dispatches necessary ops to the class instance *Attributes*: _fixed_namespace: fixed parameter namespace used for combination with a sample draw _lib_interface: class instance of the underlying hyper-parameter library \"\"\" def __init__( self, tuner_config: Union[OptunaTunerConfig, AxTunerConfig], tuner_namespace: Spockspace, fixed_namespace: Spockspace, ): \"\"\"Init call to the TunerInterface *Args*: tuner_config: necessary object to determine the interface and sample correctly from the underlying library tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types fixed_namespace: namespace of fixed parameters \"\"\" self._fixed_namespace = fixed_namespace accept_types = (OptunaTunerConfig, AxTunerConfig) if isinstance(tuner_config, OptunaTunerConfig): self._lib_interface = OptunaInterface( tuner_config=tuner_config, tuner_namespace=tuner_namespace ) elif isinstance(tuner_config, AxTunerConfig): self._lib_interface = AxInterface( tuner_config=tuner_config, tuner_namespace=tuner_namespace ) else: raise TypeError( f\"Passed incorrect tuner_config type of {type(tuner_config)} -- must be of type \" f\"{repr(accept_types)}\" ) def sample(self): \"\"\"Public interface to underlying library sepcific sample that returns a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) and combines them with the fixed parameters into a single Spockspace *Returns*: Spockspace of drawn sample of hyper-parameters and fixed parameters \"\"\" curr_sample = self._lib_interface.sample() # Merge w/ fixed parameters return Spockspace(**vars(curr_sample), **vars(self._fixed_namespace)) @property def tuner_status(self): \"\"\"Returns a dictionary of all the necessary underlying tuner internals to report the result\"\"\" return self._lib_interface.tuner_status @property def best(self): \"\"\"Returns a Spockspace of the best hyper-parameter config and the associated metric value\"\"\" return self._lib_interface.best Classes TunerInterface class TunerInterface ( tuner_config : Union [ spock . addons . tune . config . OptunaTunerConfig , spock . addons . tune . config . AxTunerConfig ], tuner_namespace : spock . backend . wrappers . Spockspace , fixed_namespace : spock . backend . wrappers . Spockspace ) ??? example \"View Source\" class TunerInterface: \"\"\"Handles the general tuner interface by creating the necessary underlying tuner class and dispatches necessary ops to the class instance *Attributes*: _fixed_namespace: fixed parameter namespace used for combination with a sample draw _lib_interface: class instance of the underlying hyper-parameter library \"\"\" def __init__( self, tuner_config: Union[OptunaTunerConfig, AxTunerConfig], tuner_namespace: Spockspace, fixed_namespace: Spockspace, ): \"\"\"Init call to the TunerInterface *Args*: tuner_config: necessary object to determine the interface and sample correctly from the underlying library tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types fixed_namespace: namespace of fixed parameters \"\"\" self._fixed_namespace = fixed_namespace accept_types = (OptunaTunerConfig, AxTunerConfig) if isinstance(tuner_config, OptunaTunerConfig): self._lib_interface = OptunaInterface( tuner_config=tuner_config, tuner_namespace=tuner_namespace ) elif isinstance(tuner_config, AxTunerConfig): self._lib_interface = AxInterface( tuner_config=tuner_config, tuner_namespace=tuner_namespace ) else: raise TypeError( f\"Passed incorrect tuner_config type of {type(tuner_config)} -- must be of type \" f\"{repr(accept_types)}\" ) def sample(self): \"\"\"Public interface to underlying library sepcific sample that returns a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) and combines them with the fixed parameters into a single Spockspace *Returns*: Spockspace of drawn sample of hyper-parameters and fixed parameters \"\"\" curr_sample = self._lib_interface.sample() # Merge w/ fixed parameters return Spockspace(**vars(curr_sample), **vars(self._fixed_namespace)) @property def tuner_status(self): \"\"\"Returns a dictionary of all the necessary underlying tuner internals to report the result\"\"\" return self._lib_interface.tuner_status @property def best(self): \"\"\"Returns a Spockspace of the best hyper-parameter config and the associated metric value\"\"\" return self._lib_interface.best Instance variables best Returns a Spockspace of the best hyper-parameter config and the associated metric value tuner_status Returns a dictionary of all the necessary underlying tuner internals to report the result Methods sample def sample ( self ) Public interface to underlying library sepcific sample that returns a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) and combines them with the fixed parameters into a single Spockspace Returns : Spockspace of drawn sample of hyper-parameters and fixed parameters ??? example \"View Source\" def sample(self): \"\"\"Public interface to underlying library sepcific sample that returns a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) and combines them with the fixed parameters into a single Spockspace *Returns*: Spockspace of drawn sample of hyper-parameters and fixed parameters \"\"\" curr_sample = self._lib_interface.sample() # Merge w/ fixed parameters return Spockspace(**vars(curr_sample), **vars(self._fixed_namespace))","title":"Tuner"},{"location":"reference/spock/addons/tune/tuner/#module-spockaddonstunetuner","text":"Handles the tuner interface interface None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the tuner interface interface\"\"\" from typing import Union from spock.addons.tune.ax import AxInterface from spock.addons.tune.config import AxTunerConfig, OptunaTunerConfig from spock.addons.tune.optuna import OptunaInterface from spock.backend.wrappers import Spockspace class TunerInterface: \"\"\"Handles the general tuner interface by creating the necessary underlying tuner class and dispatches necessary ops to the class instance *Attributes*: _fixed_namespace: fixed parameter namespace used for combination with a sample draw _lib_interface: class instance of the underlying hyper-parameter library \"\"\" def __init__( self, tuner_config: Union[OptunaTunerConfig, AxTunerConfig], tuner_namespace: Spockspace, fixed_namespace: Spockspace, ): \"\"\"Init call to the TunerInterface *Args*: tuner_config: necessary object to determine the interface and sample correctly from the underlying library tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types fixed_namespace: namespace of fixed parameters \"\"\" self._fixed_namespace = fixed_namespace accept_types = (OptunaTunerConfig, AxTunerConfig) if isinstance(tuner_config, OptunaTunerConfig): self._lib_interface = OptunaInterface( tuner_config=tuner_config, tuner_namespace=tuner_namespace ) elif isinstance(tuner_config, AxTunerConfig): self._lib_interface = AxInterface( tuner_config=tuner_config, tuner_namespace=tuner_namespace ) else: raise TypeError( f\"Passed incorrect tuner_config type of {type(tuner_config)} -- must be of type \" f\"{repr(accept_types)}\" ) def sample(self): \"\"\"Public interface to underlying library sepcific sample that returns a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) and combines them with the fixed parameters into a single Spockspace *Returns*: Spockspace of drawn sample of hyper-parameters and fixed parameters \"\"\" curr_sample = self._lib_interface.sample() # Merge w/ fixed parameters return Spockspace(**vars(curr_sample), **vars(self._fixed_namespace)) @property def tuner_status(self): \"\"\"Returns a dictionary of all the necessary underlying tuner internals to report the result\"\"\" return self._lib_interface.tuner_status @property def best(self): \"\"\"Returns a Spockspace of the best hyper-parameter config and the associated metric value\"\"\" return self._lib_interface.best","title":"Module spock.addons.tune.tuner"},{"location":"reference/spock/addons/tune/tuner/#classes","text":"","title":"Classes"},{"location":"reference/spock/addons/tune/tuner/#tunerinterface","text":"class TunerInterface ( tuner_config : Union [ spock . addons . tune . config . OptunaTunerConfig , spock . addons . tune . config . AxTunerConfig ], tuner_namespace : spock . backend . wrappers . Spockspace , fixed_namespace : spock . backend . wrappers . Spockspace ) ??? example \"View Source\" class TunerInterface: \"\"\"Handles the general tuner interface by creating the necessary underlying tuner class and dispatches necessary ops to the class instance *Attributes*: _fixed_namespace: fixed parameter namespace used for combination with a sample draw _lib_interface: class instance of the underlying hyper-parameter library \"\"\" def __init__( self, tuner_config: Union[OptunaTunerConfig, AxTunerConfig], tuner_namespace: Spockspace, fixed_namespace: Spockspace, ): \"\"\"Init call to the TunerInterface *Args*: tuner_config: necessary object to determine the interface and sample correctly from the underlying library tuner_namespace: tuner namespace that has attr classes that maps to an underlying library types fixed_namespace: namespace of fixed parameters \"\"\" self._fixed_namespace = fixed_namespace accept_types = (OptunaTunerConfig, AxTunerConfig) if isinstance(tuner_config, OptunaTunerConfig): self._lib_interface = OptunaInterface( tuner_config=tuner_config, tuner_namespace=tuner_namespace ) elif isinstance(tuner_config, AxTunerConfig): self._lib_interface = AxInterface( tuner_config=tuner_config, tuner_namespace=tuner_namespace ) else: raise TypeError( f\"Passed incorrect tuner_config type of {type(tuner_config)} -- must be of type \" f\"{repr(accept_types)}\" ) def sample(self): \"\"\"Public interface to underlying library sepcific sample that returns a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) and combines them with the fixed parameters into a single Spockspace *Returns*: Spockspace of drawn sample of hyper-parameters and fixed parameters \"\"\" curr_sample = self._lib_interface.sample() # Merge w/ fixed parameters return Spockspace(**vars(curr_sample), **vars(self._fixed_namespace)) @property def tuner_status(self): \"\"\"Returns a dictionary of all the necessary underlying tuner internals to report the result\"\"\" return self._lib_interface.tuner_status @property def best(self): \"\"\"Returns a Spockspace of the best hyper-parameter config and the associated metric value\"\"\" return self._lib_interface.best","title":"TunerInterface"},{"location":"reference/spock/addons/tune/tuner/#instance-variables","text":"best Returns a Spockspace of the best hyper-parameter config and the associated metric value tuner_status Returns a dictionary of all the necessary underlying tuner internals to report the result","title":"Instance variables"},{"location":"reference/spock/addons/tune/tuner/#methods","text":"","title":"Methods"},{"location":"reference/spock/addons/tune/tuner/#sample","text":"def sample ( self ) Public interface to underlying library sepcific sample that returns a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) and combines them with the fixed parameters into a single Spockspace Returns : Spockspace of drawn sample of hyper-parameters and fixed parameters ??? example \"View Source\" def sample(self): \"\"\"Public interface to underlying library sepcific sample that returns a single sample/draw from the hyper-parameter sets (e.g. ranges, choices) and combines them with the fixed parameters into a single Spockspace *Returns*: Spockspace of drawn sample of hyper-parameters and fixed parameters \"\"\" curr_sample = self._lib_interface.sample() # Merge w/ fixed parameters return Spockspace(**vars(curr_sample), **vars(self._fixed_namespace))","title":"sample"},{"location":"reference/spock/backend/","text":"Module spock.backend Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\" Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md \"\"\" __all__ = [\"builder\", \"config\", \"payload\", \"saver\", \"typed\"] Sub-modules spock.backend.builder spock.backend.config spock.backend.handler spock.backend.payload spock.backend.saver spock.backend.typed spock.backend.utils spock.backend.wrappers Variables builder config payload typed","title":"Index"},{"location":"reference/spock/backend/#module-spockbackend","text":"Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\" Spock is a framework that helps manage complex parameter configurations for Python applications Please refer to the documentation provided in the README.md \"\"\" __all__ = [\"builder\", \"config\", \"payload\", \"saver\", \"typed\"]","title":"Module spock.backend"},{"location":"reference/spock/backend/#sub-modules","text":"spock.backend.builder spock.backend.config spock.backend.handler spock.backend.payload spock.backend.saver spock.backend.typed spock.backend.utils spock.backend.wrappers","title":"Sub-modules"},{"location":"reference/spock/backend/#variables","text":"builder config payload typed","title":"Variables"},{"location":"reference/spock/backend/builder/","text":"Module spock.backend.builder Handles the building/saving of the configurations from the Spock config classes None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the building/saving of the configurations from the Spock config classes\"\"\" import re import sys from abc import ABC, abstractmethod from enum import EnumMeta from typing import List import attr from attr import NOTHING from spock.backend.wrappers import Spockspace from spock.utils import make_argument class BaseBuilder(ABC): # pylint: disable=too-few-public-methods \"\"\"Base class for building the backend specific builders This class handles the interface to the backend with the generic ConfigArgBuilder so that different backends can be used to handle processing *Attributes* input_classes: list of input classes that link to a backend _configs: None or List of configs to read from _desc: description for the arg parser _no_cmd_line: flag to force no command line reads _max_indent: maximum to indent between help prints save_path: list of path(s) to save the configs to \"\"\" def __init__(self, *args, max_indent=4, module_name, **kwargs): self.input_classes = args self._module_name = module_name self._max_indent = max_indent self.save_path = None @staticmethod @abstractmethod def _make_group_override_parser(parser, class_obj, class_name): \"\"\"Makes a name specific override parser for a given class obj Takes a class object of the backend and adds a new argument group with argument names given with name Class.name so that individual parameters specific to a class can be overridden. *Args*: parser: argument parser class_obj: instance of a backend class class_name: used for module matching *Returns*: parser: argument parser with new class specific overrides \"\"\" def handle_help_info(self): \"\"\"Handles walking through classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class *Returns*: None \"\"\" self._attrs_help(self.input_classes, self._module_name) def _handle_arguments(self, args, class_obj): \"\"\"Handles all argument mapping Creates a dictionary of named parameters that are mapped to the final type of object *Args*: args: read file arguments class_obj: instance of a class obj *Returns*: fields: dictionary of mapped parameters \"\"\" attr_name = class_obj.__name__ class_names = [val.__name__ for val in self.input_classes] # Handle repeated classes if ( attr_name in class_names and attr_name in args and isinstance(args[attr_name], list) ): fields = self._handle_repeated(args[attr_name], attr_name, class_names) # Handle non-repeated classes else: fields = {} for val in class_obj.__attrs_attrs__: # Check if namespace is named and then check for key -- checking for local class def if attr_name in args and val.name in args[attr_name]: fields[val.name] = self._handle_nested_class( args, args[attr_name][val.name], class_names ) # If not named then just check for keys -- checking for global def elif val.name in args: fields[val.name] = self._handle_nested_class( args, args[val.name], class_names ) # Check for special keys to set if ( \"special_key\" in val.metadata and val.metadata[\"special_key\"] is not None ): if val.name in args: self.save_path = args[val.name] elif val.default is not None: self.save_path = val.default return fields def _handle_repeated(self, args, check_value, class_names): \"\"\"Handles repeated classes as lists *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: list of input_class[match)idx[0]] types filled with repeated values \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] return [self.input_classes[match_idx[0]](**val) for val in args] def _handle_nested_class(self, args, check_value, class_names): \"\"\"Handles passing another class to the field dictionary *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: either the check_value or the necessary class \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] # If so then create the needed class object by unrolling the args to **kwargs and return it if len(match_idx) > 0: if len(match_idx) > 1: raise ValueError( \"Match error -- multiple classes with the same name definition\" ) else: if args.get(self.input_classes[match_idx[0]].__name__) is None: raise ValueError( f\"Missing config file definition for the referenced class \" f\"{self.input_classes[match_idx[0]].__name__}\" ) current_arg = args.get(self.input_classes[match_idx[0]].__name__) if isinstance(current_arg, list): class_value = [ self.input_classes[match_idx[0]](**val) for val in current_arg ] else: class_value = self.input_classes[match_idx[0]](**current_arg) return_value = class_value # else return the expected value else: return_value = check_value return return_value def generate(self, dict_args): \"\"\"Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values *Args*: dict_args: dictionary of arguments from the configs *Returns*: namespace containing automatically generated instances of the classes \"\"\" auto_dict = {} for attr_classes in self.input_classes: attr_build = self._auto_generate(dict_args, attr_classes) if isinstance(attr_build, list): class_name = list({type(val).__name__ for val in attr_build}) if len(class_name) > 1: raise ValueError(\"Repeated class has more than one unique name\") auto_dict.update({class_name[0]: attr_build}) else: auto_dict.update({type(attr_build).__name__: attr_build}) return Spockspace(**auto_dict) def _auto_generate(self, args, input_class): \"\"\"Builds an instance of an attr class Builds an instance with the necessary field values from the argument dictionary read from the config file(s) *Args*: args: dictionary of arguments read from the config file(s) data_class: data class to build *Returns*: An instance of data_class with correct values assigned to fields \"\"\" # Handle the basic data types fields = self._handle_arguments(args, input_class) if isinstance(fields, list): return_value = fields else: self._handle_late_defaults(args, fields, input_class) return_value = input_class(**fields) return return_value def _handle_late_defaults(self, args, fields, input_class): \"\"\"Handles late defaults when the type is non-standard If the default type is not a base python type then we need to catch those defaults here and build the correct values from the input classes while maintaining the optional nature. The trick is to exclude all 'base' types as these defaults are covered by the attr default value *Args*: args: dictionary of arguments read from the config file(s) fields: current fields returned from _handle_arguments input_class: which input class being checked for late defaults *Returns*: fields: updated field dictionary with late defaults set \"\"\" names = [val.name for val in input_class.__attrs_attrs__] class_names = [val.__name__ for val in self.input_classes] field_list = list(fields.keys()) arg_list = list(args.keys()) # Exclude all the base types that are supported -- these can be set by attrs exclude_list = [ \"_Nothing\", \"NoneType\", \"bool\", \"int\", \"float\", \"str\", \"list\", \"tuple\", ] for val in names: if val not in field_list: # Gets the name of the class to default to default_type_name = type( getattr(input_class.__attrs_attrs__, val).default ).__name__ if default_type_name not in exclude_list: # Gets the default class object default_attr = getattr(input_class.__attrs_attrs__, val).default # If the default is given for a class then it's the actual class and not a type -- logic needs # to deal with both if type(default_attr).__name__ == \"type\": default_name = default_attr.__name__ else: default_name = type(default_attr).__name__ # Skip if in the exclude list else: default_name = None # if we need to fall back onto the default and ff it's in the arg_list then we have a # definition coming in from the config file if default_name is not None and default_name in arg_list: # This handles lists of class type repeats -- these cannot be nested as the logic would be too # confusing to map to if isinstance(args.get(default_name), list): default_value = [ self.input_classes[class_names.index(default_name)]( **arg_val ) for arg_val in args.get(default_name) ] # This handles basics and references to other classes -- here we need to recurse to grab any nested # defs since classes are passed as strings to the config but are defined via Enums (handled #139) else: recurse_args = self._handle_recursive_defaults( args.get(default_name), args, class_names ) default_value = self.input_classes[ class_names.index(default_name) ](**recurse_args) fields.update({val: default_value}) return fields def _handle_recursive_defaults(self, curr_arg, all_args, class_names): \"\"\"Recurses through the args from the config read to determine if it can map to a definition *Args*: curr_arg: current argument all_args: all argument dictionary class_names: list of class names *Returns*: out_dict: recursively mapped dictionary of attributes \"\"\" out_dict = {} for k, v in curr_arg.items(): # If the value is a reference to another class we need to recurse if v in class_names: # Recurse only if in the all_args dict (from the config file) if v in all_args: bubbled_dict = self._handle_recursive_defaults( all_args.get(v), all_args, class_names ) out_dict.update( {k: self.input_classes[class_names.index(v)](**bubbled_dict)} ) # Else fall back on default instantiation else: out_dict.update({k: self.input_classes[class_names.index(v)]()}) else: out_dict.update({k: v}) return out_dict def build_override_parsers(self, parser): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers *Args*: parser: argument parser *Returns*: parser: argument parser with new class specific overrides \"\"\" # Build out each class override specific parser for val in self.input_classes: parser = self._make_group_override_parser( parser=parser, class_obj=val, class_name=self._module_name ) return parser @staticmethod def _get_from_kwargs(args, configs): \"\"\"Get configs from the configs kwarg *Args*: args: argument namespace configs: config kwarg *Returns*: args: arg namespace \"\"\" if isinstance(configs, list): args.config.extend(configs) else: raise TypeError( f\"configs kwarg must be of type list -- given {type(configs)}\" ) return args @staticmethod def _find_attribute_idx(newline_split_docs): \"\"\"Finds the possible split between the header and Attribute annotations *Args*: newline_split_docs: new line split text Returns: idx: -1 if none or the idx of Attributes \"\"\" for idx, val in enumerate(newline_split_docs): re_check = re.search(r\"(?i)Attribute?s?:\", val) if re_check is not None: return idx return -1 def _split_docs(self, obj): \"\"\"Possibly splits head class doc string from attribute docstrings Attempts to find the first contiguous line within the Google style docstring to use as the class docstring. Splits the docs base on the Attributes tag if present. *Args*: obj: class object to rip info from *Returns*: class_doc: class docstring if present or blank str attr_doc: list of attribute doc strings \"\"\" if obj.__doc__ is not None: # Split by new line newline_split_docs = obj.__doc__.split(\"\\n\") # Cleanup l/t whitespace newline_split_docs = [val.strip() for val in newline_split_docs] else: newline_split_docs = [] # Find the break between the class docs and the Attribute section -- if this returns -1 then there is no # Attributes section attr_idx = self._find_attribute_idx(newline_split_docs) head_docs = ( newline_split_docs[:attr_idx] if attr_idx != -1 else newline_split_docs ) attr_docs = newline_split_docs[attr_idx:] if attr_idx != -1 else [] # Grab only the first contiguous line as everything else will probably be too verbose (e.g. the # mid-level docstring that has detailed descriptions class_doc = \"\" for idx, val in enumerate(head_docs): class_doc += f\" {val}\" if idx + 1 != len(head_docs) and head_docs[idx + 1] == \"\": break # Clean up any l/t whitespace class_doc = class_doc.strip() if len(class_doc) > 0: class_doc = f\"-- {class_doc}\" return class_doc, attr_docs @staticmethod def _match_attribute_docs( attr_name, attr_docs, attr_type_str, attr_default=NOTHING ): \"\"\"Matches class attributes with attribute docstrings via regex *Args*: attr_name: attribute name attr_docs: list of attribute docstrings attr_type_str: str representation of the attribute type attr_default: str representation of a possible default value *Returns*: dictionary of packed attribute information \"\"\" # Regex match each value a_str = None for a_doc in attr_docs: match_re = re.search(r\"(?i)^\" + attr_name + \"?:\", a_doc) # Find only the first match -- if more than one than ignore if match_re: a_str = a_doc[match_re.end() :].strip() return { attr_name: { \"type\": attr_type_str, \"desc\": a_str if a_str is not None else \"\", \"default\": \"(default: \" + repr(attr_default) + \")\" if type(attr_default).__name__ != \"_Nothing\" else \"\", \"len\": {\"name\": len(attr_name), \"type\": len(attr_type_str)}, } } def _handle_attributes_print(self, info_dict): \"\"\"Prints attribute information in an argparser style format *Args*: info_dict: packed attribute info dictionary to print \"\"\" # Figure out indents max_param_length = max([len(k) for k in info_dict.keys()]) max_type_length = max([v[\"len\"][\"type\"] for v in info_dict.values()]) # Print akin to the argparser for k, v in info_dict.items(): print( f\" {k}\" + (\" \" * (max_param_length - v[\"len\"][\"name\"] + self._max_indent)) + f'{v[\"type\"]}' + (\" \" * (max_type_length - v[\"len\"][\"type\"] + self._max_indent)) + f'{v[\"desc\"]} {v[\"default\"]}' ) # Blank for spacing :-/ print(\"\") def _extract_other_types(self, typed, module_name): \"\"\"Takes a high level type and recursively extracts any enum or class types *Args*: typed: highest level type module_name: name of module to match *Returns*: return_list: list of nums (dot notation of module_path.enum_name or module_path.class_name) \"\"\" return_list = [] if hasattr(typed, \"__args__\"): for val in typed.__args__: recurse_return = self._extract_other_types(val, module_name) if isinstance(recurse_return, list): return_list.extend(recurse_return) else: return_list.append(self._extract_other_types(val, module_name)) elif isinstance(typed, EnumMeta) or (typed.__module__ == module_name): return [f\"{typed.__module__}.{typed.__name__}\"] return return_list def _attrs_help(self, input_classes, module_name): \"\"\"Handles walking through a list classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class. If it finds a repeated class in a iterable object it will recursively call self to handle information *Args*: input_classes: list of attr classes module_name: name of module to match *Returns*: None \"\"\" # Handle the main loop other_list = self._handle_help_main(input_classes, module_name) self._handle_help_enums(other_list=other_list, module_name=module_name) @staticmethod def _get_type_string(val, nested_others): \"\"\"Gets the type of the attr val as a string *Args*: val: current attr being processed nested_others: list of nested others to deal with that might have module path info in the string *Returns*: type_string: type of the attr as a str \"\"\" # Grab the base or type info depending on what is provided if \"type\" in val.metadata: type_string = repr(val.metadata[\"type\"]) elif \"base\" in val.metadata: type_string = val.metadata[\"base\"] elif hasattr(val.type, \"__name__\"): type_string = val.type.__name__ else: type_string = str(val.type) # Regex out the typing info if present type_string = re.sub(r\"typing.\", \"\", type_string) # Regex out any nested_others that have module path information for other_val in nested_others: split_other = f\"{'.'.join(other_val.split('.')[:-1])}.\" type_string = re.sub(split_other, \"\", type_string) # Regex the string to see if it matches any Enums in the __main__ module space # Construct the type with the metadata if \"optional\" in val.metadata: type_string = f\"Optional[{type_string}]\" return type_string def _handle_help_main(self, input_classes, module_name): \"\"\"Handles the print of the main class types *Args*: input_classes: current set of input classes module_name: module name to match *Returns*: other_list: extended list of other classes/enums to process \"\"\" # List to catch Enums and classes and handle post spock wrapped attr classes other_list = [] covered_set = set() for attrs_class in input_classes: # Split the docs into class docs and any attribute docs class_doc, attr_docs = self._split_docs(attrs_class) print(\" \" + attrs_class.__name__ + f\" {class_doc}\") # Keep a running info_dict of all the attribute level info info_dict = {} for val in attrs_class.__attrs_attrs__: # If the type is an enum we need to handle it outside of this attr loop # Match the style of nested enums and return a string of module.name notation if isinstance(val.type, EnumMeta): other_list.append(f\"{val.type.__module__}.{val.type.__name__}\") # if there is a type (implied Iterable) -- check it for nested Enums or classes nested_others = self._extract_fnc(val, module_name) if len(nested_others) > 0: other_list.extend(nested_others) # Get the type represented as a string type_string = self._get_type_string(val, nested_others) info_dict.update( self._match_attribute_docs( val.name, attr_docs, type_string, val.default ) ) # Add to covered so we don't print help twice in the case of some recursive nesting covered_set.add(f\"{attrs_class.__module__}.{attrs_class.__name__}\") self._handle_attributes_print(info_dict=info_dict) # Convert the enum list to a set to remove dupes and then back to a list so it is iterable -- set diff to not # repeat return list(set(other_list) - covered_set) def _handle_help_enums(self, other_list, module_name): \"\"\"handles any extra enums from non main args *Args*: other_list: extended list of other classes/enums to process module_name: module name to match *Returns*: None \"\"\" # Iterate any Enum type classes for other in other_list: # if it's longer than 2 then it's an embedded Spock class if \".\".join(other.split(\".\")[:-1]) == module_name: class_type = self._get_from_sys_modules(other) # Invoke recursive call for the class self._attrs_help([class_type], module_name) # Fall back to enum style else: enum = self._get_from_sys_modules(other) # Split the docs into class docs and any attribute docs class_doc, attr_docs = self._split_docs(enum) print(\" \" + enum.__name__ + f\" ({class_doc})\") info_dict = {} for val in enum: info_dict.update( self._match_attribute_docs( attr_name=val.name, attr_docs=attr_docs, attr_type_str=type(val.value).__name__, ) ) self._handle_attributes_print(info_dict=info_dict) @abstractmethod def _extract_fnc(self, val, module_name): \"\"\"Function that gets the nested lists within classes *Args*: val: current attr module_name: matching module name *Returns*: list of any nested classes/enums \"\"\" @staticmethod def _get_from_sys_modules(cls_name): \"\"\"Gets the class from a dot notation name *Args*: cls_name: dot notation enum name *Returns*: module: enum class \"\"\" # Split on dot notation split_string = cls_name.split(\".\") module = None for idx, val in enumerate(split_string): # idx = 0 will always be a call to the sys.modules dict if idx == 0: module = sys.modules[val] # all other idx are paths along the module that need to be traversed # idx = -1 will always be the final Enum object name we want to grab (final getattr call) else: module = getattr(module, val) return module class AttrBuilder(BaseBuilder): \"\"\"Attr specific builder Class that handles building for the attr backend *Attributes* input_classes: list of input classes that link to a backend _configs: None or List of configs to read from _create_save_path: boolean to make the path to save to _desc: description for the arg parser _no_cmd_line: flag to force no command line reads save_path: list of path(s) to save the configs to \"\"\" def __init__(self, *args, **kwargs): \"\"\"AttrBuilder init Args: *args: list of input classes that link to a backend configs: None or List of configs to read from desc: description for the arg parser no_cmd_line: flag to force no command line reads **kwargs: any extra keyword args \"\"\" super().__init__(*args, module_name=\"spock.backend.config\", **kwargs) @staticmethod def _make_group_override_parser(parser, class_obj, class_name): \"\"\"Makes a name specific override parser for a given class obj Takes a class object of the backend and adds a new argument group with argument names given with name Class.name so that individual parameters specific to a class can be overridden. *Args*: parser: argument parser class_obj: instance of a backend class class_name: used for module matching *Returns*: parser: argument parser with new class specific overrides \"\"\" attr_name = class_obj.__name__ group_parser = parser.add_argument_group( title=str(attr_name) + \" Specific Overrides\" ) for val in class_obj.__attrs_attrs__: val_type = val.metadata[\"type\"] if \"type\" in val.metadata else val.type # Check if the val type has __args__ -- this catches lists? # TODO (ncilfone): Fix up this super super ugly logic if ( hasattr(val_type, \"__args__\") and ((list(set(val_type.__args__))[0]).__module__ == class_name) and attr.has((list(set(val_type.__args__))[0])) ): args = list(set(val_type.__args__))[0] for inner_val in args.__attrs_attrs__: arg_name = f\"--{str(attr_name)}.{val.name}.{args.__name__}.{inner_val.name}\" group_parser = make_argument( arg_name, List[inner_val.type], group_parser ) # If it's a reference to a class it needs to be an arg of a simple string as class matching will take care # of it later on elif val_type.__module__ == \"spock.backend.config\": arg_name = f\"--{str(attr_name)}.{val.name}\" val_type = str group_parser = make_argument(arg_name, val_type, group_parser) else: arg_name = f\"--{str(attr_name)}.{val.name}\" group_parser = make_argument(arg_name, val_type, group_parser) return parser def _handle_arguments(self, args, class_obj): attr_name = class_obj.__name__ class_names = [val.__name__ for val in self.input_classes] # Handle repeated classes if ( attr_name in class_names and attr_name in args and isinstance(args[attr_name], list) ): fields = self._handle_repeated(args[attr_name], attr_name, class_names) # Handle non-repeated classes else: fields = {} for val in class_obj.__attrs_attrs__: # Check if namespace is named and then check for key -- checking for local class def if attr_name in args and val.name in args[attr_name]: fields[val.name] = self._handle_nested_class( args, args[attr_name][val.name], class_names ) # If not named then just check for keys -- checking for global def elif val.name in args: fields[val.name] = self._handle_nested_class( args, args[val.name], class_names ) # Check for special keys to set if ( \"special_key\" in val.metadata and val.metadata[\"special_key\"] is not None ): if val.name in args: self.save_path = args[val.name] elif val.default is not None: self.save_path = val.default return fields def _handle_repeated(self, args, check_value, class_names): \"\"\"Handles repeated classes as lists *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: list of input_class[match)idx[0]] types filled with repeated values \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] return [self.input_classes[match_idx[0]](**val) for val in args] def _handle_nested_class(self, args, check_value, class_names): \"\"\"Handles passing another class to the field dictionary *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: either the check_value or the necessary class \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] # If so then create the needed class object by unrolling the args to **kwargs and return it if len(match_idx) > 0: if len(match_idx) > 1: raise ValueError( \"Match error -- multiple classes with the same name definition\" ) else: if (args.get(self.input_classes[match_idx[0]].__name__) is None) and ( check_value not in class_names ): raise ValueError( f\"Cannot map a definition for the referenced class \" f\"{self.input_classes[match_idx[0]].__name__}\" ) current_arg = args.get(self.input_classes[match_idx[0]].__name__, {}) if isinstance(current_arg, list): class_value = [ self.input_classes[match_idx[0]](**val) for val in current_arg ] else: recurse_args = ( self._handle_recursive_defaults( args.get(check_value), args, class_names ) if check_value in args else {} ) class_value = self.input_classes[match_idx[0]](**recurse_args) return_value = class_value # else return the expected value else: return_value = check_value return return_value def _extract_fnc(self, val, module_name): \"\"\"Function that gets the nested lists within classes *Args*: val: current attr module_name: matching module name *Returns*: list of any nested classes/enums \"\"\" return ( self._extract_other_types(val.metadata[\"type\"], module_name) if \"type\" in val.metadata else [] ) Classes AttrBuilder class AttrBuilder ( * args , ** kwargs ) ??? example \"View Source\" class AttrBuilder(BaseBuilder): \"\"\"Attr specific builder Class that handles building for the attr backend *Attributes* input_classes: list of input classes that link to a backend _configs: None or List of configs to read from _create_save_path: boolean to make the path to save to _desc: description for the arg parser _no_cmd_line: flag to force no command line reads save_path: list of path(s) to save the configs to \"\"\" def __init__(self, *args, **kwargs): \"\"\"AttrBuilder init Args: *args: list of input classes that link to a backend configs: None or List of configs to read from desc: description for the arg parser no_cmd_line: flag to force no command line reads **kwargs: any extra keyword args \"\"\" super().__init__(*args, module_name=\"spock.backend.config\", **kwargs) @staticmethod def _make_group_override_parser(parser, class_obj, class_name): \"\"\"Makes a name specific override parser for a given class obj Takes a class object of the backend and adds a new argument group with argument names given with name Class.name so that individual parameters specific to a class can be overridden. *Args*: parser: argument parser class_obj: instance of a backend class class_name: used for module matching *Returns*: parser: argument parser with new class specific overrides \"\"\" attr_name = class_obj.__name__ group_parser = parser.add_argument_group( title=str(attr_name) + \" Specific Overrides\" ) for val in class_obj.__attrs_attrs__: val_type = val.metadata[\"type\"] if \"type\" in val.metadata else val.type # Check if the val type has __args__ -- this catches lists? # TODO (ncilfone): Fix up this super super ugly logic if ( hasattr(val_type, \"__args__\") and ((list(set(val_type.__args__))[0]).__module__ == class_name) and attr.has((list(set(val_type.__args__))[0])) ): args = list(set(val_type.__args__))[0] for inner_val in args.__attrs_attrs__: arg_name = f\"--{str(attr_name)}.{val.name}.{args.__name__}.{inner_val.name}\" group_parser = make_argument( arg_name, List[inner_val.type], group_parser ) # If it's a reference to a class it needs to be an arg of a simple string as class matching will take care # of it later on elif val_type.__module__ == \"spock.backend.config\": arg_name = f\"--{str(attr_name)}.{val.name}\" val_type = str group_parser = make_argument(arg_name, val_type, group_parser) else: arg_name = f\"--{str(attr_name)}.{val.name}\" group_parser = make_argument(arg_name, val_type, group_parser) return parser def _handle_arguments(self, args, class_obj): attr_name = class_obj.__name__ class_names = [val.__name__ for val in self.input_classes] # Handle repeated classes if ( attr_name in class_names and attr_name in args and isinstance(args[attr_name], list) ): fields = self._handle_repeated(args[attr_name], attr_name, class_names) # Handle non-repeated classes else: fields = {} for val in class_obj.__attrs_attrs__: # Check if namespace is named and then check for key -- checking for local class def if attr_name in args and val.name in args[attr_name]: fields[val.name] = self._handle_nested_class( args, args[attr_name][val.name], class_names ) # If not named then just check for keys -- checking for global def elif val.name in args: fields[val.name] = self._handle_nested_class( args, args[val.name], class_names ) # Check for special keys to set if ( \"special_key\" in val.metadata and val.metadata[\"special_key\"] is not None ): if val.name in args: self.save_path = args[val.name] elif val.default is not None: self.save_path = val.default return fields def _handle_repeated(self, args, check_value, class_names): \"\"\"Handles repeated classes as lists *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: list of input_class[match)idx[0]] types filled with repeated values \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] return [self.input_classes[match_idx[0]](**val) for val in args] def _handle_nested_class(self, args, check_value, class_names): \"\"\"Handles passing another class to the field dictionary *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: either the check_value or the necessary class \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] # If so then create the needed class object by unrolling the args to **kwargs and return it if len(match_idx) > 0: if len(match_idx) > 1: raise ValueError( \"Match error -- multiple classes with the same name definition\" ) else: if (args.get(self.input_classes[match_idx[0]].__name__) is None) and ( check_value not in class_names ): raise ValueError( f\"Cannot map a definition for the referenced class \" f\"{self.input_classes[match_idx[0]].__name__}\" ) current_arg = args.get(self.input_classes[match_idx[0]].__name__, {}) if isinstance(current_arg, list): class_value = [ self.input_classes[match_idx[0]](**val) for val in current_arg ] else: recurse_args = ( self._handle_recursive_defaults( args.get(check_value), args, class_names ) if check_value in args else {} ) class_value = self.input_classes[match_idx[0]](**recurse_args) return_value = class_value # else return the expected value else: return_value = check_value return return_value def _extract_fnc(self, val, module_name): \"\"\"Function that gets the nested lists within classes *Args*: val: current attr module_name: matching module name *Returns*: list of any nested classes/enums \"\"\" return ( self._extract_other_types(val.metadata[\"type\"], module_name) if \"type\" in val.metadata else [] ) Ancestors (in MRO) spock.backend.builder.BaseBuilder abc.ABC Methods build_override_parsers def build_override_parsers ( self , parser ) Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers Args : parser: argument parser Returns : parser: argument parser with new class specific overrides ??? example \"View Source\" def build_override_parsers(self, parser): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers *Args*: parser: argument parser *Returns*: parser: argument parser with new class specific overrides \"\"\" # Build out each class override specific parser for val in self.input_classes: parser = self._make_group_override_parser( parser=parser, class_obj=val, class_name=self._module_name ) return parser generate def generate ( self , dict_args ) Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values Args : dict_args: dictionary of arguments from the configs Returns : namespace containing automatically generated instances of the classes ??? example \"View Source\" def generate(self, dict_args): \"\"\"Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values *Args*: dict_args: dictionary of arguments from the configs *Returns*: namespace containing automatically generated instances of the classes \"\"\" auto_dict = {} for attr_classes in self.input_classes: attr_build = self._auto_generate(dict_args, attr_classes) if isinstance(attr_build, list): class_name = list({type(val).__name__ for val in attr_build}) if len(class_name) > 1: raise ValueError(\"Repeated class has more than one unique name\") auto_dict.update({class_name[0]: attr_build}) else: auto_dict.update({type(attr_build).__name__: attr_build}) return Spockspace(**auto_dict) handle_help_info def handle_help_info ( self ) Handles walking through classes to get help info For each class this function will search doc and attempt to pull out help information for both the class itself and each attribute within the class Returns : None ??? example \"View Source\" def handle_help_info(self): \"\"\"Handles walking through classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class *Returns*: None \"\"\" self._attrs_help(self.input_classes, self._module_name) BaseBuilder class BaseBuilder ( * args , max_indent = 4 , module_name , ** kwargs ) ??? example \"View Source\" class BaseBuilder(ABC): # pylint: disable=too-few-public-methods \"\"\"Base class for building the backend specific builders This class handles the interface to the backend with the generic ConfigArgBuilder so that different backends can be used to handle processing *Attributes* input_classes: list of input classes that link to a backend _configs: None or List of configs to read from _desc: description for the arg parser _no_cmd_line: flag to force no command line reads _max_indent: maximum to indent between help prints save_path: list of path(s) to save the configs to \"\"\" def __init__(self, *args, max_indent=4, module_name, **kwargs): self.input_classes = args self._module_name = module_name self._max_indent = max_indent self.save_path = None @staticmethod @abstractmethod def _make_group_override_parser(parser, class_obj, class_name): \"\"\"Makes a name specific override parser for a given class obj Takes a class object of the backend and adds a new argument group with argument names given with name Class.name so that individual parameters specific to a class can be overridden. *Args*: parser: argument parser class_obj: instance of a backend class class_name: used for module matching *Returns*: parser: argument parser with new class specific overrides \"\"\" def handle_help_info(self): \"\"\"Handles walking through classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class *Returns*: None \"\"\" self._attrs_help(self.input_classes, self._module_name) def _handle_arguments(self, args, class_obj): \"\"\"Handles all argument mapping Creates a dictionary of named parameters that are mapped to the final type of object *Args*: args: read file arguments class_obj: instance of a class obj *Returns*: fields: dictionary of mapped parameters \"\"\" attr_name = class_obj.__name__ class_names = [val.__name__ for val in self.input_classes] # Handle repeated classes if ( attr_name in class_names and attr_name in args and isinstance(args[attr_name], list) ): fields = self._handle_repeated(args[attr_name], attr_name, class_names) # Handle non-repeated classes else: fields = {} for val in class_obj.__attrs_attrs__: # Check if namespace is named and then check for key -- checking for local class def if attr_name in args and val.name in args[attr_name]: fields[val.name] = self._handle_nested_class( args, args[attr_name][val.name], class_names ) # If not named then just check for keys -- checking for global def elif val.name in args: fields[val.name] = self._handle_nested_class( args, args[val.name], class_names ) # Check for special keys to set if ( \"special_key\" in val.metadata and val.metadata[\"special_key\"] is not None ): if val.name in args: self.save_path = args[val.name] elif val.default is not None: self.save_path = val.default return fields def _handle_repeated(self, args, check_value, class_names): \"\"\"Handles repeated classes as lists *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: list of input_class[match)idx[0]] types filled with repeated values \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] return [self.input_classes[match_idx[0]](**val) for val in args] def _handle_nested_class(self, args, check_value, class_names): \"\"\"Handles passing another class to the field dictionary *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: either the check_value or the necessary class \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] # If so then create the needed class object by unrolling the args to **kwargs and return it if len(match_idx) > 0: if len(match_idx) > 1: raise ValueError( \"Match error -- multiple classes with the same name definition\" ) else: if args.get(self.input_classes[match_idx[0]].__name__) is None: raise ValueError( f\"Missing config file definition for the referenced class \" f\"{self.input_classes[match_idx[0]].__name__}\" ) current_arg = args.get(self.input_classes[match_idx[0]].__name__) if isinstance(current_arg, list): class_value = [ self.input_classes[match_idx[0]](**val) for val in current_arg ] else: class_value = self.input_classes[match_idx[0]](**current_arg) return_value = class_value # else return the expected value else: return_value = check_value return return_value def generate(self, dict_args): \"\"\"Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values *Args*: dict_args: dictionary of arguments from the configs *Returns*: namespace containing automatically generated instances of the classes \"\"\" auto_dict = {} for attr_classes in self.input_classes: attr_build = self._auto_generate(dict_args, attr_classes) if isinstance(attr_build, list): class_name = list({type(val).__name__ for val in attr_build}) if len(class_name) > 1: raise ValueError(\"Repeated class has more than one unique name\") auto_dict.update({class_name[0]: attr_build}) else: auto_dict.update({type(attr_build).__name__: attr_build}) return Spockspace(**auto_dict) def _auto_generate(self, args, input_class): \"\"\"Builds an instance of an attr class Builds an instance with the necessary field values from the argument dictionary read from the config file(s) *Args*: args: dictionary of arguments read from the config file(s) data_class: data class to build *Returns*: An instance of data_class with correct values assigned to fields \"\"\" # Handle the basic data types fields = self._handle_arguments(args, input_class) if isinstance(fields, list): return_value = fields else: self._handle_late_defaults(args, fields, input_class) return_value = input_class(**fields) return return_value def _handle_late_defaults(self, args, fields, input_class): \"\"\"Handles late defaults when the type is non-standard If the default type is not a base python type then we need to catch those defaults here and build the correct values from the input classes while maintaining the optional nature. The trick is to exclude all 'base' types as these defaults are covered by the attr default value *Args*: args: dictionary of arguments read from the config file(s) fields: current fields returned from _handle_arguments input_class: which input class being checked for late defaults *Returns*: fields: updated field dictionary with late defaults set \"\"\" names = [val.name for val in input_class.__attrs_attrs__] class_names = [val.__name__ for val in self.input_classes] field_list = list(fields.keys()) arg_list = list(args.keys()) # Exclude all the base types that are supported -- these can be set by attrs exclude_list = [ \"_Nothing\", \"NoneType\", \"bool\", \"int\", \"float\", \"str\", \"list\", \"tuple\", ] for val in names: if val not in field_list: # Gets the name of the class to default to default_type_name = type( getattr(input_class.__attrs_attrs__, val).default ).__name__ if default_type_name not in exclude_list: # Gets the default class object default_attr = getattr(input_class.__attrs_attrs__, val).default # If the default is given for a class then it's the actual class and not a type -- logic needs # to deal with both if type(default_attr).__name__ == \"type\": default_name = default_attr.__name__ else: default_name = type(default_attr).__name__ # Skip if in the exclude list else: default_name = None # if we need to fall back onto the default and ff it's in the arg_list then we have a # definition coming in from the config file if default_name is not None and default_name in arg_list: # This handles lists of class type repeats -- these cannot be nested as the logic would be too # confusing to map to if isinstance(args.get(default_name), list): default_value = [ self.input_classes[class_names.index(default_name)]( **arg_val ) for arg_val in args.get(default_name) ] # This handles basics and references to other classes -- here we need to recurse to grab any nested # defs since classes are passed as strings to the config but are defined via Enums (handled #139) else: recurse_args = self._handle_recursive_defaults( args.get(default_name), args, class_names ) default_value = self.input_classes[ class_names.index(default_name) ](**recurse_args) fields.update({val: default_value}) return fields def _handle_recursive_defaults(self, curr_arg, all_args, class_names): \"\"\"Recurses through the args from the config read to determine if it can map to a definition *Args*: curr_arg: current argument all_args: all argument dictionary class_names: list of class names *Returns*: out_dict: recursively mapped dictionary of attributes \"\"\" out_dict = {} for k, v in curr_arg.items(): # If the value is a reference to another class we need to recurse if v in class_names: # Recurse only if in the all_args dict (from the config file) if v in all_args: bubbled_dict = self._handle_recursive_defaults( all_args.get(v), all_args, class_names ) out_dict.update( {k: self.input_classes[class_names.index(v)](**bubbled_dict)} ) # Else fall back on default instantiation else: out_dict.update({k: self.input_classes[class_names.index(v)]()}) else: out_dict.update({k: v}) return out_dict def build_override_parsers(self, parser): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers *Args*: parser: argument parser *Returns*: parser: argument parser with new class specific overrides \"\"\" # Build out each class override specific parser for val in self.input_classes: parser = self._make_group_override_parser( parser=parser, class_obj=val, class_name=self._module_name ) return parser @staticmethod def _get_from_kwargs(args, configs): \"\"\"Get configs from the configs kwarg *Args*: args: argument namespace configs: config kwarg *Returns*: args: arg namespace \"\"\" if isinstance(configs, list): args.config.extend(configs) else: raise TypeError( f\"configs kwarg must be of type list -- given {type(configs)}\" ) return args @staticmethod def _find_attribute_idx(newline_split_docs): \"\"\"Finds the possible split between the header and Attribute annotations *Args*: newline_split_docs: new line split text Returns: idx: -1 if none or the idx of Attributes \"\"\" for idx, val in enumerate(newline_split_docs): re_check = re.search(r\"(?i)Attribute?s?:\", val) if re_check is not None: return idx return -1 def _split_docs(self, obj): \"\"\"Possibly splits head class doc string from attribute docstrings Attempts to find the first contiguous line within the Google style docstring to use as the class docstring. Splits the docs base on the Attributes tag if present. *Args*: obj: class object to rip info from *Returns*: class_doc: class docstring if present or blank str attr_doc: list of attribute doc strings \"\"\" if obj.__doc__ is not None: # Split by new line newline_split_docs = obj.__doc__.split(\"\\n\") # Cleanup l/t whitespace newline_split_docs = [val.strip() for val in newline_split_docs] else: newline_split_docs = [] # Find the break between the class docs and the Attribute section -- if this returns -1 then there is no # Attributes section attr_idx = self._find_attribute_idx(newline_split_docs) head_docs = ( newline_split_docs[:attr_idx] if attr_idx != -1 else newline_split_docs ) attr_docs = newline_split_docs[attr_idx:] if attr_idx != -1 else [] # Grab only the first contiguous line as everything else will probably be too verbose (e.g. the # mid-level docstring that has detailed descriptions class_doc = \"\" for idx, val in enumerate(head_docs): class_doc += f\" {val}\" if idx + 1 != len(head_docs) and head_docs[idx + 1] == \"\": break # Clean up any l/t whitespace class_doc = class_doc.strip() if len(class_doc) > 0: class_doc = f\"-- {class_doc}\" return class_doc, attr_docs @staticmethod def _match_attribute_docs( attr_name, attr_docs, attr_type_str, attr_default=NOTHING ): \"\"\"Matches class attributes with attribute docstrings via regex *Args*: attr_name: attribute name attr_docs: list of attribute docstrings attr_type_str: str representation of the attribute type attr_default: str representation of a possible default value *Returns*: dictionary of packed attribute information \"\"\" # Regex match each value a_str = None for a_doc in attr_docs: match_re = re.search(r\"(?i)^\" + attr_name + \"?:\", a_doc) # Find only the first match -- if more than one than ignore if match_re: a_str = a_doc[match_re.end() :].strip() return { attr_name: { \"type\": attr_type_str, \"desc\": a_str if a_str is not None else \"\", \"default\": \"(default: \" + repr(attr_default) + \")\" if type(attr_default).__name__ != \"_Nothing\" else \"\", \"len\": {\"name\": len(attr_name), \"type\": len(attr_type_str)}, } } def _handle_attributes_print(self, info_dict): \"\"\"Prints attribute information in an argparser style format *Args*: info_dict: packed attribute info dictionary to print \"\"\" # Figure out indents max_param_length = max([len(k) for k in info_dict.keys()]) max_type_length = max([v[\"len\"][\"type\"] for v in info_dict.values()]) # Print akin to the argparser for k, v in info_dict.items(): print( f\" {k}\" + (\" \" * (max_param_length - v[\"len\"][\"name\"] + self._max_indent)) + f'{v[\"type\"]}' + (\" \" * (max_type_length - v[\"len\"][\"type\"] + self._max_indent)) + f'{v[\"desc\"]} {v[\"default\"]}' ) # Blank for spacing :-/ print(\"\") def _extract_other_types(self, typed, module_name): \"\"\"Takes a high level type and recursively extracts any enum or class types *Args*: typed: highest level type module_name: name of module to match *Returns*: return_list: list of nums (dot notation of module_path.enum_name or module_path.class_name) \"\"\" return_list = [] if hasattr(typed, \"__args__\"): for val in typed.__args__: recurse_return = self._extract_other_types(val, module_name) if isinstance(recurse_return, list): return_list.extend(recurse_return) else: return_list.append(self._extract_other_types(val, module_name)) elif isinstance(typed, EnumMeta) or (typed.__module__ == module_name): return [f\"{typed.__module__}.{typed.__name__}\"] return return_list def _attrs_help(self, input_classes, module_name): \"\"\"Handles walking through a list classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class. If it finds a repeated class in a iterable object it will recursively call self to handle information *Args*: input_classes: list of attr classes module_name: name of module to match *Returns*: None \"\"\" # Handle the main loop other_list = self._handle_help_main(input_classes, module_name) self._handle_help_enums(other_list=other_list, module_name=module_name) @staticmethod def _get_type_string(val, nested_others): \"\"\"Gets the type of the attr val as a string *Args*: val: current attr being processed nested_others: list of nested others to deal with that might have module path info in the string *Returns*: type_string: type of the attr as a str \"\"\" # Grab the base or type info depending on what is provided if \"type\" in val.metadata: type_string = repr(val.metadata[\"type\"]) elif \"base\" in val.metadata: type_string = val.metadata[\"base\"] elif hasattr(val.type, \"__name__\"): type_string = val.type.__name__ else: type_string = str(val.type) # Regex out the typing info if present type_string = re.sub(r\"typing.\", \"\", type_string) # Regex out any nested_others that have module path information for other_val in nested_others: split_other = f\"{'.'.join(other_val.split('.')[:-1])}.\" type_string = re.sub(split_other, \"\", type_string) # Regex the string to see if it matches any Enums in the __main__ module space # Construct the type with the metadata if \"optional\" in val.metadata: type_string = f\"Optional[{type_string}]\" return type_string def _handle_help_main(self, input_classes, module_name): \"\"\"Handles the print of the main class types *Args*: input_classes: current set of input classes module_name: module name to match *Returns*: other_list: extended list of other classes/enums to process \"\"\" # List to catch Enums and classes and handle post spock wrapped attr classes other_list = [] covered_set = set() for attrs_class in input_classes: # Split the docs into class docs and any attribute docs class_doc, attr_docs = self._split_docs(attrs_class) print(\" \" + attrs_class.__name__ + f\" {class_doc}\") # Keep a running info_dict of all the attribute level info info_dict = {} for val in attrs_class.__attrs_attrs__: # If the type is an enum we need to handle it outside of this attr loop # Match the style of nested enums and return a string of module.name notation if isinstance(val.type, EnumMeta): other_list.append(f\"{val.type.__module__}.{val.type.__name__}\") # if there is a type (implied Iterable) -- check it for nested Enums or classes nested_others = self._extract_fnc(val, module_name) if len(nested_others) > 0: other_list.extend(nested_others) # Get the type represented as a string type_string = self._get_type_string(val, nested_others) info_dict.update( self._match_attribute_docs( val.name, attr_docs, type_string, val.default ) ) # Add to covered so we don't print help twice in the case of some recursive nesting covered_set.add(f\"{attrs_class.__module__}.{attrs_class.__name__}\") self._handle_attributes_print(info_dict=info_dict) # Convert the enum list to a set to remove dupes and then back to a list so it is iterable -- set diff to not # repeat return list(set(other_list) - covered_set) def _handle_help_enums(self, other_list, module_name): \"\"\"handles any extra enums from non main args *Args*: other_list: extended list of other classes/enums to process module_name: module name to match *Returns*: None \"\"\" # Iterate any Enum type classes for other in other_list: # if it's longer than 2 then it's an embedded Spock class if \".\".join(other.split(\".\")[:-1]) == module_name: class_type = self._get_from_sys_modules(other) # Invoke recursive call for the class self._attrs_help([class_type], module_name) # Fall back to enum style else: enum = self._get_from_sys_modules(other) # Split the docs into class docs and any attribute docs class_doc, attr_docs = self._split_docs(enum) print(\" \" + enum.__name__ + f\" ({class_doc})\") info_dict = {} for val in enum: info_dict.update( self._match_attribute_docs( attr_name=val.name, attr_docs=attr_docs, attr_type_str=type(val.value).__name__, ) ) self._handle_attributes_print(info_dict=info_dict) @abstractmethod def _extract_fnc(self, val, module_name): \"\"\"Function that gets the nested lists within classes *Args*: val: current attr module_name: matching module name *Returns*: list of any nested classes/enums \"\"\" @staticmethod def _get_from_sys_modules(cls_name): \"\"\"Gets the class from a dot notation name *Args*: cls_name: dot notation enum name *Returns*: module: enum class \"\"\" # Split on dot notation split_string = cls_name.split(\".\") module = None for idx, val in enumerate(split_string): # idx = 0 will always be a call to the sys.modules dict if idx == 0: module = sys.modules[val] # all other idx are paths along the module that need to be traversed # idx = -1 will always be the final Enum object name we want to grab (final getattr call) else: module = getattr(module, val) return module Ancestors (in MRO) abc.ABC Descendants spock.backend.builder.AttrBuilder spock.addons.tune.builder.TunerBuilder Methods build_override_parsers def build_override_parsers ( self , parser ) Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers Args : parser: argument parser Returns : parser: argument parser with new class specific overrides ??? example \"View Source\" def build_override_parsers(self, parser): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers *Args*: parser: argument parser *Returns*: parser: argument parser with new class specific overrides \"\"\" # Build out each class override specific parser for val in self.input_classes: parser = self._make_group_override_parser( parser=parser, class_obj=val, class_name=self._module_name ) return parser generate def generate ( self , dict_args ) Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values Args : dict_args: dictionary of arguments from the configs Returns : namespace containing automatically generated instances of the classes ??? example \"View Source\" def generate(self, dict_args): \"\"\"Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values *Args*: dict_args: dictionary of arguments from the configs *Returns*: namespace containing automatically generated instances of the classes \"\"\" auto_dict = {} for attr_classes in self.input_classes: attr_build = self._auto_generate(dict_args, attr_classes) if isinstance(attr_build, list): class_name = list({type(val).__name__ for val in attr_build}) if len(class_name) > 1: raise ValueError(\"Repeated class has more than one unique name\") auto_dict.update({class_name[0]: attr_build}) else: auto_dict.update({type(attr_build).__name__: attr_build}) return Spockspace(**auto_dict) handle_help_info def handle_help_info ( self ) Handles walking through classes to get help info For each class this function will search doc and attempt to pull out help information for both the class itself and each attribute within the class Returns : None ??? example \"View Source\" def handle_help_info(self): \"\"\"Handles walking through classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class *Returns*: None \"\"\" self._attrs_help(self.input_classes, self._module_name)","title":"Builder"},{"location":"reference/spock/backend/builder/#module-spockbackendbuilder","text":"Handles the building/saving of the configurations from the Spock config classes None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the building/saving of the configurations from the Spock config classes\"\"\" import re import sys from abc import ABC, abstractmethod from enum import EnumMeta from typing import List import attr from attr import NOTHING from spock.backend.wrappers import Spockspace from spock.utils import make_argument class BaseBuilder(ABC): # pylint: disable=too-few-public-methods \"\"\"Base class for building the backend specific builders This class handles the interface to the backend with the generic ConfigArgBuilder so that different backends can be used to handle processing *Attributes* input_classes: list of input classes that link to a backend _configs: None or List of configs to read from _desc: description for the arg parser _no_cmd_line: flag to force no command line reads _max_indent: maximum to indent between help prints save_path: list of path(s) to save the configs to \"\"\" def __init__(self, *args, max_indent=4, module_name, **kwargs): self.input_classes = args self._module_name = module_name self._max_indent = max_indent self.save_path = None @staticmethod @abstractmethod def _make_group_override_parser(parser, class_obj, class_name): \"\"\"Makes a name specific override parser for a given class obj Takes a class object of the backend and adds a new argument group with argument names given with name Class.name so that individual parameters specific to a class can be overridden. *Args*: parser: argument parser class_obj: instance of a backend class class_name: used for module matching *Returns*: parser: argument parser with new class specific overrides \"\"\" def handle_help_info(self): \"\"\"Handles walking through classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class *Returns*: None \"\"\" self._attrs_help(self.input_classes, self._module_name) def _handle_arguments(self, args, class_obj): \"\"\"Handles all argument mapping Creates a dictionary of named parameters that are mapped to the final type of object *Args*: args: read file arguments class_obj: instance of a class obj *Returns*: fields: dictionary of mapped parameters \"\"\" attr_name = class_obj.__name__ class_names = [val.__name__ for val in self.input_classes] # Handle repeated classes if ( attr_name in class_names and attr_name in args and isinstance(args[attr_name], list) ): fields = self._handle_repeated(args[attr_name], attr_name, class_names) # Handle non-repeated classes else: fields = {} for val in class_obj.__attrs_attrs__: # Check if namespace is named and then check for key -- checking for local class def if attr_name in args and val.name in args[attr_name]: fields[val.name] = self._handle_nested_class( args, args[attr_name][val.name], class_names ) # If not named then just check for keys -- checking for global def elif val.name in args: fields[val.name] = self._handle_nested_class( args, args[val.name], class_names ) # Check for special keys to set if ( \"special_key\" in val.metadata and val.metadata[\"special_key\"] is not None ): if val.name in args: self.save_path = args[val.name] elif val.default is not None: self.save_path = val.default return fields def _handle_repeated(self, args, check_value, class_names): \"\"\"Handles repeated classes as lists *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: list of input_class[match)idx[0]] types filled with repeated values \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] return [self.input_classes[match_idx[0]](**val) for val in args] def _handle_nested_class(self, args, check_value, class_names): \"\"\"Handles passing another class to the field dictionary *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: either the check_value or the necessary class \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] # If so then create the needed class object by unrolling the args to **kwargs and return it if len(match_idx) > 0: if len(match_idx) > 1: raise ValueError( \"Match error -- multiple classes with the same name definition\" ) else: if args.get(self.input_classes[match_idx[0]].__name__) is None: raise ValueError( f\"Missing config file definition for the referenced class \" f\"{self.input_classes[match_idx[0]].__name__}\" ) current_arg = args.get(self.input_classes[match_idx[0]].__name__) if isinstance(current_arg, list): class_value = [ self.input_classes[match_idx[0]](**val) for val in current_arg ] else: class_value = self.input_classes[match_idx[0]](**current_arg) return_value = class_value # else return the expected value else: return_value = check_value return return_value def generate(self, dict_args): \"\"\"Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values *Args*: dict_args: dictionary of arguments from the configs *Returns*: namespace containing automatically generated instances of the classes \"\"\" auto_dict = {} for attr_classes in self.input_classes: attr_build = self._auto_generate(dict_args, attr_classes) if isinstance(attr_build, list): class_name = list({type(val).__name__ for val in attr_build}) if len(class_name) > 1: raise ValueError(\"Repeated class has more than one unique name\") auto_dict.update({class_name[0]: attr_build}) else: auto_dict.update({type(attr_build).__name__: attr_build}) return Spockspace(**auto_dict) def _auto_generate(self, args, input_class): \"\"\"Builds an instance of an attr class Builds an instance with the necessary field values from the argument dictionary read from the config file(s) *Args*: args: dictionary of arguments read from the config file(s) data_class: data class to build *Returns*: An instance of data_class with correct values assigned to fields \"\"\" # Handle the basic data types fields = self._handle_arguments(args, input_class) if isinstance(fields, list): return_value = fields else: self._handle_late_defaults(args, fields, input_class) return_value = input_class(**fields) return return_value def _handle_late_defaults(self, args, fields, input_class): \"\"\"Handles late defaults when the type is non-standard If the default type is not a base python type then we need to catch those defaults here and build the correct values from the input classes while maintaining the optional nature. The trick is to exclude all 'base' types as these defaults are covered by the attr default value *Args*: args: dictionary of arguments read from the config file(s) fields: current fields returned from _handle_arguments input_class: which input class being checked for late defaults *Returns*: fields: updated field dictionary with late defaults set \"\"\" names = [val.name for val in input_class.__attrs_attrs__] class_names = [val.__name__ for val in self.input_classes] field_list = list(fields.keys()) arg_list = list(args.keys()) # Exclude all the base types that are supported -- these can be set by attrs exclude_list = [ \"_Nothing\", \"NoneType\", \"bool\", \"int\", \"float\", \"str\", \"list\", \"tuple\", ] for val in names: if val not in field_list: # Gets the name of the class to default to default_type_name = type( getattr(input_class.__attrs_attrs__, val).default ).__name__ if default_type_name not in exclude_list: # Gets the default class object default_attr = getattr(input_class.__attrs_attrs__, val).default # If the default is given for a class then it's the actual class and not a type -- logic needs # to deal with both if type(default_attr).__name__ == \"type\": default_name = default_attr.__name__ else: default_name = type(default_attr).__name__ # Skip if in the exclude list else: default_name = None # if we need to fall back onto the default and ff it's in the arg_list then we have a # definition coming in from the config file if default_name is not None and default_name in arg_list: # This handles lists of class type repeats -- these cannot be nested as the logic would be too # confusing to map to if isinstance(args.get(default_name), list): default_value = [ self.input_classes[class_names.index(default_name)]( **arg_val ) for arg_val in args.get(default_name) ] # This handles basics and references to other classes -- here we need to recurse to grab any nested # defs since classes are passed as strings to the config but are defined via Enums (handled #139) else: recurse_args = self._handle_recursive_defaults( args.get(default_name), args, class_names ) default_value = self.input_classes[ class_names.index(default_name) ](**recurse_args) fields.update({val: default_value}) return fields def _handle_recursive_defaults(self, curr_arg, all_args, class_names): \"\"\"Recurses through the args from the config read to determine if it can map to a definition *Args*: curr_arg: current argument all_args: all argument dictionary class_names: list of class names *Returns*: out_dict: recursively mapped dictionary of attributes \"\"\" out_dict = {} for k, v in curr_arg.items(): # If the value is a reference to another class we need to recurse if v in class_names: # Recurse only if in the all_args dict (from the config file) if v in all_args: bubbled_dict = self._handle_recursive_defaults( all_args.get(v), all_args, class_names ) out_dict.update( {k: self.input_classes[class_names.index(v)](**bubbled_dict)} ) # Else fall back on default instantiation else: out_dict.update({k: self.input_classes[class_names.index(v)]()}) else: out_dict.update({k: v}) return out_dict def build_override_parsers(self, parser): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers *Args*: parser: argument parser *Returns*: parser: argument parser with new class specific overrides \"\"\" # Build out each class override specific parser for val in self.input_classes: parser = self._make_group_override_parser( parser=parser, class_obj=val, class_name=self._module_name ) return parser @staticmethod def _get_from_kwargs(args, configs): \"\"\"Get configs from the configs kwarg *Args*: args: argument namespace configs: config kwarg *Returns*: args: arg namespace \"\"\" if isinstance(configs, list): args.config.extend(configs) else: raise TypeError( f\"configs kwarg must be of type list -- given {type(configs)}\" ) return args @staticmethod def _find_attribute_idx(newline_split_docs): \"\"\"Finds the possible split between the header and Attribute annotations *Args*: newline_split_docs: new line split text Returns: idx: -1 if none or the idx of Attributes \"\"\" for idx, val in enumerate(newline_split_docs): re_check = re.search(r\"(?i)Attribute?s?:\", val) if re_check is not None: return idx return -1 def _split_docs(self, obj): \"\"\"Possibly splits head class doc string from attribute docstrings Attempts to find the first contiguous line within the Google style docstring to use as the class docstring. Splits the docs base on the Attributes tag if present. *Args*: obj: class object to rip info from *Returns*: class_doc: class docstring if present or blank str attr_doc: list of attribute doc strings \"\"\" if obj.__doc__ is not None: # Split by new line newline_split_docs = obj.__doc__.split(\"\\n\") # Cleanup l/t whitespace newline_split_docs = [val.strip() for val in newline_split_docs] else: newline_split_docs = [] # Find the break between the class docs and the Attribute section -- if this returns -1 then there is no # Attributes section attr_idx = self._find_attribute_idx(newline_split_docs) head_docs = ( newline_split_docs[:attr_idx] if attr_idx != -1 else newline_split_docs ) attr_docs = newline_split_docs[attr_idx:] if attr_idx != -1 else [] # Grab only the first contiguous line as everything else will probably be too verbose (e.g. the # mid-level docstring that has detailed descriptions class_doc = \"\" for idx, val in enumerate(head_docs): class_doc += f\" {val}\" if idx + 1 != len(head_docs) and head_docs[idx + 1] == \"\": break # Clean up any l/t whitespace class_doc = class_doc.strip() if len(class_doc) > 0: class_doc = f\"-- {class_doc}\" return class_doc, attr_docs @staticmethod def _match_attribute_docs( attr_name, attr_docs, attr_type_str, attr_default=NOTHING ): \"\"\"Matches class attributes with attribute docstrings via regex *Args*: attr_name: attribute name attr_docs: list of attribute docstrings attr_type_str: str representation of the attribute type attr_default: str representation of a possible default value *Returns*: dictionary of packed attribute information \"\"\" # Regex match each value a_str = None for a_doc in attr_docs: match_re = re.search(r\"(?i)^\" + attr_name + \"?:\", a_doc) # Find only the first match -- if more than one than ignore if match_re: a_str = a_doc[match_re.end() :].strip() return { attr_name: { \"type\": attr_type_str, \"desc\": a_str if a_str is not None else \"\", \"default\": \"(default: \" + repr(attr_default) + \")\" if type(attr_default).__name__ != \"_Nothing\" else \"\", \"len\": {\"name\": len(attr_name), \"type\": len(attr_type_str)}, } } def _handle_attributes_print(self, info_dict): \"\"\"Prints attribute information in an argparser style format *Args*: info_dict: packed attribute info dictionary to print \"\"\" # Figure out indents max_param_length = max([len(k) for k in info_dict.keys()]) max_type_length = max([v[\"len\"][\"type\"] for v in info_dict.values()]) # Print akin to the argparser for k, v in info_dict.items(): print( f\" {k}\" + (\" \" * (max_param_length - v[\"len\"][\"name\"] + self._max_indent)) + f'{v[\"type\"]}' + (\" \" * (max_type_length - v[\"len\"][\"type\"] + self._max_indent)) + f'{v[\"desc\"]} {v[\"default\"]}' ) # Blank for spacing :-/ print(\"\") def _extract_other_types(self, typed, module_name): \"\"\"Takes a high level type and recursively extracts any enum or class types *Args*: typed: highest level type module_name: name of module to match *Returns*: return_list: list of nums (dot notation of module_path.enum_name or module_path.class_name) \"\"\" return_list = [] if hasattr(typed, \"__args__\"): for val in typed.__args__: recurse_return = self._extract_other_types(val, module_name) if isinstance(recurse_return, list): return_list.extend(recurse_return) else: return_list.append(self._extract_other_types(val, module_name)) elif isinstance(typed, EnumMeta) or (typed.__module__ == module_name): return [f\"{typed.__module__}.{typed.__name__}\"] return return_list def _attrs_help(self, input_classes, module_name): \"\"\"Handles walking through a list classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class. If it finds a repeated class in a iterable object it will recursively call self to handle information *Args*: input_classes: list of attr classes module_name: name of module to match *Returns*: None \"\"\" # Handle the main loop other_list = self._handle_help_main(input_classes, module_name) self._handle_help_enums(other_list=other_list, module_name=module_name) @staticmethod def _get_type_string(val, nested_others): \"\"\"Gets the type of the attr val as a string *Args*: val: current attr being processed nested_others: list of nested others to deal with that might have module path info in the string *Returns*: type_string: type of the attr as a str \"\"\" # Grab the base or type info depending on what is provided if \"type\" in val.metadata: type_string = repr(val.metadata[\"type\"]) elif \"base\" in val.metadata: type_string = val.metadata[\"base\"] elif hasattr(val.type, \"__name__\"): type_string = val.type.__name__ else: type_string = str(val.type) # Regex out the typing info if present type_string = re.sub(r\"typing.\", \"\", type_string) # Regex out any nested_others that have module path information for other_val in nested_others: split_other = f\"{'.'.join(other_val.split('.')[:-1])}.\" type_string = re.sub(split_other, \"\", type_string) # Regex the string to see if it matches any Enums in the __main__ module space # Construct the type with the metadata if \"optional\" in val.metadata: type_string = f\"Optional[{type_string}]\" return type_string def _handle_help_main(self, input_classes, module_name): \"\"\"Handles the print of the main class types *Args*: input_classes: current set of input classes module_name: module name to match *Returns*: other_list: extended list of other classes/enums to process \"\"\" # List to catch Enums and classes and handle post spock wrapped attr classes other_list = [] covered_set = set() for attrs_class in input_classes: # Split the docs into class docs and any attribute docs class_doc, attr_docs = self._split_docs(attrs_class) print(\" \" + attrs_class.__name__ + f\" {class_doc}\") # Keep a running info_dict of all the attribute level info info_dict = {} for val in attrs_class.__attrs_attrs__: # If the type is an enum we need to handle it outside of this attr loop # Match the style of nested enums and return a string of module.name notation if isinstance(val.type, EnumMeta): other_list.append(f\"{val.type.__module__}.{val.type.__name__}\") # if there is a type (implied Iterable) -- check it for nested Enums or classes nested_others = self._extract_fnc(val, module_name) if len(nested_others) > 0: other_list.extend(nested_others) # Get the type represented as a string type_string = self._get_type_string(val, nested_others) info_dict.update( self._match_attribute_docs( val.name, attr_docs, type_string, val.default ) ) # Add to covered so we don't print help twice in the case of some recursive nesting covered_set.add(f\"{attrs_class.__module__}.{attrs_class.__name__}\") self._handle_attributes_print(info_dict=info_dict) # Convert the enum list to a set to remove dupes and then back to a list so it is iterable -- set diff to not # repeat return list(set(other_list) - covered_set) def _handle_help_enums(self, other_list, module_name): \"\"\"handles any extra enums from non main args *Args*: other_list: extended list of other classes/enums to process module_name: module name to match *Returns*: None \"\"\" # Iterate any Enum type classes for other in other_list: # if it's longer than 2 then it's an embedded Spock class if \".\".join(other.split(\".\")[:-1]) == module_name: class_type = self._get_from_sys_modules(other) # Invoke recursive call for the class self._attrs_help([class_type], module_name) # Fall back to enum style else: enum = self._get_from_sys_modules(other) # Split the docs into class docs and any attribute docs class_doc, attr_docs = self._split_docs(enum) print(\" \" + enum.__name__ + f\" ({class_doc})\") info_dict = {} for val in enum: info_dict.update( self._match_attribute_docs( attr_name=val.name, attr_docs=attr_docs, attr_type_str=type(val.value).__name__, ) ) self._handle_attributes_print(info_dict=info_dict) @abstractmethod def _extract_fnc(self, val, module_name): \"\"\"Function that gets the nested lists within classes *Args*: val: current attr module_name: matching module name *Returns*: list of any nested classes/enums \"\"\" @staticmethod def _get_from_sys_modules(cls_name): \"\"\"Gets the class from a dot notation name *Args*: cls_name: dot notation enum name *Returns*: module: enum class \"\"\" # Split on dot notation split_string = cls_name.split(\".\") module = None for idx, val in enumerate(split_string): # idx = 0 will always be a call to the sys.modules dict if idx == 0: module = sys.modules[val] # all other idx are paths along the module that need to be traversed # idx = -1 will always be the final Enum object name we want to grab (final getattr call) else: module = getattr(module, val) return module class AttrBuilder(BaseBuilder): \"\"\"Attr specific builder Class that handles building for the attr backend *Attributes* input_classes: list of input classes that link to a backend _configs: None or List of configs to read from _create_save_path: boolean to make the path to save to _desc: description for the arg parser _no_cmd_line: flag to force no command line reads save_path: list of path(s) to save the configs to \"\"\" def __init__(self, *args, **kwargs): \"\"\"AttrBuilder init Args: *args: list of input classes that link to a backend configs: None or List of configs to read from desc: description for the arg parser no_cmd_line: flag to force no command line reads **kwargs: any extra keyword args \"\"\" super().__init__(*args, module_name=\"spock.backend.config\", **kwargs) @staticmethod def _make_group_override_parser(parser, class_obj, class_name): \"\"\"Makes a name specific override parser for a given class obj Takes a class object of the backend and adds a new argument group with argument names given with name Class.name so that individual parameters specific to a class can be overridden. *Args*: parser: argument parser class_obj: instance of a backend class class_name: used for module matching *Returns*: parser: argument parser with new class specific overrides \"\"\" attr_name = class_obj.__name__ group_parser = parser.add_argument_group( title=str(attr_name) + \" Specific Overrides\" ) for val in class_obj.__attrs_attrs__: val_type = val.metadata[\"type\"] if \"type\" in val.metadata else val.type # Check if the val type has __args__ -- this catches lists? # TODO (ncilfone): Fix up this super super ugly logic if ( hasattr(val_type, \"__args__\") and ((list(set(val_type.__args__))[0]).__module__ == class_name) and attr.has((list(set(val_type.__args__))[0])) ): args = list(set(val_type.__args__))[0] for inner_val in args.__attrs_attrs__: arg_name = f\"--{str(attr_name)}.{val.name}.{args.__name__}.{inner_val.name}\" group_parser = make_argument( arg_name, List[inner_val.type], group_parser ) # If it's a reference to a class it needs to be an arg of a simple string as class matching will take care # of it later on elif val_type.__module__ == \"spock.backend.config\": arg_name = f\"--{str(attr_name)}.{val.name}\" val_type = str group_parser = make_argument(arg_name, val_type, group_parser) else: arg_name = f\"--{str(attr_name)}.{val.name}\" group_parser = make_argument(arg_name, val_type, group_parser) return parser def _handle_arguments(self, args, class_obj): attr_name = class_obj.__name__ class_names = [val.__name__ for val in self.input_classes] # Handle repeated classes if ( attr_name in class_names and attr_name in args and isinstance(args[attr_name], list) ): fields = self._handle_repeated(args[attr_name], attr_name, class_names) # Handle non-repeated classes else: fields = {} for val in class_obj.__attrs_attrs__: # Check if namespace is named and then check for key -- checking for local class def if attr_name in args and val.name in args[attr_name]: fields[val.name] = self._handle_nested_class( args, args[attr_name][val.name], class_names ) # If not named then just check for keys -- checking for global def elif val.name in args: fields[val.name] = self._handle_nested_class( args, args[val.name], class_names ) # Check for special keys to set if ( \"special_key\" in val.metadata and val.metadata[\"special_key\"] is not None ): if val.name in args: self.save_path = args[val.name] elif val.default is not None: self.save_path = val.default return fields def _handle_repeated(self, args, check_value, class_names): \"\"\"Handles repeated classes as lists *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: list of input_class[match)idx[0]] types filled with repeated values \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] return [self.input_classes[match_idx[0]](**val) for val in args] def _handle_nested_class(self, args, check_value, class_names): \"\"\"Handles passing another class to the field dictionary *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: either the check_value or the necessary class \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] # If so then create the needed class object by unrolling the args to **kwargs and return it if len(match_idx) > 0: if len(match_idx) > 1: raise ValueError( \"Match error -- multiple classes with the same name definition\" ) else: if (args.get(self.input_classes[match_idx[0]].__name__) is None) and ( check_value not in class_names ): raise ValueError( f\"Cannot map a definition for the referenced class \" f\"{self.input_classes[match_idx[0]].__name__}\" ) current_arg = args.get(self.input_classes[match_idx[0]].__name__, {}) if isinstance(current_arg, list): class_value = [ self.input_classes[match_idx[0]](**val) for val in current_arg ] else: recurse_args = ( self._handle_recursive_defaults( args.get(check_value), args, class_names ) if check_value in args else {} ) class_value = self.input_classes[match_idx[0]](**recurse_args) return_value = class_value # else return the expected value else: return_value = check_value return return_value def _extract_fnc(self, val, module_name): \"\"\"Function that gets the nested lists within classes *Args*: val: current attr module_name: matching module name *Returns*: list of any nested classes/enums \"\"\" return ( self._extract_other_types(val.metadata[\"type\"], module_name) if \"type\" in val.metadata else [] )","title":"Module spock.backend.builder"},{"location":"reference/spock/backend/builder/#classes","text":"","title":"Classes"},{"location":"reference/spock/backend/builder/#attrbuilder","text":"class AttrBuilder ( * args , ** kwargs ) ??? example \"View Source\" class AttrBuilder(BaseBuilder): \"\"\"Attr specific builder Class that handles building for the attr backend *Attributes* input_classes: list of input classes that link to a backend _configs: None or List of configs to read from _create_save_path: boolean to make the path to save to _desc: description for the arg parser _no_cmd_line: flag to force no command line reads save_path: list of path(s) to save the configs to \"\"\" def __init__(self, *args, **kwargs): \"\"\"AttrBuilder init Args: *args: list of input classes that link to a backend configs: None or List of configs to read from desc: description for the arg parser no_cmd_line: flag to force no command line reads **kwargs: any extra keyword args \"\"\" super().__init__(*args, module_name=\"spock.backend.config\", **kwargs) @staticmethod def _make_group_override_parser(parser, class_obj, class_name): \"\"\"Makes a name specific override parser for a given class obj Takes a class object of the backend and adds a new argument group with argument names given with name Class.name so that individual parameters specific to a class can be overridden. *Args*: parser: argument parser class_obj: instance of a backend class class_name: used for module matching *Returns*: parser: argument parser with new class specific overrides \"\"\" attr_name = class_obj.__name__ group_parser = parser.add_argument_group( title=str(attr_name) + \" Specific Overrides\" ) for val in class_obj.__attrs_attrs__: val_type = val.metadata[\"type\"] if \"type\" in val.metadata else val.type # Check if the val type has __args__ -- this catches lists? # TODO (ncilfone): Fix up this super super ugly logic if ( hasattr(val_type, \"__args__\") and ((list(set(val_type.__args__))[0]).__module__ == class_name) and attr.has((list(set(val_type.__args__))[0])) ): args = list(set(val_type.__args__))[0] for inner_val in args.__attrs_attrs__: arg_name = f\"--{str(attr_name)}.{val.name}.{args.__name__}.{inner_val.name}\" group_parser = make_argument( arg_name, List[inner_val.type], group_parser ) # If it's a reference to a class it needs to be an arg of a simple string as class matching will take care # of it later on elif val_type.__module__ == \"spock.backend.config\": arg_name = f\"--{str(attr_name)}.{val.name}\" val_type = str group_parser = make_argument(arg_name, val_type, group_parser) else: arg_name = f\"--{str(attr_name)}.{val.name}\" group_parser = make_argument(arg_name, val_type, group_parser) return parser def _handle_arguments(self, args, class_obj): attr_name = class_obj.__name__ class_names = [val.__name__ for val in self.input_classes] # Handle repeated classes if ( attr_name in class_names and attr_name in args and isinstance(args[attr_name], list) ): fields = self._handle_repeated(args[attr_name], attr_name, class_names) # Handle non-repeated classes else: fields = {} for val in class_obj.__attrs_attrs__: # Check if namespace is named and then check for key -- checking for local class def if attr_name in args and val.name in args[attr_name]: fields[val.name] = self._handle_nested_class( args, args[attr_name][val.name], class_names ) # If not named then just check for keys -- checking for global def elif val.name in args: fields[val.name] = self._handle_nested_class( args, args[val.name], class_names ) # Check for special keys to set if ( \"special_key\" in val.metadata and val.metadata[\"special_key\"] is not None ): if val.name in args: self.save_path = args[val.name] elif val.default is not None: self.save_path = val.default return fields def _handle_repeated(self, args, check_value, class_names): \"\"\"Handles repeated classes as lists *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: list of input_class[match)idx[0]] types filled with repeated values \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] return [self.input_classes[match_idx[0]](**val) for val in args] def _handle_nested_class(self, args, check_value, class_names): \"\"\"Handles passing another class to the field dictionary *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: either the check_value or the necessary class \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] # If so then create the needed class object by unrolling the args to **kwargs and return it if len(match_idx) > 0: if len(match_idx) > 1: raise ValueError( \"Match error -- multiple classes with the same name definition\" ) else: if (args.get(self.input_classes[match_idx[0]].__name__) is None) and ( check_value not in class_names ): raise ValueError( f\"Cannot map a definition for the referenced class \" f\"{self.input_classes[match_idx[0]].__name__}\" ) current_arg = args.get(self.input_classes[match_idx[0]].__name__, {}) if isinstance(current_arg, list): class_value = [ self.input_classes[match_idx[0]](**val) for val in current_arg ] else: recurse_args = ( self._handle_recursive_defaults( args.get(check_value), args, class_names ) if check_value in args else {} ) class_value = self.input_classes[match_idx[0]](**recurse_args) return_value = class_value # else return the expected value else: return_value = check_value return return_value def _extract_fnc(self, val, module_name): \"\"\"Function that gets the nested lists within classes *Args*: val: current attr module_name: matching module name *Returns*: list of any nested classes/enums \"\"\" return ( self._extract_other_types(val.metadata[\"type\"], module_name) if \"type\" in val.metadata else [] )","title":"AttrBuilder"},{"location":"reference/spock/backend/builder/#ancestors-in-mro","text":"spock.backend.builder.BaseBuilder abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/backend/builder/#methods","text":"","title":"Methods"},{"location":"reference/spock/backend/builder/#build_override_parsers","text":"def build_override_parsers ( self , parser ) Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers Args : parser: argument parser Returns : parser: argument parser with new class specific overrides ??? example \"View Source\" def build_override_parsers(self, parser): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers *Args*: parser: argument parser *Returns*: parser: argument parser with new class specific overrides \"\"\" # Build out each class override specific parser for val in self.input_classes: parser = self._make_group_override_parser( parser=parser, class_obj=val, class_name=self._module_name ) return parser","title":"build_override_parsers"},{"location":"reference/spock/backend/builder/#generate","text":"def generate ( self , dict_args ) Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values Args : dict_args: dictionary of arguments from the configs Returns : namespace containing automatically generated instances of the classes ??? example \"View Source\" def generate(self, dict_args): \"\"\"Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values *Args*: dict_args: dictionary of arguments from the configs *Returns*: namespace containing automatically generated instances of the classes \"\"\" auto_dict = {} for attr_classes in self.input_classes: attr_build = self._auto_generate(dict_args, attr_classes) if isinstance(attr_build, list): class_name = list({type(val).__name__ for val in attr_build}) if len(class_name) > 1: raise ValueError(\"Repeated class has more than one unique name\") auto_dict.update({class_name[0]: attr_build}) else: auto_dict.update({type(attr_build).__name__: attr_build}) return Spockspace(**auto_dict)","title":"generate"},{"location":"reference/spock/backend/builder/#handle_help_info","text":"def handle_help_info ( self ) Handles walking through classes to get help info For each class this function will search doc and attempt to pull out help information for both the class itself and each attribute within the class Returns : None ??? example \"View Source\" def handle_help_info(self): \"\"\"Handles walking through classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class *Returns*: None \"\"\" self._attrs_help(self.input_classes, self._module_name)","title":"handle_help_info"},{"location":"reference/spock/backend/builder/#basebuilder","text":"class BaseBuilder ( * args , max_indent = 4 , module_name , ** kwargs ) ??? example \"View Source\" class BaseBuilder(ABC): # pylint: disable=too-few-public-methods \"\"\"Base class for building the backend specific builders This class handles the interface to the backend with the generic ConfigArgBuilder so that different backends can be used to handle processing *Attributes* input_classes: list of input classes that link to a backend _configs: None or List of configs to read from _desc: description for the arg parser _no_cmd_line: flag to force no command line reads _max_indent: maximum to indent between help prints save_path: list of path(s) to save the configs to \"\"\" def __init__(self, *args, max_indent=4, module_name, **kwargs): self.input_classes = args self._module_name = module_name self._max_indent = max_indent self.save_path = None @staticmethod @abstractmethod def _make_group_override_parser(parser, class_obj, class_name): \"\"\"Makes a name specific override parser for a given class obj Takes a class object of the backend and adds a new argument group with argument names given with name Class.name so that individual parameters specific to a class can be overridden. *Args*: parser: argument parser class_obj: instance of a backend class class_name: used for module matching *Returns*: parser: argument parser with new class specific overrides \"\"\" def handle_help_info(self): \"\"\"Handles walking through classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class *Returns*: None \"\"\" self._attrs_help(self.input_classes, self._module_name) def _handle_arguments(self, args, class_obj): \"\"\"Handles all argument mapping Creates a dictionary of named parameters that are mapped to the final type of object *Args*: args: read file arguments class_obj: instance of a class obj *Returns*: fields: dictionary of mapped parameters \"\"\" attr_name = class_obj.__name__ class_names = [val.__name__ for val in self.input_classes] # Handle repeated classes if ( attr_name in class_names and attr_name in args and isinstance(args[attr_name], list) ): fields = self._handle_repeated(args[attr_name], attr_name, class_names) # Handle non-repeated classes else: fields = {} for val in class_obj.__attrs_attrs__: # Check if namespace is named and then check for key -- checking for local class def if attr_name in args and val.name in args[attr_name]: fields[val.name] = self._handle_nested_class( args, args[attr_name][val.name], class_names ) # If not named then just check for keys -- checking for global def elif val.name in args: fields[val.name] = self._handle_nested_class( args, args[val.name], class_names ) # Check for special keys to set if ( \"special_key\" in val.metadata and val.metadata[\"special_key\"] is not None ): if val.name in args: self.save_path = args[val.name] elif val.default is not None: self.save_path = val.default return fields def _handle_repeated(self, args, check_value, class_names): \"\"\"Handles repeated classes as lists *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: list of input_class[match)idx[0]] types filled with repeated values \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] return [self.input_classes[match_idx[0]](**val) for val in args] def _handle_nested_class(self, args, check_value, class_names): \"\"\"Handles passing another class to the field dictionary *Args*: args: dictionary of arguments from the configs check_value: value to check classes against class_names: current class names *Returns*: either the check_value or the necessary class \"\"\" # Check to see if the value trying to be set is actually an input class match_idx = [idx for idx, val in enumerate(class_names) if val == check_value] # If so then create the needed class object by unrolling the args to **kwargs and return it if len(match_idx) > 0: if len(match_idx) > 1: raise ValueError( \"Match error -- multiple classes with the same name definition\" ) else: if args.get(self.input_classes[match_idx[0]].__name__) is None: raise ValueError( f\"Missing config file definition for the referenced class \" f\"{self.input_classes[match_idx[0]].__name__}\" ) current_arg = args.get(self.input_classes[match_idx[0]].__name__) if isinstance(current_arg, list): class_value = [ self.input_classes[match_idx[0]](**val) for val in current_arg ] else: class_value = self.input_classes[match_idx[0]](**current_arg) return_value = class_value # else return the expected value else: return_value = check_value return return_value def generate(self, dict_args): \"\"\"Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values *Args*: dict_args: dictionary of arguments from the configs *Returns*: namespace containing automatically generated instances of the classes \"\"\" auto_dict = {} for attr_classes in self.input_classes: attr_build = self._auto_generate(dict_args, attr_classes) if isinstance(attr_build, list): class_name = list({type(val).__name__ for val in attr_build}) if len(class_name) > 1: raise ValueError(\"Repeated class has more than one unique name\") auto_dict.update({class_name[0]: attr_build}) else: auto_dict.update({type(attr_build).__name__: attr_build}) return Spockspace(**auto_dict) def _auto_generate(self, args, input_class): \"\"\"Builds an instance of an attr class Builds an instance with the necessary field values from the argument dictionary read from the config file(s) *Args*: args: dictionary of arguments read from the config file(s) data_class: data class to build *Returns*: An instance of data_class with correct values assigned to fields \"\"\" # Handle the basic data types fields = self._handle_arguments(args, input_class) if isinstance(fields, list): return_value = fields else: self._handle_late_defaults(args, fields, input_class) return_value = input_class(**fields) return return_value def _handle_late_defaults(self, args, fields, input_class): \"\"\"Handles late defaults when the type is non-standard If the default type is not a base python type then we need to catch those defaults here and build the correct values from the input classes while maintaining the optional nature. The trick is to exclude all 'base' types as these defaults are covered by the attr default value *Args*: args: dictionary of arguments read from the config file(s) fields: current fields returned from _handle_arguments input_class: which input class being checked for late defaults *Returns*: fields: updated field dictionary with late defaults set \"\"\" names = [val.name for val in input_class.__attrs_attrs__] class_names = [val.__name__ for val in self.input_classes] field_list = list(fields.keys()) arg_list = list(args.keys()) # Exclude all the base types that are supported -- these can be set by attrs exclude_list = [ \"_Nothing\", \"NoneType\", \"bool\", \"int\", \"float\", \"str\", \"list\", \"tuple\", ] for val in names: if val not in field_list: # Gets the name of the class to default to default_type_name = type( getattr(input_class.__attrs_attrs__, val).default ).__name__ if default_type_name not in exclude_list: # Gets the default class object default_attr = getattr(input_class.__attrs_attrs__, val).default # If the default is given for a class then it's the actual class and not a type -- logic needs # to deal with both if type(default_attr).__name__ == \"type\": default_name = default_attr.__name__ else: default_name = type(default_attr).__name__ # Skip if in the exclude list else: default_name = None # if we need to fall back onto the default and ff it's in the arg_list then we have a # definition coming in from the config file if default_name is not None and default_name in arg_list: # This handles lists of class type repeats -- these cannot be nested as the logic would be too # confusing to map to if isinstance(args.get(default_name), list): default_value = [ self.input_classes[class_names.index(default_name)]( **arg_val ) for arg_val in args.get(default_name) ] # This handles basics and references to other classes -- here we need to recurse to grab any nested # defs since classes are passed as strings to the config but are defined via Enums (handled #139) else: recurse_args = self._handle_recursive_defaults( args.get(default_name), args, class_names ) default_value = self.input_classes[ class_names.index(default_name) ](**recurse_args) fields.update({val: default_value}) return fields def _handle_recursive_defaults(self, curr_arg, all_args, class_names): \"\"\"Recurses through the args from the config read to determine if it can map to a definition *Args*: curr_arg: current argument all_args: all argument dictionary class_names: list of class names *Returns*: out_dict: recursively mapped dictionary of attributes \"\"\" out_dict = {} for k, v in curr_arg.items(): # If the value is a reference to another class we need to recurse if v in class_names: # Recurse only if in the all_args dict (from the config file) if v in all_args: bubbled_dict = self._handle_recursive_defaults( all_args.get(v), all_args, class_names ) out_dict.update( {k: self.input_classes[class_names.index(v)](**bubbled_dict)} ) # Else fall back on default instantiation else: out_dict.update({k: self.input_classes[class_names.index(v)]()}) else: out_dict.update({k: v}) return out_dict def build_override_parsers(self, parser): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers *Args*: parser: argument parser *Returns*: parser: argument parser with new class specific overrides \"\"\" # Build out each class override specific parser for val in self.input_classes: parser = self._make_group_override_parser( parser=parser, class_obj=val, class_name=self._module_name ) return parser @staticmethod def _get_from_kwargs(args, configs): \"\"\"Get configs from the configs kwarg *Args*: args: argument namespace configs: config kwarg *Returns*: args: arg namespace \"\"\" if isinstance(configs, list): args.config.extend(configs) else: raise TypeError( f\"configs kwarg must be of type list -- given {type(configs)}\" ) return args @staticmethod def _find_attribute_idx(newline_split_docs): \"\"\"Finds the possible split between the header and Attribute annotations *Args*: newline_split_docs: new line split text Returns: idx: -1 if none or the idx of Attributes \"\"\" for idx, val in enumerate(newline_split_docs): re_check = re.search(r\"(?i)Attribute?s?:\", val) if re_check is not None: return idx return -1 def _split_docs(self, obj): \"\"\"Possibly splits head class doc string from attribute docstrings Attempts to find the first contiguous line within the Google style docstring to use as the class docstring. Splits the docs base on the Attributes tag if present. *Args*: obj: class object to rip info from *Returns*: class_doc: class docstring if present or blank str attr_doc: list of attribute doc strings \"\"\" if obj.__doc__ is not None: # Split by new line newline_split_docs = obj.__doc__.split(\"\\n\") # Cleanup l/t whitespace newline_split_docs = [val.strip() for val in newline_split_docs] else: newline_split_docs = [] # Find the break between the class docs and the Attribute section -- if this returns -1 then there is no # Attributes section attr_idx = self._find_attribute_idx(newline_split_docs) head_docs = ( newline_split_docs[:attr_idx] if attr_idx != -1 else newline_split_docs ) attr_docs = newline_split_docs[attr_idx:] if attr_idx != -1 else [] # Grab only the first contiguous line as everything else will probably be too verbose (e.g. the # mid-level docstring that has detailed descriptions class_doc = \"\" for idx, val in enumerate(head_docs): class_doc += f\" {val}\" if idx + 1 != len(head_docs) and head_docs[idx + 1] == \"\": break # Clean up any l/t whitespace class_doc = class_doc.strip() if len(class_doc) > 0: class_doc = f\"-- {class_doc}\" return class_doc, attr_docs @staticmethod def _match_attribute_docs( attr_name, attr_docs, attr_type_str, attr_default=NOTHING ): \"\"\"Matches class attributes with attribute docstrings via regex *Args*: attr_name: attribute name attr_docs: list of attribute docstrings attr_type_str: str representation of the attribute type attr_default: str representation of a possible default value *Returns*: dictionary of packed attribute information \"\"\" # Regex match each value a_str = None for a_doc in attr_docs: match_re = re.search(r\"(?i)^\" + attr_name + \"?:\", a_doc) # Find only the first match -- if more than one than ignore if match_re: a_str = a_doc[match_re.end() :].strip() return { attr_name: { \"type\": attr_type_str, \"desc\": a_str if a_str is not None else \"\", \"default\": \"(default: \" + repr(attr_default) + \")\" if type(attr_default).__name__ != \"_Nothing\" else \"\", \"len\": {\"name\": len(attr_name), \"type\": len(attr_type_str)}, } } def _handle_attributes_print(self, info_dict): \"\"\"Prints attribute information in an argparser style format *Args*: info_dict: packed attribute info dictionary to print \"\"\" # Figure out indents max_param_length = max([len(k) for k in info_dict.keys()]) max_type_length = max([v[\"len\"][\"type\"] for v in info_dict.values()]) # Print akin to the argparser for k, v in info_dict.items(): print( f\" {k}\" + (\" \" * (max_param_length - v[\"len\"][\"name\"] + self._max_indent)) + f'{v[\"type\"]}' + (\" \" * (max_type_length - v[\"len\"][\"type\"] + self._max_indent)) + f'{v[\"desc\"]} {v[\"default\"]}' ) # Blank for spacing :-/ print(\"\") def _extract_other_types(self, typed, module_name): \"\"\"Takes a high level type and recursively extracts any enum or class types *Args*: typed: highest level type module_name: name of module to match *Returns*: return_list: list of nums (dot notation of module_path.enum_name or module_path.class_name) \"\"\" return_list = [] if hasattr(typed, \"__args__\"): for val in typed.__args__: recurse_return = self._extract_other_types(val, module_name) if isinstance(recurse_return, list): return_list.extend(recurse_return) else: return_list.append(self._extract_other_types(val, module_name)) elif isinstance(typed, EnumMeta) or (typed.__module__ == module_name): return [f\"{typed.__module__}.{typed.__name__}\"] return return_list def _attrs_help(self, input_classes, module_name): \"\"\"Handles walking through a list classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class. If it finds a repeated class in a iterable object it will recursively call self to handle information *Args*: input_classes: list of attr classes module_name: name of module to match *Returns*: None \"\"\" # Handle the main loop other_list = self._handle_help_main(input_classes, module_name) self._handle_help_enums(other_list=other_list, module_name=module_name) @staticmethod def _get_type_string(val, nested_others): \"\"\"Gets the type of the attr val as a string *Args*: val: current attr being processed nested_others: list of nested others to deal with that might have module path info in the string *Returns*: type_string: type of the attr as a str \"\"\" # Grab the base or type info depending on what is provided if \"type\" in val.metadata: type_string = repr(val.metadata[\"type\"]) elif \"base\" in val.metadata: type_string = val.metadata[\"base\"] elif hasattr(val.type, \"__name__\"): type_string = val.type.__name__ else: type_string = str(val.type) # Regex out the typing info if present type_string = re.sub(r\"typing.\", \"\", type_string) # Regex out any nested_others that have module path information for other_val in nested_others: split_other = f\"{'.'.join(other_val.split('.')[:-1])}.\" type_string = re.sub(split_other, \"\", type_string) # Regex the string to see if it matches any Enums in the __main__ module space # Construct the type with the metadata if \"optional\" in val.metadata: type_string = f\"Optional[{type_string}]\" return type_string def _handle_help_main(self, input_classes, module_name): \"\"\"Handles the print of the main class types *Args*: input_classes: current set of input classes module_name: module name to match *Returns*: other_list: extended list of other classes/enums to process \"\"\" # List to catch Enums and classes and handle post spock wrapped attr classes other_list = [] covered_set = set() for attrs_class in input_classes: # Split the docs into class docs and any attribute docs class_doc, attr_docs = self._split_docs(attrs_class) print(\" \" + attrs_class.__name__ + f\" {class_doc}\") # Keep a running info_dict of all the attribute level info info_dict = {} for val in attrs_class.__attrs_attrs__: # If the type is an enum we need to handle it outside of this attr loop # Match the style of nested enums and return a string of module.name notation if isinstance(val.type, EnumMeta): other_list.append(f\"{val.type.__module__}.{val.type.__name__}\") # if there is a type (implied Iterable) -- check it for nested Enums or classes nested_others = self._extract_fnc(val, module_name) if len(nested_others) > 0: other_list.extend(nested_others) # Get the type represented as a string type_string = self._get_type_string(val, nested_others) info_dict.update( self._match_attribute_docs( val.name, attr_docs, type_string, val.default ) ) # Add to covered so we don't print help twice in the case of some recursive nesting covered_set.add(f\"{attrs_class.__module__}.{attrs_class.__name__}\") self._handle_attributes_print(info_dict=info_dict) # Convert the enum list to a set to remove dupes and then back to a list so it is iterable -- set diff to not # repeat return list(set(other_list) - covered_set) def _handle_help_enums(self, other_list, module_name): \"\"\"handles any extra enums from non main args *Args*: other_list: extended list of other classes/enums to process module_name: module name to match *Returns*: None \"\"\" # Iterate any Enum type classes for other in other_list: # if it's longer than 2 then it's an embedded Spock class if \".\".join(other.split(\".\")[:-1]) == module_name: class_type = self._get_from_sys_modules(other) # Invoke recursive call for the class self._attrs_help([class_type], module_name) # Fall back to enum style else: enum = self._get_from_sys_modules(other) # Split the docs into class docs and any attribute docs class_doc, attr_docs = self._split_docs(enum) print(\" \" + enum.__name__ + f\" ({class_doc})\") info_dict = {} for val in enum: info_dict.update( self._match_attribute_docs( attr_name=val.name, attr_docs=attr_docs, attr_type_str=type(val.value).__name__, ) ) self._handle_attributes_print(info_dict=info_dict) @abstractmethod def _extract_fnc(self, val, module_name): \"\"\"Function that gets the nested lists within classes *Args*: val: current attr module_name: matching module name *Returns*: list of any nested classes/enums \"\"\" @staticmethod def _get_from_sys_modules(cls_name): \"\"\"Gets the class from a dot notation name *Args*: cls_name: dot notation enum name *Returns*: module: enum class \"\"\" # Split on dot notation split_string = cls_name.split(\".\") module = None for idx, val in enumerate(split_string): # idx = 0 will always be a call to the sys.modules dict if idx == 0: module = sys.modules[val] # all other idx are paths along the module that need to be traversed # idx = -1 will always be the final Enum object name we want to grab (final getattr call) else: module = getattr(module, val) return module","title":"BaseBuilder"},{"location":"reference/spock/backend/builder/#ancestors-in-mro_1","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/backend/builder/#descendants","text":"spock.backend.builder.AttrBuilder spock.addons.tune.builder.TunerBuilder","title":"Descendants"},{"location":"reference/spock/backend/builder/#methods_1","text":"","title":"Methods"},{"location":"reference/spock/backend/builder/#build_override_parsers_1","text":"def build_override_parsers ( self , parser ) Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers Args : parser: argument parser Returns : parser: argument parser with new class specific overrides ??? example \"View Source\" def build_override_parsers(self, parser): \"\"\"Creates parsers for command-line overrides Builds the basic command line parser for configs and help then iterates through each attr instance to make namespace specific cmd line override parsers *Args*: parser: argument parser *Returns*: parser: argument parser with new class specific overrides \"\"\" # Build out each class override specific parser for val in self.input_classes: parser = self._make_group_override_parser( parser=parser, class_obj=val, class_name=self._module_name ) return parser","title":"build_override_parsers"},{"location":"reference/spock/backend/builder/#generate_1","text":"def generate ( self , dict_args ) Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values Args : dict_args: dictionary of arguments from the configs Returns : namespace containing automatically generated instances of the classes ??? example \"View Source\" def generate(self, dict_args): \"\"\"Method to auto-generate the actual class instances from the generated args Based on the generated arguments groups and the args read in from the config file(s) this function instantiates the classes with the necessary field or attr values *Args*: dict_args: dictionary of arguments from the configs *Returns*: namespace containing automatically generated instances of the classes \"\"\" auto_dict = {} for attr_classes in self.input_classes: attr_build = self._auto_generate(dict_args, attr_classes) if isinstance(attr_build, list): class_name = list({type(val).__name__ for val in attr_build}) if len(class_name) > 1: raise ValueError(\"Repeated class has more than one unique name\") auto_dict.update({class_name[0]: attr_build}) else: auto_dict.update({type(attr_build).__name__: attr_build}) return Spockspace(**auto_dict)","title":"generate"},{"location":"reference/spock/backend/builder/#handle_help_info_1","text":"def handle_help_info ( self ) Handles walking through classes to get help info For each class this function will search doc and attempt to pull out help information for both the class itself and each attribute within the class Returns : None ??? example \"View Source\" def handle_help_info(self): \"\"\"Handles walking through classes to get help info For each class this function will search __doc__ and attempt to pull out help information for both the class itself and each attribute within the class *Returns*: None \"\"\" self._attrs_help(self.input_classes, self._module_name)","title":"handle_help_info"},{"location":"reference/spock/backend/config/","text":"Module spock.backend.config Creates the spock config interface that wraps attr None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Creates the spock config interface that wraps attr\"\"\" import sys import attr from spock.backend.typed import katra def _base_attr(cls): \"\"\"Map type hints to katras Connector function that maps type hinting style to the defined katra style which uses the more strict attr.ib() definition *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" # Since we are not using the @attr.s decorator we need to get the parent classes for inheritance # We do this by using the mro and grabbing anything that is not the first and last indices in the list and wrapping # it into a tuple if len(cls.__mro__[1:-1]) > 0: bases = tuple(cls.__mro__[1:-1]) # if there are not parents pass a blank tuple else: bases = () # Make a blank attrs dict for new attrs attrs_dict = {} if hasattr(cls, \"__annotations__\"): for k, v in cls.__annotations__.items(): # If the cls has the attribute then a default was set if hasattr(cls, k): default = getattr(cls, k) else: default = None attrs_dict.update({k: katra(typed=v, default=default)}) return bases, attrs_dict def spock_attr(cls): \"\"\"Map type hints to katras Connector function that maps type hinting style to the defined katra style which uses the more strict attr.ib() definition *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" bases, attrs_dict = _base_attr(cls) # Dynamically make an attr class obj = attr.make_class( name=cls.__name__, bases=bases, attrs=attrs_dict, kw_only=True, frozen=True ) # For each class we dynamically create we need to register it within the system modules for pickle to work setattr(sys.modules[\"spock\"].backend.config, obj.__name__, obj) # Swap the __doc__ string from cls to obj obj.__doc__ = cls.__doc__ return obj Functions spock_attr def spock_attr ( cls ) Map type hints to katras Connector function that maps type hinting style to the defined katra style which uses the more strict attr.ib() definition Args : cls: basic class def Returns : cls: slotted attrs class that is frozen and kw only ??? example \"View Source\" def spock_attr(cls): \"\"\"Map type hints to katras Connector function that maps type hinting style to the defined katra style which uses the more strict attr.ib() definition *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" bases, attrs_dict = _base_attr(cls) # Dynamically make an attr class obj = attr.make_class( name=cls.__name__, bases=bases, attrs=attrs_dict, kw_only=True, frozen=True ) # For each class we dynamically create we need to register it within the system modules for pickle to work setattr(sys.modules[\"spock\"].backend.config, obj.__name__, obj) # Swap the __doc__ string from cls to obj obj.__doc__ = cls.__doc__ return obj","title":"Config"},{"location":"reference/spock/backend/config/#module-spockbackendconfig","text":"Creates the spock config interface that wraps attr None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Creates the spock config interface that wraps attr\"\"\" import sys import attr from spock.backend.typed import katra def _base_attr(cls): \"\"\"Map type hints to katras Connector function that maps type hinting style to the defined katra style which uses the more strict attr.ib() definition *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" # Since we are not using the @attr.s decorator we need to get the parent classes for inheritance # We do this by using the mro and grabbing anything that is not the first and last indices in the list and wrapping # it into a tuple if len(cls.__mro__[1:-1]) > 0: bases = tuple(cls.__mro__[1:-1]) # if there are not parents pass a blank tuple else: bases = () # Make a blank attrs dict for new attrs attrs_dict = {} if hasattr(cls, \"__annotations__\"): for k, v in cls.__annotations__.items(): # If the cls has the attribute then a default was set if hasattr(cls, k): default = getattr(cls, k) else: default = None attrs_dict.update({k: katra(typed=v, default=default)}) return bases, attrs_dict def spock_attr(cls): \"\"\"Map type hints to katras Connector function that maps type hinting style to the defined katra style which uses the more strict attr.ib() definition *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" bases, attrs_dict = _base_attr(cls) # Dynamically make an attr class obj = attr.make_class( name=cls.__name__, bases=bases, attrs=attrs_dict, kw_only=True, frozen=True ) # For each class we dynamically create we need to register it within the system modules for pickle to work setattr(sys.modules[\"spock\"].backend.config, obj.__name__, obj) # Swap the __doc__ string from cls to obj obj.__doc__ = cls.__doc__ return obj","title":"Module spock.backend.config"},{"location":"reference/spock/backend/config/#functions","text":"","title":"Functions"},{"location":"reference/spock/backend/config/#spock_attr","text":"def spock_attr ( cls ) Map type hints to katras Connector function that maps type hinting style to the defined katra style which uses the more strict attr.ib() definition Args : cls: basic class def Returns : cls: slotted attrs class that is frozen and kw only ??? example \"View Source\" def spock_attr(cls): \"\"\"Map type hints to katras Connector function that maps type hinting style to the defined katra style which uses the more strict attr.ib() definition *Args*: cls: basic class def *Returns*: cls: slotted attrs class that is frozen and kw only \"\"\" bases, attrs_dict = _base_attr(cls) # Dynamically make an attr class obj = attr.make_class( name=cls.__name__, bases=bases, attrs=attrs_dict, kw_only=True, frozen=True ) # For each class we dynamically create we need to register it within the system modules for pickle to work setattr(sys.modules[\"spock\"].backend.config, obj.__name__, obj) # Swap the __doc__ string from cls to obj obj.__doc__ = cls.__doc__ return obj","title":"spock_attr"},{"location":"reference/spock/backend/handler/","text":"Module spock.backend.handler Base handler Spock class None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Base handler Spock class\"\"\" from abc import ABC from spock.handlers import JSONHandler, TOMLHandler, YAMLHandler class BaseHandler(ABC): \"\"\"Base class for saver and payload *Attributes*: _writers: maps file extension to the correct i/o handler _s3_config: optional S3Config object to handle s3 access \"\"\" def __init__(self, s3_config=None): self._supported_extensions = { \".yaml\": YAMLHandler, \".toml\": TOMLHandler, \".json\": JSONHandler, } self._s3_config = s3_config def _check_extension(self, file_extension: str): if file_extension not in self._supported_extensions: raise TypeError( f\"File extension {file_extension} not supported -- \\n\" f\"File extension must be from {list(self._supported_extensions.keys())}\" ) Classes BaseHandler class BaseHandler ( s3_config = None ) ??? example \"View Source\" class BaseHandler(ABC): \"\"\"Base class for saver and payload *Attributes*: _writers: maps file extension to the correct i/o handler _s3_config: optional S3Config object to handle s3 access \"\"\" def __init__(self, s3_config=None): self._supported_extensions = { \".yaml\": YAMLHandler, \".toml\": TOMLHandler, \".json\": JSONHandler, } self._s3_config = s3_config def _check_extension(self, file_extension: str): if file_extension not in self._supported_extensions: raise TypeError( f\"File extension {file_extension} not supported -- \\n\" f\"File extension must be from {list(self._supported_extensions.keys())}\" ) Ancestors (in MRO) abc.ABC Descendants spock.backend.payload.BasePayload spock.backend.saver.BaseSaver","title":"Handler"},{"location":"reference/spock/backend/handler/#module-spockbackendhandler","text":"Base handler Spock class None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Base handler Spock class\"\"\" from abc import ABC from spock.handlers import JSONHandler, TOMLHandler, YAMLHandler class BaseHandler(ABC): \"\"\"Base class for saver and payload *Attributes*: _writers: maps file extension to the correct i/o handler _s3_config: optional S3Config object to handle s3 access \"\"\" def __init__(self, s3_config=None): self._supported_extensions = { \".yaml\": YAMLHandler, \".toml\": TOMLHandler, \".json\": JSONHandler, } self._s3_config = s3_config def _check_extension(self, file_extension: str): if file_extension not in self._supported_extensions: raise TypeError( f\"File extension {file_extension} not supported -- \\n\" f\"File extension must be from {list(self._supported_extensions.keys())}\" )","title":"Module spock.backend.handler"},{"location":"reference/spock/backend/handler/#classes","text":"","title":"Classes"},{"location":"reference/spock/backend/handler/#basehandler","text":"class BaseHandler ( s3_config = None ) ??? example \"View Source\" class BaseHandler(ABC): \"\"\"Base class for saver and payload *Attributes*: _writers: maps file extension to the correct i/o handler _s3_config: optional S3Config object to handle s3 access \"\"\" def __init__(self, s3_config=None): self._supported_extensions = { \".yaml\": YAMLHandler, \".toml\": TOMLHandler, \".json\": JSONHandler, } self._s3_config = s3_config def _check_extension(self, file_extension: str): if file_extension not in self._supported_extensions: raise TypeError( f\"File extension {file_extension} not supported -- \\n\" f\"File extension must be from {list(self._supported_extensions.keys())}\" )","title":"BaseHandler"},{"location":"reference/spock/backend/handler/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/backend/handler/#descendants","text":"spock.backend.payload.BasePayload spock.backend.saver.BaseSaver","title":"Descendants"},{"location":"reference/spock/backend/payload/","text":"Module spock.backend.payload Handles payloads from markup files None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles payloads from markup files\"\"\" import os import sys from abc import abstractmethod from itertools import chain from pathlib import Path from spock.backend.handler import BaseHandler from spock.backend.utils import ( convert_to_tuples, deep_update, get_attr_fields, get_type_fields, ) from spock.utils import check_path_s3 class BasePayload(BaseHandler): # pylint: disable=too-few-public-methods \"\"\"Handles building the payload for config file(s) This class builds out the payload from config files of multiple types. It handles various file types and also composition of config files via recursive calls *Attributes*: _loaders: maps of each file extension to the loader class __s3_config: optional S3Config object to handle s3 access \"\"\" def __init__(self, s3_config=None): super(BasePayload, self).__init__(s3_config=s3_config) @staticmethod @abstractmethod def _update_payload(base_payload, input_classes, ignore_classes, payload): \"\"\"Updates the payload Checks the parameters defined in the config files against the provided classes and if passable adds them to the payload *Args*: base_payload: current payload input_classes: class to roll into ignore_classes: list of classes to ignore payload: total payload *Returns*: payload: updated payload \"\"\" def payload(self, input_classes, ignore_classes, path, cmd_args, deps): \"\"\"Builds the payload from config files Public exposed call to build the payload and set any command line overrides *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" payload = self._payload(input_classes, ignore_classes, path, deps, root=True) payload = self._handle_overrides(payload, ignore_classes, cmd_args) return payload def _payload(self, input_classes, ignore_classes, path, deps, root=False): \"\"\"Private call to construct the payload Main function call that builds out the payload from config files of multiple types. It handles various file types and also composition of config files via a recursive calls *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" # empty payload payload = {} if path is not None: # Match to loader based on file-extension config_extension = Path(path).suffix.lower() # Verify extension self._check_extension(file_extension=config_extension) # Load from file base_payload = self._supported_extensions.get(config_extension)().load( path, s3_config=self._s3_config ) base_payload = {} if base_payload is None else base_payload # Check and? update the dependencies deps = self._handle_dependencies(deps, path, root) if \"config\" in base_payload: payload = self._handle_includes( base_payload, config_extension, input_classes, ignore_classes, path, payload, deps, ) payload = self._update_payload( base_payload, input_classes, ignore_classes, payload ) return payload @staticmethod def _handle_dependencies(deps, path, root): \"\"\"Handles config file dependencies Checks to see if the config path (full or relative) has already been encountered. Essentially a DFS for graph cycles *Args*: deps: dictionary of config dependencies path: current config path root: boolean if root *Returns*: deps: updated dependencies \"\"\" if root and path in deps.get(\"paths\"): raise ValueError( f\"Duplicate Read -- Config file {path} has already been encountered. \" f\"Please remove duplicate reads of config files.\" ) elif path in deps.get(\"paths\") or path in deps.get(\"rel_paths\"): raise ValueError( f\"Cyclical Dependency -- Config file {path} has already been encountered. \" f\"Please remove cyclical dependencies between config files.\" ) else: # Update the dependency lists deps.get(\"paths\").append(path) deps.get(\"rel_paths\").append(os.path.basename(path)) if root: deps.get(\"roots\").append(path) return deps def _handle_includes( self, base_payload, config_extension, input_classes, ignore_classes, path, payload, deps, ): # pylint: disable=too-many-arguments \"\"\"Handles config composition For all of the config tags in the config file this function will recursively call the payload function with the composition path to get the additional payload(s) from the composed file(s) -- checks for file validity or if it is an S3 URI via regex *Args*: base_payload: base payload that has a config kwarg config_extension: file type input_classes: defined backend classes ignore_classes: list of classes to ignore path: path to base file payload: payload pulled from composed files deps: dictionary of config dependencies *Returns*: payload: payload update from composed files \"\"\" included_params = {} for inc_path in base_payload[\"config\"]: if check_path_s3(inc_path): use_path = inc_path elif os.path.exists(inc_path): use_path = inc_path elif os.path.join(os.path.dirname(path), inc_path): use_path = os.path.join(os.path.dirname(path), inc_path) else: raise RuntimeError( f\"Could not find included {config_extension} file {inc_path} or is not an S3 URI!\" ) included_params.update( self._payload(input_classes, ignore_classes, use_path, deps) ) payload.update(included_params) return payload def _handle_overrides(self, payload, ignore_classes, args): \"\"\"Handle command line overrides Iterate through the command line override values, determine at what level to set them, and set them if possible *Args*: payload: current payload dictionary args: command line override args *Returns*: payload: updated payload dictionary with override values set \"\"\" skip_keys = [\"config\", \"help\"] pruned_args = self._prune_args(args, ignore_classes) for k, v in pruned_args.items(): if k not in skip_keys and v is not None: payload = self._handle_payload_override(payload, k, v) return payload @staticmethod def _prune_args(args, ignore_classes): \"\"\"Prunes ignored class names from the cmd line args list to prevent incorrect access *Args*: args: current cmd line args ignore_classes: list of class names to ignore *Returns*: dictionary of pruned cmd line args \"\"\" ignored_stems = [val.__name__ for val in ignore_classes] return { k: v for k, v in vars(args).items() if k.split(\".\")[0] not in ignored_stems } @staticmethod @abstractmethod def _handle_payload_override(payload, key, value): \"\"\"Handles the complex logic needed for List[spock class] overrides Messy logic that sets overrides for the various different types. The hardest being List[spock class] since str names have to be mapped backed to sys.modules and can be set at either the general or class level. *Args*: payload: current payload dictionary key: current arg key value: value at current arg key *Returns*: payload: modified payload with overrides \"\"\" class AttrPayload(BasePayload): \"\"\"Handles building the payload for attrs backend This class builds out the payload from config files of multiple types. It handles various file types and also composition of config files via a recursive calls *Attributes*: _loaders: maps of each file extension to the loader class \"\"\" def __init__(self, s3_config=None): \"\"\"Init for AttrPayload *Args*: s3_config: optional S3 config object \"\"\" super().__init__(s3_config=s3_config) def __call__(self, *args, **kwargs): \"\"\"Call to allow self chaining *Args*: *args: **kwargs: *Returns*: Payload: instance of self \"\"\" return AttrPayload(*args, **kwargs) @staticmethod def _update_payload(base_payload, input_classes, ignore_classes, payload): # Get basic args attr_fields = get_attr_fields(input_classes=input_classes) # Get the ignore fields ignore_fields = get_attr_fields(input_classes=ignore_classes) # Class names class_names = [val.__name__ for val in input_classes] # Parse out the types if generic type_fields = get_type_fields(input_classes) for keys, values in base_payload.items(): if keys not in ignore_fields: # check if the keys, value pair is expected by the attr class if keys != \"config\": # Dict infers that we are overriding a global setting in a specific config if isinstance(values, dict): # we're in a namespace # Check for incorrect specific override of global def if keys not in attr_fields: raise TypeError( f\"Referring to a class space {keys} that is undefined\" ) for i_keys in values.keys(): if i_keys not in attr_fields[keys]: raise ValueError( f\"Provided an unknown argument named {keys}.{i_keys}\" ) else: # Check if the key is actually a reference to another class if keys in class_names: if isinstance(values, list): # Check for incorrect specific override of global def if keys not in attr_fields: raise ValueError( f\"Referring to a class space {keys} that is undefined\" ) # We are in a repeated class def # Raise if the key set is different from the defined set (i.e. incorrect arguments) key_set = set( list(chain(*[list(val.keys()) for val in values])) ) for i_keys in key_set: if i_keys not in attr_fields[keys]: raise ValueError( f\"Provided an unknown argument named {keys}.{i_keys}\" ) # Chain all the values from multiple spock classes into one list elif keys not in list(chain(*attr_fields.values())): raise ValueError( f\"Provided an unknown argument named {keys}\" ) # Chain all the values from multiple spock classes into one list elif keys not in list(chain(*attr_fields.values())): raise ValueError( f\"Provided an unknown argument named {keys}\" ) if keys in payload and isinstance(values, dict): payload[keys].update(values) else: payload[keys] = values tuple_payload = convert_to_tuples(payload, type_fields, class_names) payload = deep_update(payload, tuple_payload) return payload @staticmethod def _handle_payload_override(payload, key, value): \"\"\"Handles the complex logic needed for List[spock class] overrides Messy logic that sets overrides for the various different types. The hardest being List[spock class] since str names have to be mapped backed to sys.modules and can be set at either the general or class level. *Args*: payload: current payload dictionary key: current arg key value: value at current arg key *Returns*: payload: modified payload with overrides \"\"\" key_split = key.split(\".\") curr_ref = payload # Handle non existing parts of the payload for specific cases root_classes = [ idx for idx, val in enumerate(key_split) if hasattr(sys.modules[\"spock\"].backend.config, val) ] # Verify any classes have roots in the payload dict for idx in root_classes: # Update all root classes if not present if key_split[idx] not in payload: payload.update({key_split[idx]: {}}) # If not updating the root then it is a reference to another class which might not be in the payload # Make sure it's there by setting it -- since this is an override setting is fine as these should be the # final say in the param values so don't worry about clashing if idx != 0: payload[key_split[0]][key_split[idx - 1]] = key_split[idx] # Check also for repeated classes -- value will be a list when the type is not var = getattr( getattr( sys.modules[\"spock\"].backend.config, key_split[idx] ).__attrs_attrs__, key_split[-1], ) if isinstance(value, list) and var.type != list: # If the dict is blank we need to handle the creation of the list of dicts if len(payload[key_split[idx]]) == 0: payload.update( { key_split[idx]: [ {key_split[-1]: None} for _ in range(len(value)) ] } ) # If it's already partially filled we need to update not overwrite else: for val in payload[key_split[idx]]: val.update({key_split[-1]: None}) for idx, split in enumerate(key_split): # Check for curr_ref switch over -- verify by checking the sys modules names if ( idx != 0 and (split in payload) and (isinstance(curr_ref, str)) and (hasattr(sys.modules[\"spock\"].backend.config, split)) ): curr_ref = payload[split] # Look ahead to check if the next value exists in the dictionary elif ( idx != 0 and (split in payload) and (isinstance(payload[split], str)) and (hasattr(sys.modules[\"spock\"].backend.config, payload[split])) ): curr_ref = payload[split] # elif check if it's the last value and figure out the override elif idx == (len(key_split) - 1): # Handle bool(s) a bit differently as they are store_true if isinstance(curr_ref, dict) and isinstance(value, bool): if value is not False: curr_ref[split] = value # If we are at the dictionary level we should be able to just payload override elif isinstance(curr_ref, dict) and not isinstance(value, bool): curr_ref[split] = value # If we are at a list level it must be some form of repeated class since this is the end of the class # tree -- check the instance type but also make sure the cmd-line override is the correct len elif isinstance(curr_ref, list) and len(value) == len(curr_ref): # Walk the list and check for the key for ref_idx, val in enumerate(curr_ref): if split in val: val[split] = value[ref_idx] else: raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Failed to find key {split} within lowest level List[Dict]\" ) elif isinstance(curr_ref, list) and len(value) != len(curr_ref): raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Specified key {split} with len {len(value)} does not match len {len(curr_ref)} \" f\"of List[Dict]\" ) else: raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Failed to find key {split} within lowest level Dict\" ) # If it's not keep walking the current payload else: curr_ref = curr_ref[split] return payload Classes AttrPayload class AttrPayload ( s3_config = None ) ??? example \"View Source\" class AttrPayload(BasePayload): \"\"\"Handles building the payload for attrs backend This class builds out the payload from config files of multiple types. It handles various file types and also composition of config files via a recursive calls *Attributes*: _loaders: maps of each file extension to the loader class \"\"\" def __init__(self, s3_config=None): \"\"\"Init for AttrPayload *Args*: s3_config: optional S3 config object \"\"\" super().__init__(s3_config=s3_config) def __call__(self, *args, **kwargs): \"\"\"Call to allow self chaining *Args*: *args: **kwargs: *Returns*: Payload: instance of self \"\"\" return AttrPayload(*args, **kwargs) @staticmethod def _update_payload(base_payload, input_classes, ignore_classes, payload): # Get basic args attr_fields = get_attr_fields(input_classes=input_classes) # Get the ignore fields ignore_fields = get_attr_fields(input_classes=ignore_classes) # Class names class_names = [val.__name__ for val in input_classes] # Parse out the types if generic type_fields = get_type_fields(input_classes) for keys, values in base_payload.items(): if keys not in ignore_fields: # check if the keys, value pair is expected by the attr class if keys != \"config\": # Dict infers that we are overriding a global setting in a specific config if isinstance(values, dict): # we're in a namespace # Check for incorrect specific override of global def if keys not in attr_fields: raise TypeError( f\"Referring to a class space {keys} that is undefined\" ) for i_keys in values.keys(): if i_keys not in attr_fields[keys]: raise ValueError( f\"Provided an unknown argument named {keys}.{i_keys}\" ) else: # Check if the key is actually a reference to another class if keys in class_names: if isinstance(values, list): # Check for incorrect specific override of global def if keys not in attr_fields: raise ValueError( f\"Referring to a class space {keys} that is undefined\" ) # We are in a repeated class def # Raise if the key set is different from the defined set (i.e. incorrect arguments) key_set = set( list(chain(*[list(val.keys()) for val in values])) ) for i_keys in key_set: if i_keys not in attr_fields[keys]: raise ValueError( f\"Provided an unknown argument named {keys}.{i_keys}\" ) # Chain all the values from multiple spock classes into one list elif keys not in list(chain(*attr_fields.values())): raise ValueError( f\"Provided an unknown argument named {keys}\" ) # Chain all the values from multiple spock classes into one list elif keys not in list(chain(*attr_fields.values())): raise ValueError( f\"Provided an unknown argument named {keys}\" ) if keys in payload and isinstance(values, dict): payload[keys].update(values) else: payload[keys] = values tuple_payload = convert_to_tuples(payload, type_fields, class_names) payload = deep_update(payload, tuple_payload) return payload @staticmethod def _handle_payload_override(payload, key, value): \"\"\"Handles the complex logic needed for List[spock class] overrides Messy logic that sets overrides for the various different types. The hardest being List[spock class] since str names have to be mapped backed to sys.modules and can be set at either the general or class level. *Args*: payload: current payload dictionary key: current arg key value: value at current arg key *Returns*: payload: modified payload with overrides \"\"\" key_split = key.split(\".\") curr_ref = payload # Handle non existing parts of the payload for specific cases root_classes = [ idx for idx, val in enumerate(key_split) if hasattr(sys.modules[\"spock\"].backend.config, val) ] # Verify any classes have roots in the payload dict for idx in root_classes: # Update all root classes if not present if key_split[idx] not in payload: payload.update({key_split[idx]: {}}) # If not updating the root then it is a reference to another class which might not be in the payload # Make sure it's there by setting it -- since this is an override setting is fine as these should be the # final say in the param values so don't worry about clashing if idx != 0: payload[key_split[0]][key_split[idx - 1]] = key_split[idx] # Check also for repeated classes -- value will be a list when the type is not var = getattr( getattr( sys.modules[\"spock\"].backend.config, key_split[idx] ).__attrs_attrs__, key_split[-1], ) if isinstance(value, list) and var.type != list: # If the dict is blank we need to handle the creation of the list of dicts if len(payload[key_split[idx]]) == 0: payload.update( { key_split[idx]: [ {key_split[-1]: None} for _ in range(len(value)) ] } ) # If it's already partially filled we need to update not overwrite else: for val in payload[key_split[idx]]: val.update({key_split[-1]: None}) for idx, split in enumerate(key_split): # Check for curr_ref switch over -- verify by checking the sys modules names if ( idx != 0 and (split in payload) and (isinstance(curr_ref, str)) and (hasattr(sys.modules[\"spock\"].backend.config, split)) ): curr_ref = payload[split] # Look ahead to check if the next value exists in the dictionary elif ( idx != 0 and (split in payload) and (isinstance(payload[split], str)) and (hasattr(sys.modules[\"spock\"].backend.config, payload[split])) ): curr_ref = payload[split] # elif check if it's the last value and figure out the override elif idx == (len(key_split) - 1): # Handle bool(s) a bit differently as they are store_true if isinstance(curr_ref, dict) and isinstance(value, bool): if value is not False: curr_ref[split] = value # If we are at the dictionary level we should be able to just payload override elif isinstance(curr_ref, dict) and not isinstance(value, bool): curr_ref[split] = value # If we are at a list level it must be some form of repeated class since this is the end of the class # tree -- check the instance type but also make sure the cmd-line override is the correct len elif isinstance(curr_ref, list) and len(value) == len(curr_ref): # Walk the list and check for the key for ref_idx, val in enumerate(curr_ref): if split in val: val[split] = value[ref_idx] else: raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Failed to find key {split} within lowest level List[Dict]\" ) elif isinstance(curr_ref, list) and len(value) != len(curr_ref): raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Specified key {split} with len {len(value)} does not match len {len(curr_ref)} \" f\"of List[Dict]\" ) else: raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Failed to find key {split} within lowest level Dict\" ) # If it's not keep walking the current payload else: curr_ref = curr_ref[split] return payload Ancestors (in MRO) spock.backend.payload.BasePayload spock.backend.handler.BaseHandler abc.ABC Methods payload def payload ( self , input_classes , ignore_classes , path , cmd_args , deps ) Builds the payload from config files Public exposed call to build the payload and set any command line overrides Args : input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies Returns : payload: dictionary of all mapped parameters ??? example \"View Source\" def payload(self, input_classes, ignore_classes, path, cmd_args, deps): \"\"\"Builds the payload from config files Public exposed call to build the payload and set any command line overrides *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" payload = self._payload(input_classes, ignore_classes, path, deps, root=True) payload = self._handle_overrides(payload, ignore_classes, cmd_args) return payload BasePayload class BasePayload ( s3_config = None ) ??? example \"View Source\" class BasePayload(BaseHandler): # pylint: disable=too-few-public-methods \"\"\"Handles building the payload for config file(s) This class builds out the payload from config files of multiple types. It handles various file types and also composition of config files via recursive calls *Attributes*: _loaders: maps of each file extension to the loader class __s3_config: optional S3Config object to handle s3 access \"\"\" def __init__(self, s3_config=None): super(BasePayload, self).__init__(s3_config=s3_config) @staticmethod @abstractmethod def _update_payload(base_payload, input_classes, ignore_classes, payload): \"\"\"Updates the payload Checks the parameters defined in the config files against the provided classes and if passable adds them to the payload *Args*: base_payload: current payload input_classes: class to roll into ignore_classes: list of classes to ignore payload: total payload *Returns*: payload: updated payload \"\"\" def payload(self, input_classes, ignore_classes, path, cmd_args, deps): \"\"\"Builds the payload from config files Public exposed call to build the payload and set any command line overrides *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" payload = self._payload(input_classes, ignore_classes, path, deps, root=True) payload = self._handle_overrides(payload, ignore_classes, cmd_args) return payload def _payload(self, input_classes, ignore_classes, path, deps, root=False): \"\"\"Private call to construct the payload Main function call that builds out the payload from config files of multiple types. It handles various file types and also composition of config files via a recursive calls *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" # empty payload payload = {} if path is not None: # Match to loader based on file-extension config_extension = Path(path).suffix.lower() # Verify extension self._check_extension(file_extension=config_extension) # Load from file base_payload = self._supported_extensions.get(config_extension)().load( path, s3_config=self._s3_config ) base_payload = {} if base_payload is None else base_payload # Check and? update the dependencies deps = self._handle_dependencies(deps, path, root) if \"config\" in base_payload: payload = self._handle_includes( base_payload, config_extension, input_classes, ignore_classes, path, payload, deps, ) payload = self._update_payload( base_payload, input_classes, ignore_classes, payload ) return payload @staticmethod def _handle_dependencies(deps, path, root): \"\"\"Handles config file dependencies Checks to see if the config path (full or relative) has already been encountered. Essentially a DFS for graph cycles *Args*: deps: dictionary of config dependencies path: current config path root: boolean if root *Returns*: deps: updated dependencies \"\"\" if root and path in deps.get(\"paths\"): raise ValueError( f\"Duplicate Read -- Config file {path} has already been encountered. \" f\"Please remove duplicate reads of config files.\" ) elif path in deps.get(\"paths\") or path in deps.get(\"rel_paths\"): raise ValueError( f\"Cyclical Dependency -- Config file {path} has already been encountered. \" f\"Please remove cyclical dependencies between config files.\" ) else: # Update the dependency lists deps.get(\"paths\").append(path) deps.get(\"rel_paths\").append(os.path.basename(path)) if root: deps.get(\"roots\").append(path) return deps def _handle_includes( self, base_payload, config_extension, input_classes, ignore_classes, path, payload, deps, ): # pylint: disable=too-many-arguments \"\"\"Handles config composition For all of the config tags in the config file this function will recursively call the payload function with the composition path to get the additional payload(s) from the composed file(s) -- checks for file validity or if it is an S3 URI via regex *Args*: base_payload: base payload that has a config kwarg config_extension: file type input_classes: defined backend classes ignore_classes: list of classes to ignore path: path to base file payload: payload pulled from composed files deps: dictionary of config dependencies *Returns*: payload: payload update from composed files \"\"\" included_params = {} for inc_path in base_payload[\"config\"]: if check_path_s3(inc_path): use_path = inc_path elif os.path.exists(inc_path): use_path = inc_path elif os.path.join(os.path.dirname(path), inc_path): use_path = os.path.join(os.path.dirname(path), inc_path) else: raise RuntimeError( f\"Could not find included {config_extension} file {inc_path} or is not an S3 URI!\" ) included_params.update( self._payload(input_classes, ignore_classes, use_path, deps) ) payload.update(included_params) return payload def _handle_overrides(self, payload, ignore_classes, args): \"\"\"Handle command line overrides Iterate through the command line override values, determine at what level to set them, and set them if possible *Args*: payload: current payload dictionary args: command line override args *Returns*: payload: updated payload dictionary with override values set \"\"\" skip_keys = [\"config\", \"help\"] pruned_args = self._prune_args(args, ignore_classes) for k, v in pruned_args.items(): if k not in skip_keys and v is not None: payload = self._handle_payload_override(payload, k, v) return payload @staticmethod def _prune_args(args, ignore_classes): \"\"\"Prunes ignored class names from the cmd line args list to prevent incorrect access *Args*: args: current cmd line args ignore_classes: list of class names to ignore *Returns*: dictionary of pruned cmd line args \"\"\" ignored_stems = [val.__name__ for val in ignore_classes] return { k: v for k, v in vars(args).items() if k.split(\".\")[0] not in ignored_stems } @staticmethod @abstractmethod def _handle_payload_override(payload, key, value): \"\"\"Handles the complex logic needed for List[spock class] overrides Messy logic that sets overrides for the various different types. The hardest being List[spock class] since str names have to be mapped backed to sys.modules and can be set at either the general or class level. *Args*: payload: current payload dictionary key: current arg key value: value at current arg key *Returns*: payload: modified payload with overrides \"\"\" Ancestors (in MRO) spock.backend.handler.BaseHandler abc.ABC Descendants spock.backend.payload.AttrPayload spock.addons.tune.payload.TunerPayload Methods payload def payload ( self , input_classes , ignore_classes , path , cmd_args , deps ) Builds the payload from config files Public exposed call to build the payload and set any command line overrides Args : input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies Returns : payload: dictionary of all mapped parameters ??? example \"View Source\" def payload(self, input_classes, ignore_classes, path, cmd_args, deps): \"\"\"Builds the payload from config files Public exposed call to build the payload and set any command line overrides *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" payload = self._payload(input_classes, ignore_classes, path, deps, root=True) payload = self._handle_overrides(payload, ignore_classes, cmd_args) return payload","title":"Payload"},{"location":"reference/spock/backend/payload/#module-spockbackendpayload","text":"Handles payloads from markup files None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles payloads from markup files\"\"\" import os import sys from abc import abstractmethod from itertools import chain from pathlib import Path from spock.backend.handler import BaseHandler from spock.backend.utils import ( convert_to_tuples, deep_update, get_attr_fields, get_type_fields, ) from spock.utils import check_path_s3 class BasePayload(BaseHandler): # pylint: disable=too-few-public-methods \"\"\"Handles building the payload for config file(s) This class builds out the payload from config files of multiple types. It handles various file types and also composition of config files via recursive calls *Attributes*: _loaders: maps of each file extension to the loader class __s3_config: optional S3Config object to handle s3 access \"\"\" def __init__(self, s3_config=None): super(BasePayload, self).__init__(s3_config=s3_config) @staticmethod @abstractmethod def _update_payload(base_payload, input_classes, ignore_classes, payload): \"\"\"Updates the payload Checks the parameters defined in the config files against the provided classes and if passable adds them to the payload *Args*: base_payload: current payload input_classes: class to roll into ignore_classes: list of classes to ignore payload: total payload *Returns*: payload: updated payload \"\"\" def payload(self, input_classes, ignore_classes, path, cmd_args, deps): \"\"\"Builds the payload from config files Public exposed call to build the payload and set any command line overrides *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" payload = self._payload(input_classes, ignore_classes, path, deps, root=True) payload = self._handle_overrides(payload, ignore_classes, cmd_args) return payload def _payload(self, input_classes, ignore_classes, path, deps, root=False): \"\"\"Private call to construct the payload Main function call that builds out the payload from config files of multiple types. It handles various file types and also composition of config files via a recursive calls *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" # empty payload payload = {} if path is not None: # Match to loader based on file-extension config_extension = Path(path).suffix.lower() # Verify extension self._check_extension(file_extension=config_extension) # Load from file base_payload = self._supported_extensions.get(config_extension)().load( path, s3_config=self._s3_config ) base_payload = {} if base_payload is None else base_payload # Check and? update the dependencies deps = self._handle_dependencies(deps, path, root) if \"config\" in base_payload: payload = self._handle_includes( base_payload, config_extension, input_classes, ignore_classes, path, payload, deps, ) payload = self._update_payload( base_payload, input_classes, ignore_classes, payload ) return payload @staticmethod def _handle_dependencies(deps, path, root): \"\"\"Handles config file dependencies Checks to see if the config path (full or relative) has already been encountered. Essentially a DFS for graph cycles *Args*: deps: dictionary of config dependencies path: current config path root: boolean if root *Returns*: deps: updated dependencies \"\"\" if root and path in deps.get(\"paths\"): raise ValueError( f\"Duplicate Read -- Config file {path} has already been encountered. \" f\"Please remove duplicate reads of config files.\" ) elif path in deps.get(\"paths\") or path in deps.get(\"rel_paths\"): raise ValueError( f\"Cyclical Dependency -- Config file {path} has already been encountered. \" f\"Please remove cyclical dependencies between config files.\" ) else: # Update the dependency lists deps.get(\"paths\").append(path) deps.get(\"rel_paths\").append(os.path.basename(path)) if root: deps.get(\"roots\").append(path) return deps def _handle_includes( self, base_payload, config_extension, input_classes, ignore_classes, path, payload, deps, ): # pylint: disable=too-many-arguments \"\"\"Handles config composition For all of the config tags in the config file this function will recursively call the payload function with the composition path to get the additional payload(s) from the composed file(s) -- checks for file validity or if it is an S3 URI via regex *Args*: base_payload: base payload that has a config kwarg config_extension: file type input_classes: defined backend classes ignore_classes: list of classes to ignore path: path to base file payload: payload pulled from composed files deps: dictionary of config dependencies *Returns*: payload: payload update from composed files \"\"\" included_params = {} for inc_path in base_payload[\"config\"]: if check_path_s3(inc_path): use_path = inc_path elif os.path.exists(inc_path): use_path = inc_path elif os.path.join(os.path.dirname(path), inc_path): use_path = os.path.join(os.path.dirname(path), inc_path) else: raise RuntimeError( f\"Could not find included {config_extension} file {inc_path} or is not an S3 URI!\" ) included_params.update( self._payload(input_classes, ignore_classes, use_path, deps) ) payload.update(included_params) return payload def _handle_overrides(self, payload, ignore_classes, args): \"\"\"Handle command line overrides Iterate through the command line override values, determine at what level to set them, and set them if possible *Args*: payload: current payload dictionary args: command line override args *Returns*: payload: updated payload dictionary with override values set \"\"\" skip_keys = [\"config\", \"help\"] pruned_args = self._prune_args(args, ignore_classes) for k, v in pruned_args.items(): if k not in skip_keys and v is not None: payload = self._handle_payload_override(payload, k, v) return payload @staticmethod def _prune_args(args, ignore_classes): \"\"\"Prunes ignored class names from the cmd line args list to prevent incorrect access *Args*: args: current cmd line args ignore_classes: list of class names to ignore *Returns*: dictionary of pruned cmd line args \"\"\" ignored_stems = [val.__name__ for val in ignore_classes] return { k: v for k, v in vars(args).items() if k.split(\".\")[0] not in ignored_stems } @staticmethod @abstractmethod def _handle_payload_override(payload, key, value): \"\"\"Handles the complex logic needed for List[spock class] overrides Messy logic that sets overrides for the various different types. The hardest being List[spock class] since str names have to be mapped backed to sys.modules and can be set at either the general or class level. *Args*: payload: current payload dictionary key: current arg key value: value at current arg key *Returns*: payload: modified payload with overrides \"\"\" class AttrPayload(BasePayload): \"\"\"Handles building the payload for attrs backend This class builds out the payload from config files of multiple types. It handles various file types and also composition of config files via a recursive calls *Attributes*: _loaders: maps of each file extension to the loader class \"\"\" def __init__(self, s3_config=None): \"\"\"Init for AttrPayload *Args*: s3_config: optional S3 config object \"\"\" super().__init__(s3_config=s3_config) def __call__(self, *args, **kwargs): \"\"\"Call to allow self chaining *Args*: *args: **kwargs: *Returns*: Payload: instance of self \"\"\" return AttrPayload(*args, **kwargs) @staticmethod def _update_payload(base_payload, input_classes, ignore_classes, payload): # Get basic args attr_fields = get_attr_fields(input_classes=input_classes) # Get the ignore fields ignore_fields = get_attr_fields(input_classes=ignore_classes) # Class names class_names = [val.__name__ for val in input_classes] # Parse out the types if generic type_fields = get_type_fields(input_classes) for keys, values in base_payload.items(): if keys not in ignore_fields: # check if the keys, value pair is expected by the attr class if keys != \"config\": # Dict infers that we are overriding a global setting in a specific config if isinstance(values, dict): # we're in a namespace # Check for incorrect specific override of global def if keys not in attr_fields: raise TypeError( f\"Referring to a class space {keys} that is undefined\" ) for i_keys in values.keys(): if i_keys not in attr_fields[keys]: raise ValueError( f\"Provided an unknown argument named {keys}.{i_keys}\" ) else: # Check if the key is actually a reference to another class if keys in class_names: if isinstance(values, list): # Check for incorrect specific override of global def if keys not in attr_fields: raise ValueError( f\"Referring to a class space {keys} that is undefined\" ) # We are in a repeated class def # Raise if the key set is different from the defined set (i.e. incorrect arguments) key_set = set( list(chain(*[list(val.keys()) for val in values])) ) for i_keys in key_set: if i_keys not in attr_fields[keys]: raise ValueError( f\"Provided an unknown argument named {keys}.{i_keys}\" ) # Chain all the values from multiple spock classes into one list elif keys not in list(chain(*attr_fields.values())): raise ValueError( f\"Provided an unknown argument named {keys}\" ) # Chain all the values from multiple spock classes into one list elif keys not in list(chain(*attr_fields.values())): raise ValueError( f\"Provided an unknown argument named {keys}\" ) if keys in payload and isinstance(values, dict): payload[keys].update(values) else: payload[keys] = values tuple_payload = convert_to_tuples(payload, type_fields, class_names) payload = deep_update(payload, tuple_payload) return payload @staticmethod def _handle_payload_override(payload, key, value): \"\"\"Handles the complex logic needed for List[spock class] overrides Messy logic that sets overrides for the various different types. The hardest being List[spock class] since str names have to be mapped backed to sys.modules and can be set at either the general or class level. *Args*: payload: current payload dictionary key: current arg key value: value at current arg key *Returns*: payload: modified payload with overrides \"\"\" key_split = key.split(\".\") curr_ref = payload # Handle non existing parts of the payload for specific cases root_classes = [ idx for idx, val in enumerate(key_split) if hasattr(sys.modules[\"spock\"].backend.config, val) ] # Verify any classes have roots in the payload dict for idx in root_classes: # Update all root classes if not present if key_split[idx] not in payload: payload.update({key_split[idx]: {}}) # If not updating the root then it is a reference to another class which might not be in the payload # Make sure it's there by setting it -- since this is an override setting is fine as these should be the # final say in the param values so don't worry about clashing if idx != 0: payload[key_split[0]][key_split[idx - 1]] = key_split[idx] # Check also for repeated classes -- value will be a list when the type is not var = getattr( getattr( sys.modules[\"spock\"].backend.config, key_split[idx] ).__attrs_attrs__, key_split[-1], ) if isinstance(value, list) and var.type != list: # If the dict is blank we need to handle the creation of the list of dicts if len(payload[key_split[idx]]) == 0: payload.update( { key_split[idx]: [ {key_split[-1]: None} for _ in range(len(value)) ] } ) # If it's already partially filled we need to update not overwrite else: for val in payload[key_split[idx]]: val.update({key_split[-1]: None}) for idx, split in enumerate(key_split): # Check for curr_ref switch over -- verify by checking the sys modules names if ( idx != 0 and (split in payload) and (isinstance(curr_ref, str)) and (hasattr(sys.modules[\"spock\"].backend.config, split)) ): curr_ref = payload[split] # Look ahead to check if the next value exists in the dictionary elif ( idx != 0 and (split in payload) and (isinstance(payload[split], str)) and (hasattr(sys.modules[\"spock\"].backend.config, payload[split])) ): curr_ref = payload[split] # elif check if it's the last value and figure out the override elif idx == (len(key_split) - 1): # Handle bool(s) a bit differently as they are store_true if isinstance(curr_ref, dict) and isinstance(value, bool): if value is not False: curr_ref[split] = value # If we are at the dictionary level we should be able to just payload override elif isinstance(curr_ref, dict) and not isinstance(value, bool): curr_ref[split] = value # If we are at a list level it must be some form of repeated class since this is the end of the class # tree -- check the instance type but also make sure the cmd-line override is the correct len elif isinstance(curr_ref, list) and len(value) == len(curr_ref): # Walk the list and check for the key for ref_idx, val in enumerate(curr_ref): if split in val: val[split] = value[ref_idx] else: raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Failed to find key {split} within lowest level List[Dict]\" ) elif isinstance(curr_ref, list) and len(value) != len(curr_ref): raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Specified key {split} with len {len(value)} does not match len {len(curr_ref)} \" f\"of List[Dict]\" ) else: raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Failed to find key {split} within lowest level Dict\" ) # If it's not keep walking the current payload else: curr_ref = curr_ref[split] return payload","title":"Module spock.backend.payload"},{"location":"reference/spock/backend/payload/#classes","text":"","title":"Classes"},{"location":"reference/spock/backend/payload/#attrpayload","text":"class AttrPayload ( s3_config = None ) ??? example \"View Source\" class AttrPayload(BasePayload): \"\"\"Handles building the payload for attrs backend This class builds out the payload from config files of multiple types. It handles various file types and also composition of config files via a recursive calls *Attributes*: _loaders: maps of each file extension to the loader class \"\"\" def __init__(self, s3_config=None): \"\"\"Init for AttrPayload *Args*: s3_config: optional S3 config object \"\"\" super().__init__(s3_config=s3_config) def __call__(self, *args, **kwargs): \"\"\"Call to allow self chaining *Args*: *args: **kwargs: *Returns*: Payload: instance of self \"\"\" return AttrPayload(*args, **kwargs) @staticmethod def _update_payload(base_payload, input_classes, ignore_classes, payload): # Get basic args attr_fields = get_attr_fields(input_classes=input_classes) # Get the ignore fields ignore_fields = get_attr_fields(input_classes=ignore_classes) # Class names class_names = [val.__name__ for val in input_classes] # Parse out the types if generic type_fields = get_type_fields(input_classes) for keys, values in base_payload.items(): if keys not in ignore_fields: # check if the keys, value pair is expected by the attr class if keys != \"config\": # Dict infers that we are overriding a global setting in a specific config if isinstance(values, dict): # we're in a namespace # Check for incorrect specific override of global def if keys not in attr_fields: raise TypeError( f\"Referring to a class space {keys} that is undefined\" ) for i_keys in values.keys(): if i_keys not in attr_fields[keys]: raise ValueError( f\"Provided an unknown argument named {keys}.{i_keys}\" ) else: # Check if the key is actually a reference to another class if keys in class_names: if isinstance(values, list): # Check for incorrect specific override of global def if keys not in attr_fields: raise ValueError( f\"Referring to a class space {keys} that is undefined\" ) # We are in a repeated class def # Raise if the key set is different from the defined set (i.e. incorrect arguments) key_set = set( list(chain(*[list(val.keys()) for val in values])) ) for i_keys in key_set: if i_keys not in attr_fields[keys]: raise ValueError( f\"Provided an unknown argument named {keys}.{i_keys}\" ) # Chain all the values from multiple spock classes into one list elif keys not in list(chain(*attr_fields.values())): raise ValueError( f\"Provided an unknown argument named {keys}\" ) # Chain all the values from multiple spock classes into one list elif keys not in list(chain(*attr_fields.values())): raise ValueError( f\"Provided an unknown argument named {keys}\" ) if keys in payload and isinstance(values, dict): payload[keys].update(values) else: payload[keys] = values tuple_payload = convert_to_tuples(payload, type_fields, class_names) payload = deep_update(payload, tuple_payload) return payload @staticmethod def _handle_payload_override(payload, key, value): \"\"\"Handles the complex logic needed for List[spock class] overrides Messy logic that sets overrides for the various different types. The hardest being List[spock class] since str names have to be mapped backed to sys.modules and can be set at either the general or class level. *Args*: payload: current payload dictionary key: current arg key value: value at current arg key *Returns*: payload: modified payload with overrides \"\"\" key_split = key.split(\".\") curr_ref = payload # Handle non existing parts of the payload for specific cases root_classes = [ idx for idx, val in enumerate(key_split) if hasattr(sys.modules[\"spock\"].backend.config, val) ] # Verify any classes have roots in the payload dict for idx in root_classes: # Update all root classes if not present if key_split[idx] not in payload: payload.update({key_split[idx]: {}}) # If not updating the root then it is a reference to another class which might not be in the payload # Make sure it's there by setting it -- since this is an override setting is fine as these should be the # final say in the param values so don't worry about clashing if idx != 0: payload[key_split[0]][key_split[idx - 1]] = key_split[idx] # Check also for repeated classes -- value will be a list when the type is not var = getattr( getattr( sys.modules[\"spock\"].backend.config, key_split[idx] ).__attrs_attrs__, key_split[-1], ) if isinstance(value, list) and var.type != list: # If the dict is blank we need to handle the creation of the list of dicts if len(payload[key_split[idx]]) == 0: payload.update( { key_split[idx]: [ {key_split[-1]: None} for _ in range(len(value)) ] } ) # If it's already partially filled we need to update not overwrite else: for val in payload[key_split[idx]]: val.update({key_split[-1]: None}) for idx, split in enumerate(key_split): # Check for curr_ref switch over -- verify by checking the sys modules names if ( idx != 0 and (split in payload) and (isinstance(curr_ref, str)) and (hasattr(sys.modules[\"spock\"].backend.config, split)) ): curr_ref = payload[split] # Look ahead to check if the next value exists in the dictionary elif ( idx != 0 and (split in payload) and (isinstance(payload[split], str)) and (hasattr(sys.modules[\"spock\"].backend.config, payload[split])) ): curr_ref = payload[split] # elif check if it's the last value and figure out the override elif idx == (len(key_split) - 1): # Handle bool(s) a bit differently as they are store_true if isinstance(curr_ref, dict) and isinstance(value, bool): if value is not False: curr_ref[split] = value # If we are at the dictionary level we should be able to just payload override elif isinstance(curr_ref, dict) and not isinstance(value, bool): curr_ref[split] = value # If we are at a list level it must be some form of repeated class since this is the end of the class # tree -- check the instance type but also make sure the cmd-line override is the correct len elif isinstance(curr_ref, list) and len(value) == len(curr_ref): # Walk the list and check for the key for ref_idx, val in enumerate(curr_ref): if split in val: val[split] = value[ref_idx] else: raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Failed to find key {split} within lowest level List[Dict]\" ) elif isinstance(curr_ref, list) and len(value) != len(curr_ref): raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Specified key {split} with len {len(value)} does not match len {len(curr_ref)} \" f\"of List[Dict]\" ) else: raise ValueError( f\"cmd-line override failed for {key} -- \" f\"Failed to find key {split} within lowest level Dict\" ) # If it's not keep walking the current payload else: curr_ref = curr_ref[split] return payload","title":"AttrPayload"},{"location":"reference/spock/backend/payload/#ancestors-in-mro","text":"spock.backend.payload.BasePayload spock.backend.handler.BaseHandler abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/backend/payload/#methods","text":"","title":"Methods"},{"location":"reference/spock/backend/payload/#payload","text":"def payload ( self , input_classes , ignore_classes , path , cmd_args , deps ) Builds the payload from config files Public exposed call to build the payload and set any command line overrides Args : input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies Returns : payload: dictionary of all mapped parameters ??? example \"View Source\" def payload(self, input_classes, ignore_classes, path, cmd_args, deps): \"\"\"Builds the payload from config files Public exposed call to build the payload and set any command line overrides *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" payload = self._payload(input_classes, ignore_classes, path, deps, root=True) payload = self._handle_overrides(payload, ignore_classes, cmd_args) return payload","title":"payload"},{"location":"reference/spock/backend/payload/#basepayload","text":"class BasePayload ( s3_config = None ) ??? example \"View Source\" class BasePayload(BaseHandler): # pylint: disable=too-few-public-methods \"\"\"Handles building the payload for config file(s) This class builds out the payload from config files of multiple types. It handles various file types and also composition of config files via recursive calls *Attributes*: _loaders: maps of each file extension to the loader class __s3_config: optional S3Config object to handle s3 access \"\"\" def __init__(self, s3_config=None): super(BasePayload, self).__init__(s3_config=s3_config) @staticmethod @abstractmethod def _update_payload(base_payload, input_classes, ignore_classes, payload): \"\"\"Updates the payload Checks the parameters defined in the config files against the provided classes and if passable adds them to the payload *Args*: base_payload: current payload input_classes: class to roll into ignore_classes: list of classes to ignore payload: total payload *Returns*: payload: updated payload \"\"\" def payload(self, input_classes, ignore_classes, path, cmd_args, deps): \"\"\"Builds the payload from config files Public exposed call to build the payload and set any command line overrides *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" payload = self._payload(input_classes, ignore_classes, path, deps, root=True) payload = self._handle_overrides(payload, ignore_classes, cmd_args) return payload def _payload(self, input_classes, ignore_classes, path, deps, root=False): \"\"\"Private call to construct the payload Main function call that builds out the payload from config files of multiple types. It handles various file types and also composition of config files via a recursive calls *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" # empty payload payload = {} if path is not None: # Match to loader based on file-extension config_extension = Path(path).suffix.lower() # Verify extension self._check_extension(file_extension=config_extension) # Load from file base_payload = self._supported_extensions.get(config_extension)().load( path, s3_config=self._s3_config ) base_payload = {} if base_payload is None else base_payload # Check and? update the dependencies deps = self._handle_dependencies(deps, path, root) if \"config\" in base_payload: payload = self._handle_includes( base_payload, config_extension, input_classes, ignore_classes, path, payload, deps, ) payload = self._update_payload( base_payload, input_classes, ignore_classes, payload ) return payload @staticmethod def _handle_dependencies(deps, path, root): \"\"\"Handles config file dependencies Checks to see if the config path (full or relative) has already been encountered. Essentially a DFS for graph cycles *Args*: deps: dictionary of config dependencies path: current config path root: boolean if root *Returns*: deps: updated dependencies \"\"\" if root and path in deps.get(\"paths\"): raise ValueError( f\"Duplicate Read -- Config file {path} has already been encountered. \" f\"Please remove duplicate reads of config files.\" ) elif path in deps.get(\"paths\") or path in deps.get(\"rel_paths\"): raise ValueError( f\"Cyclical Dependency -- Config file {path} has already been encountered. \" f\"Please remove cyclical dependencies between config files.\" ) else: # Update the dependency lists deps.get(\"paths\").append(path) deps.get(\"rel_paths\").append(os.path.basename(path)) if root: deps.get(\"roots\").append(path) return deps def _handle_includes( self, base_payload, config_extension, input_classes, ignore_classes, path, payload, deps, ): # pylint: disable=too-many-arguments \"\"\"Handles config composition For all of the config tags in the config file this function will recursively call the payload function with the composition path to get the additional payload(s) from the composed file(s) -- checks for file validity or if it is an S3 URI via regex *Args*: base_payload: base payload that has a config kwarg config_extension: file type input_classes: defined backend classes ignore_classes: list of classes to ignore path: path to base file payload: payload pulled from composed files deps: dictionary of config dependencies *Returns*: payload: payload update from composed files \"\"\" included_params = {} for inc_path in base_payload[\"config\"]: if check_path_s3(inc_path): use_path = inc_path elif os.path.exists(inc_path): use_path = inc_path elif os.path.join(os.path.dirname(path), inc_path): use_path = os.path.join(os.path.dirname(path), inc_path) else: raise RuntimeError( f\"Could not find included {config_extension} file {inc_path} or is not an S3 URI!\" ) included_params.update( self._payload(input_classes, ignore_classes, use_path, deps) ) payload.update(included_params) return payload def _handle_overrides(self, payload, ignore_classes, args): \"\"\"Handle command line overrides Iterate through the command line override values, determine at what level to set them, and set them if possible *Args*: payload: current payload dictionary args: command line override args *Returns*: payload: updated payload dictionary with override values set \"\"\" skip_keys = [\"config\", \"help\"] pruned_args = self._prune_args(args, ignore_classes) for k, v in pruned_args.items(): if k not in skip_keys and v is not None: payload = self._handle_payload_override(payload, k, v) return payload @staticmethod def _prune_args(args, ignore_classes): \"\"\"Prunes ignored class names from the cmd line args list to prevent incorrect access *Args*: args: current cmd line args ignore_classes: list of class names to ignore *Returns*: dictionary of pruned cmd line args \"\"\" ignored_stems = [val.__name__ for val in ignore_classes] return { k: v for k, v in vars(args).items() if k.split(\".\")[0] not in ignored_stems } @staticmethod @abstractmethod def _handle_payload_override(payload, key, value): \"\"\"Handles the complex logic needed for List[spock class] overrides Messy logic that sets overrides for the various different types. The hardest being List[spock class] since str names have to be mapped backed to sys.modules and can be set at either the general or class level. *Args*: payload: current payload dictionary key: current arg key value: value at current arg key *Returns*: payload: modified payload with overrides \"\"\"","title":"BasePayload"},{"location":"reference/spock/backend/payload/#ancestors-in-mro_1","text":"spock.backend.handler.BaseHandler abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/backend/payload/#descendants","text":"spock.backend.payload.AttrPayload spock.addons.tune.payload.TunerPayload","title":"Descendants"},{"location":"reference/spock/backend/payload/#methods_1","text":"","title":"Methods"},{"location":"reference/spock/backend/payload/#payload_1","text":"def payload ( self , input_classes , ignore_classes , path , cmd_args , deps ) Builds the payload from config files Public exposed call to build the payload and set any command line overrides Args : input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies Returns : payload: dictionary of all mapped parameters ??? example \"View Source\" def payload(self, input_classes, ignore_classes, path, cmd_args, deps): \"\"\"Builds the payload from config files Public exposed call to build the payload and set any command line overrides *Args*: input_classes: list of backend classes ignore_classes: list of classes to ignore path: path to config file(s) cmd_args: command line overrides deps: dictionary of config dependencies *Returns*: payload: dictionary of all mapped parameters \"\"\" payload = self._payload(input_classes, ignore_classes, path, deps, root=True) payload = self._handle_overrides(payload, ignore_classes, cmd_args) return payload","title":"payload"},{"location":"reference/spock/backend/saver/","text":"Module spock.backend.saver Handles prepping and saving the Spock config None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles prepping and saving the Spock config\"\"\" from abc import abstractmethod from uuid import uuid4 import attr from spock.backend.handler import BaseHandler from spock.utils import add_info class BaseSaver(BaseHandler): # pylint: disable=too-few-public-methods \"\"\"Base class for saving configs Contains methods to build a correct output payload and then writes to file based on the file extension *Attributes*: _writers: maps file extension to the correct i/o handler _s3_config: optional S3Config object to handle s3 access \"\"\" def __init__(self, s3_config=None): super(BaseSaver, self).__init__(s3_config=s3_config) def dict_payload(self, payload): \"\"\"Clean up the config payload so it can be returned as a dict representation *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" # Fix up values -- parameters return self._clean_up_values(payload) def save( self, payload, path, file_name=None, create_save_path=False, extra_info=True, file_extension=\".yaml\", tuner_payload=None, fixed_uuid=None, ): # pylint: disable=too-many-arguments \"\"\"Writes Spock config to file Cleans and builds an output payload and then correctly writes it to file based on the specified file extension *Args*: payload: current config payload path: path to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to uuid if None create_save_path: boolean to create the path if non-existent extra_info: boolean to write extra info file_extension: what type of file to write tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite *Returns*: None \"\"\" # Check extension self._check_extension(file_extension=file_extension) # Make the filename -- always append a uuid for unique-ness uuid_str = str(uuid4()) if fixed_uuid is None else fixed_uuid fname = \"\" if file_name is None else f\"{file_name}.\" name = f\"{fname}{uuid_str}.spock.cfg{file_extension}\" # Fix up values -- parameters out_dict = self._clean_up_values(payload) # Fix up the tuner values if present tuner_dict = ( self._clean_tuner_values(tuner_payload) if tuner_payload is not None else None ) if tuner_dict is not None: out_dict.update(tuner_dict) # Get extra info extra_dict = add_info() if extra_info else None try: self._supported_extensions.get(file_extension)().save( out_dict=out_dict, info_dict=extra_dict, path=str(path), name=name, create_path=create_save_path, s3_config=self._s3_config, ) except OSError as e: print(f\"Unable to write to given path: {path / name}\") raise e @abstractmethod def _clean_up_values(self, payload): \"\"\"Clean up the config payload so it can be written to file *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" @abstractmethod def _clean_tuner_values(self, payload): \"\"\"Cleans up the base tuner payload that is not sampled *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" def _clean_output(self, out_dict): \"\"\"Clean up the dictionary so it can be written to file *Args*: out_dict: cleaned dictionary extra_info: boolean to add extra info *Returns*: clean_dict: cleaned output payload \"\"\" # Convert values clean_dict = {} for key, val in out_dict.items(): clean_inner_dict = {} if isinstance(val, list): for idx, list_val in enumerate(val): tmp_dict = {} for inner_key, inner_val in list_val.items(): tmp_dict = self._convert(tmp_dict, inner_val, inner_key) val[idx] = tmp_dict clean_inner_dict = val else: for inner_key, inner_val in val.items(): clean_inner_dict = self._convert( clean_inner_dict, inner_val, inner_key ) clean_dict.update({key: clean_inner_dict}) return clean_dict def _convert(self, clean_inner_dict, inner_val, inner_key): # Convert tuples to lists so they get written correctly if isinstance(inner_val, tuple): clean_inner_dict.update( {inner_key: self._recursive_tuple_to_list(inner_val)} ) elif inner_val is not None: clean_inner_dict.update({inner_key: inner_val}) return clean_inner_dict def _recursive_tuple_to_list(self, value): \"\"\"Recursively turn tuples into lists Recursively looks through tuple(s) and convert to lists *Args*: value: value to check and set typ if necessary typed: type of the generic alias to check against *Returns*: value: updated value with correct type casts \"\"\" # Check for __args__ as it signifies a generic and make sure it's not already been cast as a tuple # from a composed payload list_v = [] for v in value: if isinstance(v, tuple): v = self._recursive_tuple_to_list(v) list_v.append(v) else: list_v.append(v) return list_v class AttrSaver(BaseSaver): \"\"\"Base class for saving configs for the attrs backend Contains methods to build a correct output payload and then writes to file based on the file extension *Attributes*: _writers: maps file extension to the correct i/o handler \"\"\" def __init__(self, s3_config=None): super().__init__(s3_config=s3_config) def __call__(self, *args, **kwargs): return AttrSaver(*args, **kwargs) def _clean_up_values(self, payload): # Dictionary to recursively write to out_dict = {} # All of the classes are defined at the top level all_spock_cls = set(vars(payload).keys()) out_dict = self._recursively_handle_clean( payload, out_dict, all_cls=all_spock_cls ) # Convert values clean_dict = self._clean_output(out_dict) return clean_dict def _clean_tuner_values(self, payload): # Just a double nested dict comprehension to unroll to dicts out_dict = { k: {ik: vars(iv) for ik, iv in vars(v).items()} for k, v in vars(payload).items() } # Convert values clean_dict = self._clean_output(out_dict) return clean_dict def _recursively_handle_clean( self, payload, out_dict, parent_name=None, all_cls=None ): \"\"\"Recursively works through spock classes and adds clean data to a dictionary Given a payload (Spockspace) work recursively through items that don't have parents to catch all parameter definitions while correctly mapping nested class definitions to their base level class thus allowing the output markdown to be a valid input file *Args*: payload: current payload (namespace) out_dict: output dictionary parent_name: name of the parent spock class if nested all_cls: all top level spock class definitions *Returns*: out_dict: modified dictionary with the cleaned data \"\"\" for key, val in vars(payload).items(): val_name = type(val).__name__ # This catches basic lists and list of classes if isinstance(val, list): # Check if each entry is a spock class clean_val = [] repeat_flag = False for l_val in val: cls_name = type(l_val).__name__ # For those that are a spock class and are repeated (cls_name == key) simply convert to dict if (cls_name in all_cls) and (cls_name == key): clean_val.append(attr.asdict(l_val)) # For those whose cls is different than the key just append the cls name elif cls_name in all_cls: # Change the flag as this is a repeated class -- which needs to be compressed into a single # k:v pair repeat_flag = True clean_val.append(cls_name) # Fall back to the passed in values else: clean_val.append(l_val) # Handle repeated classes if repeat_flag: clean_val = list(set(clean_val))[-1] out_dict.update({key: clean_val}) # If it's a spock class but has a parent then just use the class name to reference the values elif (val_name in all_cls) and parent_name is not None: out_dict.update({key: val_name}) # Check if it's a spock class without a parent -- iterate the values and recurse to catch more lists elif val_name in all_cls: new_dict = self._recursively_handle_clean( val, {}, parent_name=key, all_cls=all_cls ) out_dict.update({key: new_dict}) # Either base type or no nested values that could be Spock classes else: out_dict.update({key: val}) return out_dict Classes AttrSaver class AttrSaver ( s3_config = None ) ??? example \"View Source\" class AttrSaver(BaseSaver): \"\"\"Base class for saving configs for the attrs backend Contains methods to build a correct output payload and then writes to file based on the file extension *Attributes*: _writers: maps file extension to the correct i/o handler \"\"\" def __init__(self, s3_config=None): super().__init__(s3_config=s3_config) def __call__(self, *args, **kwargs): return AttrSaver(*args, **kwargs) def _clean_up_values(self, payload): # Dictionary to recursively write to out_dict = {} # All of the classes are defined at the top level all_spock_cls = set(vars(payload).keys()) out_dict = self._recursively_handle_clean( payload, out_dict, all_cls=all_spock_cls ) # Convert values clean_dict = self._clean_output(out_dict) return clean_dict def _clean_tuner_values(self, payload): # Just a double nested dict comprehension to unroll to dicts out_dict = { k: {ik: vars(iv) for ik, iv in vars(v).items()} for k, v in vars(payload).items() } # Convert values clean_dict = self._clean_output(out_dict) return clean_dict def _recursively_handle_clean( self, payload, out_dict, parent_name=None, all_cls=None ): \"\"\"Recursively works through spock classes and adds clean data to a dictionary Given a payload (Spockspace) work recursively through items that don't have parents to catch all parameter definitions while correctly mapping nested class definitions to their base level class thus allowing the output markdown to be a valid input file *Args*: payload: current payload (namespace) out_dict: output dictionary parent_name: name of the parent spock class if nested all_cls: all top level spock class definitions *Returns*: out_dict: modified dictionary with the cleaned data \"\"\" for key, val in vars(payload).items(): val_name = type(val).__name__ # This catches basic lists and list of classes if isinstance(val, list): # Check if each entry is a spock class clean_val = [] repeat_flag = False for l_val in val: cls_name = type(l_val).__name__ # For those that are a spock class and are repeated (cls_name == key) simply convert to dict if (cls_name in all_cls) and (cls_name == key): clean_val.append(attr.asdict(l_val)) # For those whose cls is different than the key just append the cls name elif cls_name in all_cls: # Change the flag as this is a repeated class -- which needs to be compressed into a single # k:v pair repeat_flag = True clean_val.append(cls_name) # Fall back to the passed in values else: clean_val.append(l_val) # Handle repeated classes if repeat_flag: clean_val = list(set(clean_val))[-1] out_dict.update({key: clean_val}) # If it's a spock class but has a parent then just use the class name to reference the values elif (val_name in all_cls) and parent_name is not None: out_dict.update({key: val_name}) # Check if it's a spock class without a parent -- iterate the values and recurse to catch more lists elif val_name in all_cls: new_dict = self._recursively_handle_clean( val, {}, parent_name=key, all_cls=all_cls ) out_dict.update({key: new_dict}) # Either base type or no nested values that could be Spock classes else: out_dict.update({key: val}) return out_dict Ancestors (in MRO) spock.backend.saver.BaseSaver spock.backend.handler.BaseHandler abc.ABC Methods dict_payload def dict_payload ( self , payload ) Clean up the config payload so it can be returned as a dict representation Args : payload: dirty payload Returns : clean_dict: cleaned output payload ??? example \"View Source\" def dict_payload(self, payload): \"\"\"Clean up the config payload so it can be returned as a dict representation *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" # Fix up values -- parameters return self._clean_up_values(payload) save def save ( self , payload , path , file_name = None , create_save_path = False , extra_info = True , file_extension = '.yaml' , tuner_payload = None , fixed_uuid = None ) Writes Spock config to file Cleans and builds an output payload and then correctly writes it to file based on the specified file extension Args : payload: current config payload path: path to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to uuid if None create_save_path: boolean to create the path if non-existent extra_info: boolean to write extra info file_extension: what type of file to write tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite Returns : None ??? example \"View Source\" def save( self, payload, path, file_name=None, create_save_path=False, extra_info=True, file_extension=\".yaml\", tuner_payload=None, fixed_uuid=None, ): # pylint: disable=too-many-arguments \"\"\"Writes Spock config to file Cleans and builds an output payload and then correctly writes it to file based on the specified file extension *Args*: payload: current config payload path: path to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to uuid if None create_save_path: boolean to create the path if non-existent extra_info: boolean to write extra info file_extension: what type of file to write tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite *Returns*: None \"\"\" # Check extension self._check_extension(file_extension=file_extension) # Make the filename -- always append a uuid for unique-ness uuid_str = str(uuid4()) if fixed_uuid is None else fixed_uuid fname = \"\" if file_name is None else f\"{file_name}.\" name = f\"{fname}{uuid_str}.spock.cfg{file_extension}\" # Fix up values -- parameters out_dict = self._clean_up_values(payload) # Fix up the tuner values if present tuner_dict = ( self._clean_tuner_values(tuner_payload) if tuner_payload is not None else None ) if tuner_dict is not None: out_dict.update(tuner_dict) # Get extra info extra_dict = add_info() if extra_info else None try: self._supported_extensions.get(file_extension)().save( out_dict=out_dict, info_dict=extra_dict, path=str(path), name=name, create_path=create_save_path, s3_config=self._s3_config, ) except OSError as e: print(f\"Unable to write to given path: {path / name}\") raise e BaseSaver class BaseSaver ( s3_config = None ) ??? example \"View Source\" class BaseSaver(BaseHandler): # pylint: disable=too-few-public-methods \"\"\"Base class for saving configs Contains methods to build a correct output payload and then writes to file based on the file extension *Attributes*: _writers: maps file extension to the correct i/o handler _s3_config: optional S3Config object to handle s3 access \"\"\" def __init__(self, s3_config=None): super(BaseSaver, self).__init__(s3_config=s3_config) def dict_payload(self, payload): \"\"\"Clean up the config payload so it can be returned as a dict representation *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" # Fix up values -- parameters return self._clean_up_values(payload) def save( self, payload, path, file_name=None, create_save_path=False, extra_info=True, file_extension=\".yaml\", tuner_payload=None, fixed_uuid=None, ): # pylint: disable=too-many-arguments \"\"\"Writes Spock config to file Cleans and builds an output payload and then correctly writes it to file based on the specified file extension *Args*: payload: current config payload path: path to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to uuid if None create_save_path: boolean to create the path if non-existent extra_info: boolean to write extra info file_extension: what type of file to write tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite *Returns*: None \"\"\" # Check extension self._check_extension(file_extension=file_extension) # Make the filename -- always append a uuid for unique-ness uuid_str = str(uuid4()) if fixed_uuid is None else fixed_uuid fname = \"\" if file_name is None else f\"{file_name}.\" name = f\"{fname}{uuid_str}.spock.cfg{file_extension}\" # Fix up values -- parameters out_dict = self._clean_up_values(payload) # Fix up the tuner values if present tuner_dict = ( self._clean_tuner_values(tuner_payload) if tuner_payload is not None else None ) if tuner_dict is not None: out_dict.update(tuner_dict) # Get extra info extra_dict = add_info() if extra_info else None try: self._supported_extensions.get(file_extension)().save( out_dict=out_dict, info_dict=extra_dict, path=str(path), name=name, create_path=create_save_path, s3_config=self._s3_config, ) except OSError as e: print(f\"Unable to write to given path: {path / name}\") raise e @abstractmethod def _clean_up_values(self, payload): \"\"\"Clean up the config payload so it can be written to file *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" @abstractmethod def _clean_tuner_values(self, payload): \"\"\"Cleans up the base tuner payload that is not sampled *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" def _clean_output(self, out_dict): \"\"\"Clean up the dictionary so it can be written to file *Args*: out_dict: cleaned dictionary extra_info: boolean to add extra info *Returns*: clean_dict: cleaned output payload \"\"\" # Convert values clean_dict = {} for key, val in out_dict.items(): clean_inner_dict = {} if isinstance(val, list): for idx, list_val in enumerate(val): tmp_dict = {} for inner_key, inner_val in list_val.items(): tmp_dict = self._convert(tmp_dict, inner_val, inner_key) val[idx] = tmp_dict clean_inner_dict = val else: for inner_key, inner_val in val.items(): clean_inner_dict = self._convert( clean_inner_dict, inner_val, inner_key ) clean_dict.update({key: clean_inner_dict}) return clean_dict def _convert(self, clean_inner_dict, inner_val, inner_key): # Convert tuples to lists so they get written correctly if isinstance(inner_val, tuple): clean_inner_dict.update( {inner_key: self._recursive_tuple_to_list(inner_val)} ) elif inner_val is not None: clean_inner_dict.update({inner_key: inner_val}) return clean_inner_dict def _recursive_tuple_to_list(self, value): \"\"\"Recursively turn tuples into lists Recursively looks through tuple(s) and convert to lists *Args*: value: value to check and set typ if necessary typed: type of the generic alias to check against *Returns*: value: updated value with correct type casts \"\"\" # Check for __args__ as it signifies a generic and make sure it's not already been cast as a tuple # from a composed payload list_v = [] for v in value: if isinstance(v, tuple): v = self._recursive_tuple_to_list(v) list_v.append(v) else: list_v.append(v) return list_v Ancestors (in MRO) spock.backend.handler.BaseHandler abc.ABC Descendants spock.backend.saver.AttrSaver Methods dict_payload def dict_payload ( self , payload ) Clean up the config payload so it can be returned as a dict representation Args : payload: dirty payload Returns : clean_dict: cleaned output payload ??? example \"View Source\" def dict_payload(self, payload): \"\"\"Clean up the config payload so it can be returned as a dict representation *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" # Fix up values -- parameters return self._clean_up_values(payload) save def save ( self , payload , path , file_name = None , create_save_path = False , extra_info = True , file_extension = '.yaml' , tuner_payload = None , fixed_uuid = None ) Writes Spock config to file Cleans and builds an output payload and then correctly writes it to file based on the specified file extension Args : payload: current config payload path: path to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to uuid if None create_save_path: boolean to create the path if non-existent extra_info: boolean to write extra info file_extension: what type of file to write tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite Returns : None ??? example \"View Source\" def save( self, payload, path, file_name=None, create_save_path=False, extra_info=True, file_extension=\".yaml\", tuner_payload=None, fixed_uuid=None, ): # pylint: disable=too-many-arguments \"\"\"Writes Spock config to file Cleans and builds an output payload and then correctly writes it to file based on the specified file extension *Args*: payload: current config payload path: path to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to uuid if None create_save_path: boolean to create the path if non-existent extra_info: boolean to write extra info file_extension: what type of file to write tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite *Returns*: None \"\"\" # Check extension self._check_extension(file_extension=file_extension) # Make the filename -- always append a uuid for unique-ness uuid_str = str(uuid4()) if fixed_uuid is None else fixed_uuid fname = \"\" if file_name is None else f\"{file_name}.\" name = f\"{fname}{uuid_str}.spock.cfg{file_extension}\" # Fix up values -- parameters out_dict = self._clean_up_values(payload) # Fix up the tuner values if present tuner_dict = ( self._clean_tuner_values(tuner_payload) if tuner_payload is not None else None ) if tuner_dict is not None: out_dict.update(tuner_dict) # Get extra info extra_dict = add_info() if extra_info else None try: self._supported_extensions.get(file_extension)().save( out_dict=out_dict, info_dict=extra_dict, path=str(path), name=name, create_path=create_save_path, s3_config=self._s3_config, ) except OSError as e: print(f\"Unable to write to given path: {path / name}\") raise e","title":"Saver"},{"location":"reference/spock/backend/saver/#module-spockbackendsaver","text":"Handles prepping and saving the Spock config None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles prepping and saving the Spock config\"\"\" from abc import abstractmethod from uuid import uuid4 import attr from spock.backend.handler import BaseHandler from spock.utils import add_info class BaseSaver(BaseHandler): # pylint: disable=too-few-public-methods \"\"\"Base class for saving configs Contains methods to build a correct output payload and then writes to file based on the file extension *Attributes*: _writers: maps file extension to the correct i/o handler _s3_config: optional S3Config object to handle s3 access \"\"\" def __init__(self, s3_config=None): super(BaseSaver, self).__init__(s3_config=s3_config) def dict_payload(self, payload): \"\"\"Clean up the config payload so it can be returned as a dict representation *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" # Fix up values -- parameters return self._clean_up_values(payload) def save( self, payload, path, file_name=None, create_save_path=False, extra_info=True, file_extension=\".yaml\", tuner_payload=None, fixed_uuid=None, ): # pylint: disable=too-many-arguments \"\"\"Writes Spock config to file Cleans and builds an output payload and then correctly writes it to file based on the specified file extension *Args*: payload: current config payload path: path to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to uuid if None create_save_path: boolean to create the path if non-existent extra_info: boolean to write extra info file_extension: what type of file to write tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite *Returns*: None \"\"\" # Check extension self._check_extension(file_extension=file_extension) # Make the filename -- always append a uuid for unique-ness uuid_str = str(uuid4()) if fixed_uuid is None else fixed_uuid fname = \"\" if file_name is None else f\"{file_name}.\" name = f\"{fname}{uuid_str}.spock.cfg{file_extension}\" # Fix up values -- parameters out_dict = self._clean_up_values(payload) # Fix up the tuner values if present tuner_dict = ( self._clean_tuner_values(tuner_payload) if tuner_payload is not None else None ) if tuner_dict is not None: out_dict.update(tuner_dict) # Get extra info extra_dict = add_info() if extra_info else None try: self._supported_extensions.get(file_extension)().save( out_dict=out_dict, info_dict=extra_dict, path=str(path), name=name, create_path=create_save_path, s3_config=self._s3_config, ) except OSError as e: print(f\"Unable to write to given path: {path / name}\") raise e @abstractmethod def _clean_up_values(self, payload): \"\"\"Clean up the config payload so it can be written to file *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" @abstractmethod def _clean_tuner_values(self, payload): \"\"\"Cleans up the base tuner payload that is not sampled *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" def _clean_output(self, out_dict): \"\"\"Clean up the dictionary so it can be written to file *Args*: out_dict: cleaned dictionary extra_info: boolean to add extra info *Returns*: clean_dict: cleaned output payload \"\"\" # Convert values clean_dict = {} for key, val in out_dict.items(): clean_inner_dict = {} if isinstance(val, list): for idx, list_val in enumerate(val): tmp_dict = {} for inner_key, inner_val in list_val.items(): tmp_dict = self._convert(tmp_dict, inner_val, inner_key) val[idx] = tmp_dict clean_inner_dict = val else: for inner_key, inner_val in val.items(): clean_inner_dict = self._convert( clean_inner_dict, inner_val, inner_key ) clean_dict.update({key: clean_inner_dict}) return clean_dict def _convert(self, clean_inner_dict, inner_val, inner_key): # Convert tuples to lists so they get written correctly if isinstance(inner_val, tuple): clean_inner_dict.update( {inner_key: self._recursive_tuple_to_list(inner_val)} ) elif inner_val is not None: clean_inner_dict.update({inner_key: inner_val}) return clean_inner_dict def _recursive_tuple_to_list(self, value): \"\"\"Recursively turn tuples into lists Recursively looks through tuple(s) and convert to lists *Args*: value: value to check and set typ if necessary typed: type of the generic alias to check against *Returns*: value: updated value with correct type casts \"\"\" # Check for __args__ as it signifies a generic and make sure it's not already been cast as a tuple # from a composed payload list_v = [] for v in value: if isinstance(v, tuple): v = self._recursive_tuple_to_list(v) list_v.append(v) else: list_v.append(v) return list_v class AttrSaver(BaseSaver): \"\"\"Base class for saving configs for the attrs backend Contains methods to build a correct output payload and then writes to file based on the file extension *Attributes*: _writers: maps file extension to the correct i/o handler \"\"\" def __init__(self, s3_config=None): super().__init__(s3_config=s3_config) def __call__(self, *args, **kwargs): return AttrSaver(*args, **kwargs) def _clean_up_values(self, payload): # Dictionary to recursively write to out_dict = {} # All of the classes are defined at the top level all_spock_cls = set(vars(payload).keys()) out_dict = self._recursively_handle_clean( payload, out_dict, all_cls=all_spock_cls ) # Convert values clean_dict = self._clean_output(out_dict) return clean_dict def _clean_tuner_values(self, payload): # Just a double nested dict comprehension to unroll to dicts out_dict = { k: {ik: vars(iv) for ik, iv in vars(v).items()} for k, v in vars(payload).items() } # Convert values clean_dict = self._clean_output(out_dict) return clean_dict def _recursively_handle_clean( self, payload, out_dict, parent_name=None, all_cls=None ): \"\"\"Recursively works through spock classes and adds clean data to a dictionary Given a payload (Spockspace) work recursively through items that don't have parents to catch all parameter definitions while correctly mapping nested class definitions to their base level class thus allowing the output markdown to be a valid input file *Args*: payload: current payload (namespace) out_dict: output dictionary parent_name: name of the parent spock class if nested all_cls: all top level spock class definitions *Returns*: out_dict: modified dictionary with the cleaned data \"\"\" for key, val in vars(payload).items(): val_name = type(val).__name__ # This catches basic lists and list of classes if isinstance(val, list): # Check if each entry is a spock class clean_val = [] repeat_flag = False for l_val in val: cls_name = type(l_val).__name__ # For those that are a spock class and are repeated (cls_name == key) simply convert to dict if (cls_name in all_cls) and (cls_name == key): clean_val.append(attr.asdict(l_val)) # For those whose cls is different than the key just append the cls name elif cls_name in all_cls: # Change the flag as this is a repeated class -- which needs to be compressed into a single # k:v pair repeat_flag = True clean_val.append(cls_name) # Fall back to the passed in values else: clean_val.append(l_val) # Handle repeated classes if repeat_flag: clean_val = list(set(clean_val))[-1] out_dict.update({key: clean_val}) # If it's a spock class but has a parent then just use the class name to reference the values elif (val_name in all_cls) and parent_name is not None: out_dict.update({key: val_name}) # Check if it's a spock class without a parent -- iterate the values and recurse to catch more lists elif val_name in all_cls: new_dict = self._recursively_handle_clean( val, {}, parent_name=key, all_cls=all_cls ) out_dict.update({key: new_dict}) # Either base type or no nested values that could be Spock classes else: out_dict.update({key: val}) return out_dict","title":"Module spock.backend.saver"},{"location":"reference/spock/backend/saver/#classes","text":"","title":"Classes"},{"location":"reference/spock/backend/saver/#attrsaver","text":"class AttrSaver ( s3_config = None ) ??? example \"View Source\" class AttrSaver(BaseSaver): \"\"\"Base class for saving configs for the attrs backend Contains methods to build a correct output payload and then writes to file based on the file extension *Attributes*: _writers: maps file extension to the correct i/o handler \"\"\" def __init__(self, s3_config=None): super().__init__(s3_config=s3_config) def __call__(self, *args, **kwargs): return AttrSaver(*args, **kwargs) def _clean_up_values(self, payload): # Dictionary to recursively write to out_dict = {} # All of the classes are defined at the top level all_spock_cls = set(vars(payload).keys()) out_dict = self._recursively_handle_clean( payload, out_dict, all_cls=all_spock_cls ) # Convert values clean_dict = self._clean_output(out_dict) return clean_dict def _clean_tuner_values(self, payload): # Just a double nested dict comprehension to unroll to dicts out_dict = { k: {ik: vars(iv) for ik, iv in vars(v).items()} for k, v in vars(payload).items() } # Convert values clean_dict = self._clean_output(out_dict) return clean_dict def _recursively_handle_clean( self, payload, out_dict, parent_name=None, all_cls=None ): \"\"\"Recursively works through spock classes and adds clean data to a dictionary Given a payload (Spockspace) work recursively through items that don't have parents to catch all parameter definitions while correctly mapping nested class definitions to their base level class thus allowing the output markdown to be a valid input file *Args*: payload: current payload (namespace) out_dict: output dictionary parent_name: name of the parent spock class if nested all_cls: all top level spock class definitions *Returns*: out_dict: modified dictionary with the cleaned data \"\"\" for key, val in vars(payload).items(): val_name = type(val).__name__ # This catches basic lists and list of classes if isinstance(val, list): # Check if each entry is a spock class clean_val = [] repeat_flag = False for l_val in val: cls_name = type(l_val).__name__ # For those that are a spock class and are repeated (cls_name == key) simply convert to dict if (cls_name in all_cls) and (cls_name == key): clean_val.append(attr.asdict(l_val)) # For those whose cls is different than the key just append the cls name elif cls_name in all_cls: # Change the flag as this is a repeated class -- which needs to be compressed into a single # k:v pair repeat_flag = True clean_val.append(cls_name) # Fall back to the passed in values else: clean_val.append(l_val) # Handle repeated classes if repeat_flag: clean_val = list(set(clean_val))[-1] out_dict.update({key: clean_val}) # If it's a spock class but has a parent then just use the class name to reference the values elif (val_name in all_cls) and parent_name is not None: out_dict.update({key: val_name}) # Check if it's a spock class without a parent -- iterate the values and recurse to catch more lists elif val_name in all_cls: new_dict = self._recursively_handle_clean( val, {}, parent_name=key, all_cls=all_cls ) out_dict.update({key: new_dict}) # Either base type or no nested values that could be Spock classes else: out_dict.update({key: val}) return out_dict","title":"AttrSaver"},{"location":"reference/spock/backend/saver/#ancestors-in-mro","text":"spock.backend.saver.BaseSaver spock.backend.handler.BaseHandler abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/backend/saver/#methods","text":"","title":"Methods"},{"location":"reference/spock/backend/saver/#dict_payload","text":"def dict_payload ( self , payload ) Clean up the config payload so it can be returned as a dict representation Args : payload: dirty payload Returns : clean_dict: cleaned output payload ??? example \"View Source\" def dict_payload(self, payload): \"\"\"Clean up the config payload so it can be returned as a dict representation *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" # Fix up values -- parameters return self._clean_up_values(payload)","title":"dict_payload"},{"location":"reference/spock/backend/saver/#save","text":"def save ( self , payload , path , file_name = None , create_save_path = False , extra_info = True , file_extension = '.yaml' , tuner_payload = None , fixed_uuid = None ) Writes Spock config to file Cleans and builds an output payload and then correctly writes it to file based on the specified file extension Args : payload: current config payload path: path to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to uuid if None create_save_path: boolean to create the path if non-existent extra_info: boolean to write extra info file_extension: what type of file to write tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite Returns : None ??? example \"View Source\" def save( self, payload, path, file_name=None, create_save_path=False, extra_info=True, file_extension=\".yaml\", tuner_payload=None, fixed_uuid=None, ): # pylint: disable=too-many-arguments \"\"\"Writes Spock config to file Cleans and builds an output payload and then correctly writes it to file based on the specified file extension *Args*: payload: current config payload path: path to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to uuid if None create_save_path: boolean to create the path if non-existent extra_info: boolean to write extra info file_extension: what type of file to write tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite *Returns*: None \"\"\" # Check extension self._check_extension(file_extension=file_extension) # Make the filename -- always append a uuid for unique-ness uuid_str = str(uuid4()) if fixed_uuid is None else fixed_uuid fname = \"\" if file_name is None else f\"{file_name}.\" name = f\"{fname}{uuid_str}.spock.cfg{file_extension}\" # Fix up values -- parameters out_dict = self._clean_up_values(payload) # Fix up the tuner values if present tuner_dict = ( self._clean_tuner_values(tuner_payload) if tuner_payload is not None else None ) if tuner_dict is not None: out_dict.update(tuner_dict) # Get extra info extra_dict = add_info() if extra_info else None try: self._supported_extensions.get(file_extension)().save( out_dict=out_dict, info_dict=extra_dict, path=str(path), name=name, create_path=create_save_path, s3_config=self._s3_config, ) except OSError as e: print(f\"Unable to write to given path: {path / name}\") raise e","title":"save"},{"location":"reference/spock/backend/saver/#basesaver","text":"class BaseSaver ( s3_config = None ) ??? example \"View Source\" class BaseSaver(BaseHandler): # pylint: disable=too-few-public-methods \"\"\"Base class for saving configs Contains methods to build a correct output payload and then writes to file based on the file extension *Attributes*: _writers: maps file extension to the correct i/o handler _s3_config: optional S3Config object to handle s3 access \"\"\" def __init__(self, s3_config=None): super(BaseSaver, self).__init__(s3_config=s3_config) def dict_payload(self, payload): \"\"\"Clean up the config payload so it can be returned as a dict representation *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" # Fix up values -- parameters return self._clean_up_values(payload) def save( self, payload, path, file_name=None, create_save_path=False, extra_info=True, file_extension=\".yaml\", tuner_payload=None, fixed_uuid=None, ): # pylint: disable=too-many-arguments \"\"\"Writes Spock config to file Cleans and builds an output payload and then correctly writes it to file based on the specified file extension *Args*: payload: current config payload path: path to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to uuid if None create_save_path: boolean to create the path if non-existent extra_info: boolean to write extra info file_extension: what type of file to write tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite *Returns*: None \"\"\" # Check extension self._check_extension(file_extension=file_extension) # Make the filename -- always append a uuid for unique-ness uuid_str = str(uuid4()) if fixed_uuid is None else fixed_uuid fname = \"\" if file_name is None else f\"{file_name}.\" name = f\"{fname}{uuid_str}.spock.cfg{file_extension}\" # Fix up values -- parameters out_dict = self._clean_up_values(payload) # Fix up the tuner values if present tuner_dict = ( self._clean_tuner_values(tuner_payload) if tuner_payload is not None else None ) if tuner_dict is not None: out_dict.update(tuner_dict) # Get extra info extra_dict = add_info() if extra_info else None try: self._supported_extensions.get(file_extension)().save( out_dict=out_dict, info_dict=extra_dict, path=str(path), name=name, create_path=create_save_path, s3_config=self._s3_config, ) except OSError as e: print(f\"Unable to write to given path: {path / name}\") raise e @abstractmethod def _clean_up_values(self, payload): \"\"\"Clean up the config payload so it can be written to file *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" @abstractmethod def _clean_tuner_values(self, payload): \"\"\"Cleans up the base tuner payload that is not sampled *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" def _clean_output(self, out_dict): \"\"\"Clean up the dictionary so it can be written to file *Args*: out_dict: cleaned dictionary extra_info: boolean to add extra info *Returns*: clean_dict: cleaned output payload \"\"\" # Convert values clean_dict = {} for key, val in out_dict.items(): clean_inner_dict = {} if isinstance(val, list): for idx, list_val in enumerate(val): tmp_dict = {} for inner_key, inner_val in list_val.items(): tmp_dict = self._convert(tmp_dict, inner_val, inner_key) val[idx] = tmp_dict clean_inner_dict = val else: for inner_key, inner_val in val.items(): clean_inner_dict = self._convert( clean_inner_dict, inner_val, inner_key ) clean_dict.update({key: clean_inner_dict}) return clean_dict def _convert(self, clean_inner_dict, inner_val, inner_key): # Convert tuples to lists so they get written correctly if isinstance(inner_val, tuple): clean_inner_dict.update( {inner_key: self._recursive_tuple_to_list(inner_val)} ) elif inner_val is not None: clean_inner_dict.update({inner_key: inner_val}) return clean_inner_dict def _recursive_tuple_to_list(self, value): \"\"\"Recursively turn tuples into lists Recursively looks through tuple(s) and convert to lists *Args*: value: value to check and set typ if necessary typed: type of the generic alias to check against *Returns*: value: updated value with correct type casts \"\"\" # Check for __args__ as it signifies a generic and make sure it's not already been cast as a tuple # from a composed payload list_v = [] for v in value: if isinstance(v, tuple): v = self._recursive_tuple_to_list(v) list_v.append(v) else: list_v.append(v) return list_v","title":"BaseSaver"},{"location":"reference/spock/backend/saver/#ancestors-in-mro_1","text":"spock.backend.handler.BaseHandler abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/spock/backend/saver/#descendants","text":"spock.backend.saver.AttrSaver","title":"Descendants"},{"location":"reference/spock/backend/saver/#methods_1","text":"","title":"Methods"},{"location":"reference/spock/backend/saver/#dict_payload_1","text":"def dict_payload ( self , payload ) Clean up the config payload so it can be returned as a dict representation Args : payload: dirty payload Returns : clean_dict: cleaned output payload ??? example \"View Source\" def dict_payload(self, payload): \"\"\"Clean up the config payload so it can be returned as a dict representation *Args*: payload: dirty payload *Returns*: clean_dict: cleaned output payload \"\"\" # Fix up values -- parameters return self._clean_up_values(payload)","title":"dict_payload"},{"location":"reference/spock/backend/saver/#save_1","text":"def save ( self , payload , path , file_name = None , create_save_path = False , extra_info = True , file_extension = '.yaml' , tuner_payload = None , fixed_uuid = None ) Writes Spock config to file Cleans and builds an output payload and then correctly writes it to file based on the specified file extension Args : payload: current config payload path: path to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to uuid if None create_save_path: boolean to create the path if non-existent extra_info: boolean to write extra info file_extension: what type of file to write tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite Returns : None ??? example \"View Source\" def save( self, payload, path, file_name=None, create_save_path=False, extra_info=True, file_extension=\".yaml\", tuner_payload=None, fixed_uuid=None, ): # pylint: disable=too-many-arguments \"\"\"Writes Spock config to file Cleans and builds an output payload and then correctly writes it to file based on the specified file extension *Args*: payload: current config payload path: path to save file_name: name of file (will be appended with .spock.cfg.file_extension) -- falls back to uuid if None create_save_path: boolean to create the path if non-existent extra_info: boolean to write extra info file_extension: what type of file to write tuner_payload: tuner level payload (unsampled) fixed_uuid: fixed uuid to allow for file overwrite *Returns*: None \"\"\" # Check extension self._check_extension(file_extension=file_extension) # Make the filename -- always append a uuid for unique-ness uuid_str = str(uuid4()) if fixed_uuid is None else fixed_uuid fname = \"\" if file_name is None else f\"{file_name}.\" name = f\"{fname}{uuid_str}.spock.cfg{file_extension}\" # Fix up values -- parameters out_dict = self._clean_up_values(payload) # Fix up the tuner values if present tuner_dict = ( self._clean_tuner_values(tuner_payload) if tuner_payload is not None else None ) if tuner_dict is not None: out_dict.update(tuner_dict) # Get extra info extra_dict = add_info() if extra_info else None try: self._supported_extensions.get(file_extension)().save( out_dict=out_dict, info_dict=extra_dict, path=str(path), name=name, create_path=create_save_path, s3_config=self._s3_config, ) except OSError as e: print(f\"Unable to write to given path: {path / name}\") raise e","title":"save"},{"location":"reference/spock/backend/typed/","text":"Module spock.backend.typed Handles the definitions of arguments types for Spock (backend: attrs) None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the definitions of arguments types for Spock (backend: attrs)\"\"\" import sys from enum import EnumMeta from functools import partial from typing import TypeVar, Union import attr minor = sys.version_info.minor if minor < 7: from typing import GenericMeta as _GenericAlias else: from typing import _GenericAlias class SavePath(str): \"\"\"Spock special key for saving the Spock config to file Defines a special key use to save the current Spock config to file \"\"\" def __new__(cls, x): return super().__new__(cls, x) def _get_name_py_version(typed): \"\"\"Gets the name of the type depending on the python version *Args*: typed: the type of the parameter *Returns*: name of the type \"\"\" return typed._name if hasattr(typed, \"_name\") else typed.__name__ def _extract_base_type(typed): \"\"\"Extracts the name of the type from a _GenericAlias Assumes that the derived types are only of length 1 as the __args__ are [0] recursed... this is not true for tuples *Args*: typed: the type of the parameter *Returns*: name of type \"\"\" if hasattr(typed, \"__args__\"): name = _get_name_py_version(typed=typed) bracket_val = f\"{name}[{_extract_base_type(typed.__args__[0])}]\" return bracket_val else: bracket_value = typed.__name__ return bracket_value def _recursive_generic_validator(typed): \"\"\"Recursively assembles the validators for nested generic types Walks through the nested type structure and determines whether to recurse all the way to a base type. Once it hits the base type it bubbles up the correct validator that is nested within the upper validator *Args*: typed: input type *Returns*: return_type: recursively built deep_iterable validators \"\"\" if hasattr(typed, \"__args__\"): # If there are more __args__ then we still need to recurse as it is still a GenericAlias # Iterate through since there might be multiple types? if len(typed.__args__) > 1: return_type = attr.validators.deep_iterable( member_validator=_recursive_generic_validator(typed.__args__), iterable_validator=attr.validators.instance_of(typed.__origin__), ) else: return_type = attr.validators.deep_iterable( member_validator=_recursive_generic_validator(typed.__args__[0]), iterable_validator=attr.validators.instance_of(typed.__origin__), ) return return_type else: # If no more __args__ then we are to the base type and need to bubble up the type # But we need to check against base types and enums if isinstance(typed, EnumMeta): base_type, allowed = _check_enum_props(typed) return_type = attr.validators.and_( attr.validators.instance_of(base_type), attr.validators.in_(allowed) ) else: return_type = attr.validators.instance_of(typed) return return_type def _generic_alias_katra(typed, default=None, optional=False): \"\"\"Private interface to create a subscripted generic_alias katra A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality Handles: List[type] and Tuple[type] *Args*: typed: the type of the parameter to define default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) *Returns*: x: Attribute from attrs \"\"\" # base python class from which a GenericAlias is derived base_typed = typed.__origin__ if default is not None: x = attr.ib( validator=_recursive_generic_validator(typed), default=default, type=base_typed, metadata={\"base\": _extract_base_type(typed), \"type\": typed}, ) # x = attr.ib(validator=_recursive_generic_iterator(typed), default=default, type=base_typed, # metadata={'base': _extract_base_type(typed)}) elif optional: # if there's no default, but marked as optional, then set the default to None x = attr.ib( validator=attr.validators.optional(_recursive_generic_validator(typed)), type=base_typed, default=default, metadata={ \"optional\": True, \"base\": _extract_base_type(typed), \"type\": typed, }, ) # x = attr.ib(validator=attr.validators.optional(_recursive_generic_iterator(typed)), type=base_typed, # default=default, metadata={'optional': True, 'base': _extract_base_type(typed)}) else: x = attr.ib( validator=_recursive_generic_validator(typed), type=base_typed, metadata={\"base\": _extract_base_type(typed), \"type\": typed}, ) # x = attr.ib(validator=_recursive_generic_iterator(typed), type=base_typed, # metadata={'base': _extract_base_type(typed)}) return x def _check_enum_props(typed): \"\"\"Handles properties of enums Checks if all types of the enum are the same and assembles a list of allowed values *Args*: typed: the type of parameter (Enum) *Returns*: base_type: the base type of the Enum allowed: List of allowed values of the Enum \"\"\" # First check if the types of Enum are the same type_set = {type(val.value) for val in typed} if len(type_set) > 1: raise TypeError(f\"Enum cannot be defined with multiple types: {type_set}\") base_type = list(type_set)[-1] allowed = [val.value for val in typed] return base_type, allowed def _enum_katra(typed, default=None, optional=False): \"\"\"Private interface to create a Enum typed katra A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality *Args*: typed: the type of the parameter to define default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) *Returns*: x: Attribute from attrs \"\"\" # First check if the types of Enum are the same base_type, allowed = _check_enum_props(typed) if base_type.__name__ == \"type\": x = _enum_class_katra( typed=typed, allowed=allowed, default=default, optional=optional ) else: x = _enum_base_katra( typed=typed, base_type=base_type, allowed=allowed, default=default, optional=optional, ) return x def _enum_base_katra(typed, base_type, allowed, default=None, optional=False): \"\"\"Private interface to create a base Enum typed katra Here we handle the base types of enums that allows us to force a type check on the instance A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality *Args*: typed: the type of the parameter to define base_type: underlying base type allowed: set of allowed values default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) *Returns*: x: Attribute from attrs \"\"\" if default is not None: x = attr.ib( validator=[ attr.validators.instance_of(base_type), attr.validators.in_(allowed), ], default=default, type=typed, metadata={\"base\": typed.__name__}, ) elif optional: x = attr.ib( validator=attr.validators.optional( [attr.validators.instance_of(base_type), attr.validators.in_(allowed)] ), default=default, type=typed, metadata={\"base\": typed.__name__, \"optional\": True}, ) else: x = attr.ib( validator=[ attr.validators.instance_of(base_type), attr.validators.in_(allowed), ], type=typed, metadata={\"base\": typed.__name__}, ) return x def _in_type(instance, attribute, value, options): \"\"\"attrs validator for class type enum Checks if the type of the class (e.g. value) is in the specified set of types provided *Args*: instance: current object instance attribute: current attribute instance value: current value trying to be set in the attrs instance options: list, tuple, or enum of allowed options *Returns*: \"\"\" if type(value) not in options: raise ValueError(f\"{attribute.name} must be in {options}\") def _enum_class_katra(typed, allowed, default=None, optional=False): \"\"\"Private interface to create a base Enum typed katra Here we handle the class based types of enums. Seeing as these classes are generated dynamically we cannot force type checking of a specific instance however the in_ validator will catch an incorrect instance type A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality *Args*: typed: the type of the parameter to define allowed: set of allowed values default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) *Returns*: x: Attribute from attrs \"\"\" if default is not None: x = attr.ib( validator=[partial(_in_type, options=allowed)], default=default, type=typed, metadata={\"base\": typed.__name__}, ) elif optional: x = attr.ib( validator=attr.validators.optional([partial(_in_type, options=allowed)]), default=default, type=typed, metadata={\"base\": typed.__name__, \"optional\": True}, ) else: x = attr.ib( validator=[partial(_in_type, options=allowed)], type=typed, metadata={\"base\": typed.__name__}, ) return x def _type_katra(typed, default=None, optional=False): \"\"\"Private interface to create a simple typed katra A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality Handles: bool, string, float, int, List, and Tuple *Args*: typed: the type of the parameter to define default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) *Returns*: x: Attribute from attrs \"\"\" # Grab the name first based on if it is a base type or GenericAlias if isinstance(typed, type): name = typed.__name__ elif isinstance(typed, _GenericAlias): name = _get_name_py_version(typed=typed) else: raise TypeError(\"Encountered an unexpected type in _type_katra\") special_key = None # Default booleans to false and optional due to the nature of a boolean if isinstance(typed, type) and name == \"bool\": optional = True if default is not True: default = False # For the save path type we need to swap the type back to it's base class (str) elif isinstance(typed, type) and name == \"SavePath\": optional = True special_key = name typed = str if default is not None: # if a default is provided, that takes precedence x = attr.ib( validator=attr.validators.instance_of(typed), default=default, type=typed, metadata={\"base\": name, \"special_key\": special_key}, ) elif optional: x = attr.ib( validator=attr.validators.optional(attr.validators.instance_of(typed)), default=default, type=typed, metadata={\"optional\": True, \"base\": name, \"special_key\": special_key}, ) else: x = attr.ib( validator=attr.validators.instance_of(typed), type=typed, metadata={\"base\": name, \"special_key\": special_key}, ) return x def _handle_optional_typing(typed): \"\"\"Handles when a type hint is Optional Handles Optional[type] typing and strips out the base type to pass back to the creation of a katra which needs base typing *Args*: typed: type *Returns*: typed: type (modified if Optional) optional: boolean for katra creation \"\"\" # Set optional to false optional = False # Check if it has __args__ to look for optionality as it is a GenericAlias if hasattr(typed, \"__args__\"): # If it is more than one than it is most likely optional but check against NoneType in the tuple to verify # Check the length of type __args__ type_args = typed.__args__ # Optional[X] has type_args = (X, None) and is equal to Union[X, None] if (len(type_args) == 2) and (typed == Union[type_args[0], None]): typed = type_args[0] # Set the optional flag to true optional = True return typed, optional def _check_generic_recursive_single_type(typed): \"\"\"Checks generics for the single types -- mixed types of generics are not allowed DEPRECATED -- NOW SUPPORTS MIXED TYPES OF TUPLES *Args*: typed: type *Returns*: \"\"\" # Check if it has __args__ to look for optionality as it is a GenericAlias # if hasattr(typed, '__args__'): # if len(set(typed.__args__)) > 1: # type_list = [str(val) for val in typed.__args__] # raise TypeError(f\"Passing multiple different subscript types to GenericAlias is not supported: {type_list}\") # else: # for val in typed.__args__: # _check_generic_recursive_single_type(typed=val) pass def katra(typed, default=None): \"\"\"Public interface to create a katra A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality *Args*: typed: the type of the parameter to define default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) Returns: x: Attribute from attrs \"\"\" # Handle optionals typed, optional = _handle_optional_typing(typed) # Check generic types for consistent types _check_generic_recursive_single_type(typed) # We need to check if the type is a _GenericAlias so that we can handle subscripted general types # If it is subscript typed it will not be T which python uses as a generic type name if isinstance(typed, _GenericAlias) and ( not isinstance(typed.__args__[0], TypeVar) ): x = _generic_alias_katra(typed=typed, default=default, optional=optional) elif isinstance(typed, EnumMeta): x = _enum_katra(typed=typed, default=default, optional=optional) else: x = _type_katra(typed=typed, default=default, optional=optional) return x Variables minor Functions katra def katra ( typed , default = None ) Public interface to create a katra A 'katra' is the basic functional unit of spock . It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality Args : typed: the type of the parameter to define default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) Returns: Type Description x Attribute from attrs ??? example \"View Source\" def katra(typed, default=None): \"\"\"Public interface to create a katra A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality *Args*: typed: the type of the parameter to define default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) Returns: x: Attribute from attrs \"\"\" # Handle optionals typed, optional = _handle_optional_typing(typed) # Check generic types for consistent types _check_generic_recursive_single_type(typed) # We need to check if the type is a _GenericAlias so that we can handle subscripted general types # If it is subscript typed it will not be T which python uses as a generic type name if isinstance(typed, _GenericAlias) and ( not isinstance(typed.__args__[0], TypeVar) ): x = _generic_alias_katra(typed=typed, default=default, optional=optional) elif isinstance(typed, EnumMeta): x = _enum_katra(typed=typed, default=default, optional=optional) else: x = _type_katra(typed=typed, default=default, optional=optional) return x Classes SavePath class SavePath ( / , * args , ** kwargs ) ??? example \"View Source\" class SavePath(str): \"\"\"Spock special key for saving the Spock config to file Defines a special key use to save the current Spock config to file \"\"\" def __new__(cls, x): return super().__new__(cls, x) Ancestors (in MRO) builtins.str Static methods maketrans def maketrans ( ... ) Return a translation table usable for str.translate(). If there is only one argument, it must be a dictionary mapping Unicode ordinals (integers) or characters to Unicode ordinals, strings or None. Character keys will be then converted to ordinals. If there are two arguments, they must be strings of equal length, and in the resulting dictionary, each character in x will be mapped to the character at the same position in y. If there is a third argument, it must be a string, whose characters will be mapped to None in the result. Methods capitalize def capitalize ( self , / ) Return a capitalized version of the string. More specifically, make the first character have upper case and the rest lower case. casefold def casefold ( self , / ) Return a version of the string suitable for caseless comparisons. center def center ( self , width , fillchar = ' ' , / ) Return a centered string of length width. Padding is done using the specified fill character (default is a space). count def count ( ... ) S.count(sub[, start[, end]]) -> int Return the number of non-overlapping occurrences of substring sub in string S[start:end]. Optional arguments start and end are interpreted as in slice notation. encode def encode ( self , / , encoding = 'utf-8' , errors = 'strict' ) Encode the string using the codec registered for encoding. encoding The encoding in which to encode the string. errors The error handling scheme to use for encoding errors. The default is 'strict' meaning that encoding errors raise a UnicodeEncodeError. Other possible values are 'ignore', 'replace' and 'xmlcharrefreplace' as well as any other name registered with codecs.register_error that can handle UnicodeEncodeErrors. endswith def endswith ( ... ) S.endswith(suffix[, start[, end]]) -> bool Return True if S ends with the specified suffix, False otherwise. With optional start, test S beginning at that position. With optional end, stop comparing S at that position. suffix can also be a tuple of strings to try. expandtabs def expandtabs ( self , / , tabsize = 8 ) Return a copy where all tab characters are expanded using spaces. If tabsize is not given, a tab size of 8 characters is assumed. find def find ( ... ) S.find(sub[, start[, end]]) -> int Return the lowest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure. format def format ( ... ) S.format( args, *kwargs) -> str Return a formatted version of S, using substitutions from args and kwargs. The substitutions are identified by braces ('{' and '}'). format_map def format_map ( ... ) S.format_map(mapping) -> str Return a formatted version of S, using substitutions from mapping. The substitutions are identified by braces ('{' and '}'). index def index ( ... ) S.index(sub[, start[, end]]) -> int Return the lowest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Raises ValueError when the substring is not found. isalnum def isalnum ( self , / ) Return True if the string is an alpha-numeric string, False otherwise. A string is alpha-numeric if all characters in the string are alpha-numeric and there is at least one character in the string. isalpha def isalpha ( self , / ) Return True if the string is an alphabetic string, False otherwise. A string is alphabetic if all characters in the string are alphabetic and there is at least one character in the string. isascii def isascii ( self , / ) Return True if all characters in the string are ASCII, False otherwise. ASCII characters have code points in the range U+0000-U+007F. Empty string is ASCII too. isdecimal def isdecimal ( self , / ) Return True if the string is a decimal string, False otherwise. A string is a decimal string if all characters in the string are decimal and there is at least one character in the string. isdigit def isdigit ( self , / ) Return True if the string is a digit string, False otherwise. A string is a digit string if all characters in the string are digits and there is at least one character in the string. isidentifier def isidentifier ( self , / ) Return True if the string is a valid Python identifier, False otherwise. Call keyword.iskeyword(s) to test whether string s is a reserved identifier, such as \"def\" or \"class\". islower def islower ( self , / ) Return True if the string is a lowercase string, False otherwise. A string is lowercase if all cased characters in the string are lowercase and there is at least one cased character in the string. isnumeric def isnumeric ( self , / ) Return True if the string is a numeric string, False otherwise. A string is numeric if all characters in the string are numeric and there is at least one character in the string. isprintable def isprintable ( self , / ) Return True if the string is printable, False otherwise. A string is printable if all of its characters are considered printable in repr() or if it is empty. isspace def isspace ( self , / ) Return True if the string is a whitespace string, False otherwise. A string is whitespace if all characters in the string are whitespace and there is at least one character in the string. istitle def istitle ( self , / ) Return True if the string is a title-cased string, False otherwise. In a title-cased string, upper- and title-case characters may only follow uncased characters and lowercase characters only cased ones. isupper def isupper ( self , / ) Return True if the string is an uppercase string, False otherwise. A string is uppercase if all cased characters in the string are uppercase and there is at least one cased character in the string. join def join ( self , iterable , / ) Concatenate any number of strings. The string whose method is called is inserted in between each given string. The result is returned as a new string. Example: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs' ljust def ljust ( self , width , fillchar = ' ' , / ) Return a left-justified string of length width. Padding is done using the specified fill character (default is a space). lower def lower ( self , / ) Return a copy of the string converted to lowercase. lstrip def lstrip ( self , chars = None , / ) Return a copy of the string with leading whitespace removed. If chars is given and not None, remove characters in chars instead. partition def partition ( self , sep , / ) Partition the string into three parts using the given separator. This will search for the separator in the string. If the separator is found, returns a 3-tuple containing the part before the separator, the separator itself, and the part after it. If the separator is not found, returns a 3-tuple containing the original string and two empty strings. replace def replace ( self , old , new , count =- 1 , / ) Return a copy with all occurrences of substring old replaced by new. count Maximum number of occurrences to replace. -1 (the default value) means replace all occurrences. If the optional argument count is given, only the first count occurrences are replaced. rfind def rfind ( ... ) S.rfind(sub[, start[, end]]) -> int Return the highest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure. rindex def rindex ( ... ) S.rindex(sub[, start[, end]]) -> int Return the highest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Raises ValueError when the substring is not found. rjust def rjust ( self , width , fillchar = ' ' , / ) Return a right-justified string of length width. Padding is done using the specified fill character (default is a space). rpartition def rpartition ( self , sep , / ) Partition the string into three parts using the given separator. This will search for the separator in the string, starting at the end. If the separator is found, returns a 3-tuple containing the part before the separator, the separator itself, and the part after it. If the separator is not found, returns a 3-tuple containing two empty strings and the original string. rsplit def rsplit ( self , / , sep = None , maxsplit =- 1 ) Return a list of the words in the string, using sep as the delimiter string. sep The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit Maximum number of splits to do. -1 (the default value) means no limit. Splits are done starting at the end of the string and working to the front. rstrip def rstrip ( self , chars = None , / ) Return a copy of the string with trailing whitespace removed. If chars is given and not None, remove characters in chars instead. split def split ( self , / , sep = None , maxsplit =- 1 ) Return a list of the words in the string, using sep as the delimiter string. sep The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit Maximum number of splits to do. -1 (the default value) means no limit. splitlines def splitlines ( self , / , keepends = False ) Return a list of the lines in the string, breaking at line boundaries. Line breaks are not included in the resulting list unless keepends is given and true. startswith def startswith ( ... ) S.startswith(prefix[, start[, end]]) -> bool Return True if S starts with the specified prefix, False otherwise. With optional start, test S beginning at that position. With optional end, stop comparing S at that position. prefix can also be a tuple of strings to try. strip def strip ( self , chars = None , / ) Return a copy of the string with leading and trailing whitespace removed. If chars is given and not None, remove characters in chars instead. swapcase def swapcase ( self , / ) Convert uppercase characters to lowercase and lowercase characters to uppercase. title def title ( self , / ) Return a version of the string where each word is titlecased. More specifically, words start with uppercased characters and all remaining cased characters have lower case. translate def translate ( self , table , / ) Replace each character in the string using the given translation table. table Translation table, which must be a mapping of Unicode ordinals to Unicode ordinals, strings, or None. The table must implement lookup/indexing via getitem , for instance a dictionary or list. If this operation raises LookupError, the character is left untouched. Characters mapped to None are deleted. upper def upper ( self , / ) Return a copy of the string converted to uppercase. zfill def zfill ( self , width , / ) Pad a numeric string with zeros on the left, to fill a field of the given width. The string is never truncated.","title":"Typed"},{"location":"reference/spock/backend/typed/#module-spockbackendtyped","text":"Handles the definitions of arguments types for Spock (backend: attrs) None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles the definitions of arguments types for Spock (backend: attrs)\"\"\" import sys from enum import EnumMeta from functools import partial from typing import TypeVar, Union import attr minor = sys.version_info.minor if minor < 7: from typing import GenericMeta as _GenericAlias else: from typing import _GenericAlias class SavePath(str): \"\"\"Spock special key for saving the Spock config to file Defines a special key use to save the current Spock config to file \"\"\" def __new__(cls, x): return super().__new__(cls, x) def _get_name_py_version(typed): \"\"\"Gets the name of the type depending on the python version *Args*: typed: the type of the parameter *Returns*: name of the type \"\"\" return typed._name if hasattr(typed, \"_name\") else typed.__name__ def _extract_base_type(typed): \"\"\"Extracts the name of the type from a _GenericAlias Assumes that the derived types are only of length 1 as the __args__ are [0] recursed... this is not true for tuples *Args*: typed: the type of the parameter *Returns*: name of type \"\"\" if hasattr(typed, \"__args__\"): name = _get_name_py_version(typed=typed) bracket_val = f\"{name}[{_extract_base_type(typed.__args__[0])}]\" return bracket_val else: bracket_value = typed.__name__ return bracket_value def _recursive_generic_validator(typed): \"\"\"Recursively assembles the validators for nested generic types Walks through the nested type structure and determines whether to recurse all the way to a base type. Once it hits the base type it bubbles up the correct validator that is nested within the upper validator *Args*: typed: input type *Returns*: return_type: recursively built deep_iterable validators \"\"\" if hasattr(typed, \"__args__\"): # If there are more __args__ then we still need to recurse as it is still a GenericAlias # Iterate through since there might be multiple types? if len(typed.__args__) > 1: return_type = attr.validators.deep_iterable( member_validator=_recursive_generic_validator(typed.__args__), iterable_validator=attr.validators.instance_of(typed.__origin__), ) else: return_type = attr.validators.deep_iterable( member_validator=_recursive_generic_validator(typed.__args__[0]), iterable_validator=attr.validators.instance_of(typed.__origin__), ) return return_type else: # If no more __args__ then we are to the base type and need to bubble up the type # But we need to check against base types and enums if isinstance(typed, EnumMeta): base_type, allowed = _check_enum_props(typed) return_type = attr.validators.and_( attr.validators.instance_of(base_type), attr.validators.in_(allowed) ) else: return_type = attr.validators.instance_of(typed) return return_type def _generic_alias_katra(typed, default=None, optional=False): \"\"\"Private interface to create a subscripted generic_alias katra A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality Handles: List[type] and Tuple[type] *Args*: typed: the type of the parameter to define default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) *Returns*: x: Attribute from attrs \"\"\" # base python class from which a GenericAlias is derived base_typed = typed.__origin__ if default is not None: x = attr.ib( validator=_recursive_generic_validator(typed), default=default, type=base_typed, metadata={\"base\": _extract_base_type(typed), \"type\": typed}, ) # x = attr.ib(validator=_recursive_generic_iterator(typed), default=default, type=base_typed, # metadata={'base': _extract_base_type(typed)}) elif optional: # if there's no default, but marked as optional, then set the default to None x = attr.ib( validator=attr.validators.optional(_recursive_generic_validator(typed)), type=base_typed, default=default, metadata={ \"optional\": True, \"base\": _extract_base_type(typed), \"type\": typed, }, ) # x = attr.ib(validator=attr.validators.optional(_recursive_generic_iterator(typed)), type=base_typed, # default=default, metadata={'optional': True, 'base': _extract_base_type(typed)}) else: x = attr.ib( validator=_recursive_generic_validator(typed), type=base_typed, metadata={\"base\": _extract_base_type(typed), \"type\": typed}, ) # x = attr.ib(validator=_recursive_generic_iterator(typed), type=base_typed, # metadata={'base': _extract_base_type(typed)}) return x def _check_enum_props(typed): \"\"\"Handles properties of enums Checks if all types of the enum are the same and assembles a list of allowed values *Args*: typed: the type of parameter (Enum) *Returns*: base_type: the base type of the Enum allowed: List of allowed values of the Enum \"\"\" # First check if the types of Enum are the same type_set = {type(val.value) for val in typed} if len(type_set) > 1: raise TypeError(f\"Enum cannot be defined with multiple types: {type_set}\") base_type = list(type_set)[-1] allowed = [val.value for val in typed] return base_type, allowed def _enum_katra(typed, default=None, optional=False): \"\"\"Private interface to create a Enum typed katra A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality *Args*: typed: the type of the parameter to define default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) *Returns*: x: Attribute from attrs \"\"\" # First check if the types of Enum are the same base_type, allowed = _check_enum_props(typed) if base_type.__name__ == \"type\": x = _enum_class_katra( typed=typed, allowed=allowed, default=default, optional=optional ) else: x = _enum_base_katra( typed=typed, base_type=base_type, allowed=allowed, default=default, optional=optional, ) return x def _enum_base_katra(typed, base_type, allowed, default=None, optional=False): \"\"\"Private interface to create a base Enum typed katra Here we handle the base types of enums that allows us to force a type check on the instance A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality *Args*: typed: the type of the parameter to define base_type: underlying base type allowed: set of allowed values default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) *Returns*: x: Attribute from attrs \"\"\" if default is not None: x = attr.ib( validator=[ attr.validators.instance_of(base_type), attr.validators.in_(allowed), ], default=default, type=typed, metadata={\"base\": typed.__name__}, ) elif optional: x = attr.ib( validator=attr.validators.optional( [attr.validators.instance_of(base_type), attr.validators.in_(allowed)] ), default=default, type=typed, metadata={\"base\": typed.__name__, \"optional\": True}, ) else: x = attr.ib( validator=[ attr.validators.instance_of(base_type), attr.validators.in_(allowed), ], type=typed, metadata={\"base\": typed.__name__}, ) return x def _in_type(instance, attribute, value, options): \"\"\"attrs validator for class type enum Checks if the type of the class (e.g. value) is in the specified set of types provided *Args*: instance: current object instance attribute: current attribute instance value: current value trying to be set in the attrs instance options: list, tuple, or enum of allowed options *Returns*: \"\"\" if type(value) not in options: raise ValueError(f\"{attribute.name} must be in {options}\") def _enum_class_katra(typed, allowed, default=None, optional=False): \"\"\"Private interface to create a base Enum typed katra Here we handle the class based types of enums. Seeing as these classes are generated dynamically we cannot force type checking of a specific instance however the in_ validator will catch an incorrect instance type A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality *Args*: typed: the type of the parameter to define allowed: set of allowed values default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) *Returns*: x: Attribute from attrs \"\"\" if default is not None: x = attr.ib( validator=[partial(_in_type, options=allowed)], default=default, type=typed, metadata={\"base\": typed.__name__}, ) elif optional: x = attr.ib( validator=attr.validators.optional([partial(_in_type, options=allowed)]), default=default, type=typed, metadata={\"base\": typed.__name__, \"optional\": True}, ) else: x = attr.ib( validator=[partial(_in_type, options=allowed)], type=typed, metadata={\"base\": typed.__name__}, ) return x def _type_katra(typed, default=None, optional=False): \"\"\"Private interface to create a simple typed katra A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality Handles: bool, string, float, int, List, and Tuple *Args*: typed: the type of the parameter to define default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) *Returns*: x: Attribute from attrs \"\"\" # Grab the name first based on if it is a base type or GenericAlias if isinstance(typed, type): name = typed.__name__ elif isinstance(typed, _GenericAlias): name = _get_name_py_version(typed=typed) else: raise TypeError(\"Encountered an unexpected type in _type_katra\") special_key = None # Default booleans to false and optional due to the nature of a boolean if isinstance(typed, type) and name == \"bool\": optional = True if default is not True: default = False # For the save path type we need to swap the type back to it's base class (str) elif isinstance(typed, type) and name == \"SavePath\": optional = True special_key = name typed = str if default is not None: # if a default is provided, that takes precedence x = attr.ib( validator=attr.validators.instance_of(typed), default=default, type=typed, metadata={\"base\": name, \"special_key\": special_key}, ) elif optional: x = attr.ib( validator=attr.validators.optional(attr.validators.instance_of(typed)), default=default, type=typed, metadata={\"optional\": True, \"base\": name, \"special_key\": special_key}, ) else: x = attr.ib( validator=attr.validators.instance_of(typed), type=typed, metadata={\"base\": name, \"special_key\": special_key}, ) return x def _handle_optional_typing(typed): \"\"\"Handles when a type hint is Optional Handles Optional[type] typing and strips out the base type to pass back to the creation of a katra which needs base typing *Args*: typed: type *Returns*: typed: type (modified if Optional) optional: boolean for katra creation \"\"\" # Set optional to false optional = False # Check if it has __args__ to look for optionality as it is a GenericAlias if hasattr(typed, \"__args__\"): # If it is more than one than it is most likely optional but check against NoneType in the tuple to verify # Check the length of type __args__ type_args = typed.__args__ # Optional[X] has type_args = (X, None) and is equal to Union[X, None] if (len(type_args) == 2) and (typed == Union[type_args[0], None]): typed = type_args[0] # Set the optional flag to true optional = True return typed, optional def _check_generic_recursive_single_type(typed): \"\"\"Checks generics for the single types -- mixed types of generics are not allowed DEPRECATED -- NOW SUPPORTS MIXED TYPES OF TUPLES *Args*: typed: type *Returns*: \"\"\" # Check if it has __args__ to look for optionality as it is a GenericAlias # if hasattr(typed, '__args__'): # if len(set(typed.__args__)) > 1: # type_list = [str(val) for val in typed.__args__] # raise TypeError(f\"Passing multiple different subscript types to GenericAlias is not supported: {type_list}\") # else: # for val in typed.__args__: # _check_generic_recursive_single_type(typed=val) pass def katra(typed, default=None): \"\"\"Public interface to create a katra A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality *Args*: typed: the type of the parameter to define default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) Returns: x: Attribute from attrs \"\"\" # Handle optionals typed, optional = _handle_optional_typing(typed) # Check generic types for consistent types _check_generic_recursive_single_type(typed) # We need to check if the type is a _GenericAlias so that we can handle subscripted general types # If it is subscript typed it will not be T which python uses as a generic type name if isinstance(typed, _GenericAlias) and ( not isinstance(typed.__args__[0], TypeVar) ): x = _generic_alias_katra(typed=typed, default=default, optional=optional) elif isinstance(typed, EnumMeta): x = _enum_katra(typed=typed, default=default, optional=optional) else: x = _type_katra(typed=typed, default=default, optional=optional) return x","title":"Module spock.backend.typed"},{"location":"reference/spock/backend/typed/#variables","text":"minor","title":"Variables"},{"location":"reference/spock/backend/typed/#functions","text":"","title":"Functions"},{"location":"reference/spock/backend/typed/#katra","text":"def katra ( typed , default = None ) Public interface to create a katra A 'katra' is the basic functional unit of spock . It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality Args : typed: the type of the parameter to define default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) Returns: Type Description x Attribute from attrs ??? example \"View Source\" def katra(typed, default=None): \"\"\"Public interface to create a katra A 'katra' is the basic functional unit of `spock`. It defines a parameter using attrs as the backend, type checks both simple types and subscripted GenericAlias types (e.g. lists and tuples), handles setting default parameters, and deals with parameter optionality *Args*: typed: the type of the parameter to define default: the default value to assign if given optional: whether to make the parameter optional or not (thus allowing None) Returns: x: Attribute from attrs \"\"\" # Handle optionals typed, optional = _handle_optional_typing(typed) # Check generic types for consistent types _check_generic_recursive_single_type(typed) # We need to check if the type is a _GenericAlias so that we can handle subscripted general types # If it is subscript typed it will not be T which python uses as a generic type name if isinstance(typed, _GenericAlias) and ( not isinstance(typed.__args__[0], TypeVar) ): x = _generic_alias_katra(typed=typed, default=default, optional=optional) elif isinstance(typed, EnumMeta): x = _enum_katra(typed=typed, default=default, optional=optional) else: x = _type_katra(typed=typed, default=default, optional=optional) return x","title":"katra"},{"location":"reference/spock/backend/typed/#classes","text":"","title":"Classes"},{"location":"reference/spock/backend/typed/#savepath","text":"class SavePath ( / , * args , ** kwargs ) ??? example \"View Source\" class SavePath(str): \"\"\"Spock special key for saving the Spock config to file Defines a special key use to save the current Spock config to file \"\"\" def __new__(cls, x): return super().__new__(cls, x)","title":"SavePath"},{"location":"reference/spock/backend/typed/#ancestors-in-mro","text":"builtins.str","title":"Ancestors (in MRO)"},{"location":"reference/spock/backend/typed/#static-methods","text":"","title":"Static methods"},{"location":"reference/spock/backend/typed/#maketrans","text":"def maketrans ( ... ) Return a translation table usable for str.translate(). If there is only one argument, it must be a dictionary mapping Unicode ordinals (integers) or characters to Unicode ordinals, strings or None. Character keys will be then converted to ordinals. If there are two arguments, they must be strings of equal length, and in the resulting dictionary, each character in x will be mapped to the character at the same position in y. If there is a third argument, it must be a string, whose characters will be mapped to None in the result.","title":"maketrans"},{"location":"reference/spock/backend/typed/#methods","text":"","title":"Methods"},{"location":"reference/spock/backend/typed/#capitalize","text":"def capitalize ( self , / ) Return a capitalized version of the string. More specifically, make the first character have upper case and the rest lower case.","title":"capitalize"},{"location":"reference/spock/backend/typed/#casefold","text":"def casefold ( self , / ) Return a version of the string suitable for caseless comparisons.","title":"casefold"},{"location":"reference/spock/backend/typed/#center","text":"def center ( self , width , fillchar = ' ' , / ) Return a centered string of length width. Padding is done using the specified fill character (default is a space).","title":"center"},{"location":"reference/spock/backend/typed/#count","text":"def count ( ... ) S.count(sub[, start[, end]]) -> int Return the number of non-overlapping occurrences of substring sub in string S[start:end]. Optional arguments start and end are interpreted as in slice notation.","title":"count"},{"location":"reference/spock/backend/typed/#encode","text":"def encode ( self , / , encoding = 'utf-8' , errors = 'strict' ) Encode the string using the codec registered for encoding. encoding The encoding in which to encode the string. errors The error handling scheme to use for encoding errors. The default is 'strict' meaning that encoding errors raise a UnicodeEncodeError. Other possible values are 'ignore', 'replace' and 'xmlcharrefreplace' as well as any other name registered with codecs.register_error that can handle UnicodeEncodeErrors.","title":"encode"},{"location":"reference/spock/backend/typed/#endswith","text":"def endswith ( ... ) S.endswith(suffix[, start[, end]]) -> bool Return True if S ends with the specified suffix, False otherwise. With optional start, test S beginning at that position. With optional end, stop comparing S at that position. suffix can also be a tuple of strings to try.","title":"endswith"},{"location":"reference/spock/backend/typed/#expandtabs","text":"def expandtabs ( self , / , tabsize = 8 ) Return a copy where all tab characters are expanded using spaces. If tabsize is not given, a tab size of 8 characters is assumed.","title":"expandtabs"},{"location":"reference/spock/backend/typed/#find","text":"def find ( ... ) S.find(sub[, start[, end]]) -> int Return the lowest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure.","title":"find"},{"location":"reference/spock/backend/typed/#format","text":"def format ( ... ) S.format( args, *kwargs) -> str Return a formatted version of S, using substitutions from args and kwargs. The substitutions are identified by braces ('{' and '}').","title":"format"},{"location":"reference/spock/backend/typed/#format_map","text":"def format_map ( ... ) S.format_map(mapping) -> str Return a formatted version of S, using substitutions from mapping. The substitutions are identified by braces ('{' and '}').","title":"format_map"},{"location":"reference/spock/backend/typed/#index","text":"def index ( ... ) S.index(sub[, start[, end]]) -> int Return the lowest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Raises ValueError when the substring is not found.","title":"index"},{"location":"reference/spock/backend/typed/#isalnum","text":"def isalnum ( self , / ) Return True if the string is an alpha-numeric string, False otherwise. A string is alpha-numeric if all characters in the string are alpha-numeric and there is at least one character in the string.","title":"isalnum"},{"location":"reference/spock/backend/typed/#isalpha","text":"def isalpha ( self , / ) Return True if the string is an alphabetic string, False otherwise. A string is alphabetic if all characters in the string are alphabetic and there is at least one character in the string.","title":"isalpha"},{"location":"reference/spock/backend/typed/#isascii","text":"def isascii ( self , / ) Return True if all characters in the string are ASCII, False otherwise. ASCII characters have code points in the range U+0000-U+007F. Empty string is ASCII too.","title":"isascii"},{"location":"reference/spock/backend/typed/#isdecimal","text":"def isdecimal ( self , / ) Return True if the string is a decimal string, False otherwise. A string is a decimal string if all characters in the string are decimal and there is at least one character in the string.","title":"isdecimal"},{"location":"reference/spock/backend/typed/#isdigit","text":"def isdigit ( self , / ) Return True if the string is a digit string, False otherwise. A string is a digit string if all characters in the string are digits and there is at least one character in the string.","title":"isdigit"},{"location":"reference/spock/backend/typed/#isidentifier","text":"def isidentifier ( self , / ) Return True if the string is a valid Python identifier, False otherwise. Call keyword.iskeyword(s) to test whether string s is a reserved identifier, such as \"def\" or \"class\".","title":"isidentifier"},{"location":"reference/spock/backend/typed/#islower","text":"def islower ( self , / ) Return True if the string is a lowercase string, False otherwise. A string is lowercase if all cased characters in the string are lowercase and there is at least one cased character in the string.","title":"islower"},{"location":"reference/spock/backend/typed/#isnumeric","text":"def isnumeric ( self , / ) Return True if the string is a numeric string, False otherwise. A string is numeric if all characters in the string are numeric and there is at least one character in the string.","title":"isnumeric"},{"location":"reference/spock/backend/typed/#isprintable","text":"def isprintable ( self , / ) Return True if the string is printable, False otherwise. A string is printable if all of its characters are considered printable in repr() or if it is empty.","title":"isprintable"},{"location":"reference/spock/backend/typed/#isspace","text":"def isspace ( self , / ) Return True if the string is a whitespace string, False otherwise. A string is whitespace if all characters in the string are whitespace and there is at least one character in the string.","title":"isspace"},{"location":"reference/spock/backend/typed/#istitle","text":"def istitle ( self , / ) Return True if the string is a title-cased string, False otherwise. In a title-cased string, upper- and title-case characters may only follow uncased characters and lowercase characters only cased ones.","title":"istitle"},{"location":"reference/spock/backend/typed/#isupper","text":"def isupper ( self , / ) Return True if the string is an uppercase string, False otherwise. A string is uppercase if all cased characters in the string are uppercase and there is at least one cased character in the string.","title":"isupper"},{"location":"reference/spock/backend/typed/#join","text":"def join ( self , iterable , / ) Concatenate any number of strings. The string whose method is called is inserted in between each given string. The result is returned as a new string. Example: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs'","title":"join"},{"location":"reference/spock/backend/typed/#ljust","text":"def ljust ( self , width , fillchar = ' ' , / ) Return a left-justified string of length width. Padding is done using the specified fill character (default is a space).","title":"ljust"},{"location":"reference/spock/backend/typed/#lower","text":"def lower ( self , / ) Return a copy of the string converted to lowercase.","title":"lower"},{"location":"reference/spock/backend/typed/#lstrip","text":"def lstrip ( self , chars = None , / ) Return a copy of the string with leading whitespace removed. If chars is given and not None, remove characters in chars instead.","title":"lstrip"},{"location":"reference/spock/backend/typed/#partition","text":"def partition ( self , sep , / ) Partition the string into three parts using the given separator. This will search for the separator in the string. If the separator is found, returns a 3-tuple containing the part before the separator, the separator itself, and the part after it. If the separator is not found, returns a 3-tuple containing the original string and two empty strings.","title":"partition"},{"location":"reference/spock/backend/typed/#replace","text":"def replace ( self , old , new , count =- 1 , / ) Return a copy with all occurrences of substring old replaced by new. count Maximum number of occurrences to replace. -1 (the default value) means replace all occurrences. If the optional argument count is given, only the first count occurrences are replaced.","title":"replace"},{"location":"reference/spock/backend/typed/#rfind","text":"def rfind ( ... ) S.rfind(sub[, start[, end]]) -> int Return the highest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure.","title":"rfind"},{"location":"reference/spock/backend/typed/#rindex","text":"def rindex ( ... ) S.rindex(sub[, start[, end]]) -> int Return the highest index in S where substring sub is found, such that sub is contained within S[start:end]. Optional arguments start and end are interpreted as in slice notation. Raises ValueError when the substring is not found.","title":"rindex"},{"location":"reference/spock/backend/typed/#rjust","text":"def rjust ( self , width , fillchar = ' ' , / ) Return a right-justified string of length width. Padding is done using the specified fill character (default is a space).","title":"rjust"},{"location":"reference/spock/backend/typed/#rpartition","text":"def rpartition ( self , sep , / ) Partition the string into three parts using the given separator. This will search for the separator in the string, starting at the end. If the separator is found, returns a 3-tuple containing the part before the separator, the separator itself, and the part after it. If the separator is not found, returns a 3-tuple containing two empty strings and the original string.","title":"rpartition"},{"location":"reference/spock/backend/typed/#rsplit","text":"def rsplit ( self , / , sep = None , maxsplit =- 1 ) Return a list of the words in the string, using sep as the delimiter string. sep The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit Maximum number of splits to do. -1 (the default value) means no limit. Splits are done starting at the end of the string and working to the front.","title":"rsplit"},{"location":"reference/spock/backend/typed/#rstrip","text":"def rstrip ( self , chars = None , / ) Return a copy of the string with trailing whitespace removed. If chars is given and not None, remove characters in chars instead.","title":"rstrip"},{"location":"reference/spock/backend/typed/#split","text":"def split ( self , / , sep = None , maxsplit =- 1 ) Return a list of the words in the string, using sep as the delimiter string. sep The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit Maximum number of splits to do. -1 (the default value) means no limit.","title":"split"},{"location":"reference/spock/backend/typed/#splitlines","text":"def splitlines ( self , / , keepends = False ) Return a list of the lines in the string, breaking at line boundaries. Line breaks are not included in the resulting list unless keepends is given and true.","title":"splitlines"},{"location":"reference/spock/backend/typed/#startswith","text":"def startswith ( ... ) S.startswith(prefix[, start[, end]]) -> bool Return True if S starts with the specified prefix, False otherwise. With optional start, test S beginning at that position. With optional end, stop comparing S at that position. prefix can also be a tuple of strings to try.","title":"startswith"},{"location":"reference/spock/backend/typed/#strip","text":"def strip ( self , chars = None , / ) Return a copy of the string with leading and trailing whitespace removed. If chars is given and not None, remove characters in chars instead.","title":"strip"},{"location":"reference/spock/backend/typed/#swapcase","text":"def swapcase ( self , / ) Convert uppercase characters to lowercase and lowercase characters to uppercase.","title":"swapcase"},{"location":"reference/spock/backend/typed/#title","text":"def title ( self , / ) Return a version of the string where each word is titlecased. More specifically, words start with uppercased characters and all remaining cased characters have lower case.","title":"title"},{"location":"reference/spock/backend/typed/#translate","text":"def translate ( self , table , / ) Replace each character in the string using the given translation table. table Translation table, which must be a mapping of Unicode ordinals to Unicode ordinals, strings, or None. The table must implement lookup/indexing via getitem , for instance a dictionary or list. If this operation raises LookupError, the character is left untouched. Characters mapped to None are deleted.","title":"translate"},{"location":"reference/spock/backend/typed/#upper","text":"def upper ( self , / ) Return a copy of the string converted to uppercase.","title":"upper"},{"location":"reference/spock/backend/typed/#zfill","text":"def zfill ( self , width , / ) Pad a numeric string with zeros on the left, to fill a field of the given width. The string is never truncated.","title":"zfill"},{"location":"reference/spock/backend/utils/","text":"Module spock.backend.utils Attr utility functions for Spock None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Attr utility functions for Spock\"\"\" def get_attr_fields(input_classes): \"\"\"Gets the attribute fields from all classes *Args*: input_classes: current list of input classes *Returns*: dictionary of all attrs attribute fields \"\"\" return { attr.__name__: [val.name for val in attr.__attrs_attrs__] for attr in input_classes } def get_type_fields(input_classes): \"\"\"Creates a dictionary of names and types *Args*: input_classes: list of input classes *Returns*: type_fields: dictionary of names and types \"\"\" # Parse out the types if generic type_fields = {} for attr in input_classes: input_attr = {} for val in attr.__attrs_attrs__: if \"type\" in val.metadata: input_attr.update({val.name: val.metadata[\"type\"]}) else: input_attr.update({val.name: None}) type_fields.update({attr.__name__: input_attr}) return type_fields def flatten_type_dict(type_dict): \"\"\"Flattens a nested dictionary *Args*: type_dict: dictionary of types that are generic *Returns*: flat_dict: flatten dictionary to a single level \"\"\" flat_dict = {} for k, v in type_dict.items(): if isinstance(v, dict): return_dict = flatten_type_dict(v) flat_dict.update(return_dict) else: flat_dict[k] = v return flat_dict def convert_to_tuples(input_dict, named_type_dict, class_names): \"\"\"Convert lists to tuples Payloads from markup come in as Lists and not Tuples. This function turns lists in to tuples for the payloads so the attr values are set correctly. Will call itself recursively if a dictionary is present for class specific values *Args*: input_dict: input dictionary named_type_dict: dictionary of names with generic types *Returns*: updated_dict: a dictionary with lists converted to tuples \"\"\" updated_dict = {} all_typed_dict = flatten_type_dict(named_type_dict) for k, v in input_dict.items(): if k != \"config\": if isinstance(v, dict): updated = convert_to_tuples(v, named_type_dict.get(k), class_names) if updated: updated_dict.update({k: updated}) elif isinstance(v, list) and k in class_names: for val in v: updated = convert_to_tuples( val, named_type_dict.get(k), class_names ) if updated: updated_dict.update({k: updated}) elif all_typed_dict[k] is not None: updated = _recursive_list_to_tuple(v, all_typed_dict[k], class_names) updated_dict.update({k: updated}) return updated_dict def deep_update(source, updates): \"\"\"Deeply updates a dictionary Iterates through a dictionary recursively to update individual values within a possibly nested dictionary of dictionaries *Args*: source: source dictionary updates: updates to the dictionary *Returns*: source: updated version of the source dictionary \"\"\" for k, v in updates.items(): if isinstance(v, dict) and v: updated_dict = deep_update(source.get(k), v) if updated_dict: source[k] = updated_dict else: source[k] = v return source def _recursive_list_to_tuple(value, typed, class_names): \"\"\"Recursively turn lists into tuples Recursively looks through a pair of value and type and sets any of the possibly nested type of value to tuple if tuple is the specified type *Args*: value: value to check and set typ if necessary typed: type of the generic alias to check against *Returns*: value: updated value with correct type casts \"\"\" # Check for __args__ as it signifies a generic and make sure it's not already been cast as a tuple # from a composed payload if ( hasattr(typed, \"__args__\") and not isinstance(value, tuple) and not (isinstance(value, str) and value in class_names) ): # Force those with origin tuple types to be of the defined length if (typed.__origin__.__name__.lower() == \"tuple\") and len(value) != len( typed.__args__ ): raise ValueError( f\"Tuple(s) use a fixed/defined length -- Length of the provided argument ({len(value)}) \" f\"does not match the length of the defined argument ({len(typed.__args__)})\" ) # need to recurse before casting as we can't set values in a tuple with idx # Since it's generic it should be iterable to recurse and check it's children for idx, val in enumerate(value): value[idx] = _recursive_list_to_tuple(val, typed.__args__[0], class_names) # First check if list and then swap to tuple if the origin is tuple if isinstance(value, list) and typed.__origin__.__name__.lower() == \"tuple\": value = tuple(value) else: return value return value Functions convert_to_tuples def convert_to_tuples ( input_dict , named_type_dict , class_names ) Convert lists to tuples Payloads from markup come in as Lists and not Tuples. This function turns lists in to tuples for the payloads so the attr values are set correctly. Will call itself recursively if a dictionary is present for class specific values Args : input_dict: input dictionary named_type_dict: dictionary of names with generic types Returns : updated_dict: a dictionary with lists converted to tuples ??? example \"View Source\" def convert_to_tuples(input_dict, named_type_dict, class_names): \"\"\"Convert lists to tuples Payloads from markup come in as Lists and not Tuples. This function turns lists in to tuples for the payloads so the attr values are set correctly. Will call itself recursively if a dictionary is present for class specific values *Args*: input_dict: input dictionary named_type_dict: dictionary of names with generic types *Returns*: updated_dict: a dictionary with lists converted to tuples \"\"\" updated_dict = {} all_typed_dict = flatten_type_dict(named_type_dict) for k, v in input_dict.items(): if k != \"config\": if isinstance(v, dict): updated = convert_to_tuples(v, named_type_dict.get(k), class_names) if updated: updated_dict.update({k: updated}) elif isinstance(v, list) and k in class_names: for val in v: updated = convert_to_tuples( val, named_type_dict.get(k), class_names ) if updated: updated_dict.update({k: updated}) elif all_typed_dict[k] is not None: updated = _recursive_list_to_tuple(v, all_typed_dict[k], class_names) updated_dict.update({k: updated}) return updated_dict deep_update def deep_update ( source , updates ) Deeply updates a dictionary Iterates through a dictionary recursively to update individual values within a possibly nested dictionary of dictionaries Args : source: source dictionary updates: updates to the dictionary Returns : source: updated version of the source dictionary ??? example \"View Source\" def deep_update(source, updates): \"\"\"Deeply updates a dictionary Iterates through a dictionary recursively to update individual values within a possibly nested dictionary of dictionaries *Args*: source: source dictionary updates: updates to the dictionary *Returns*: source: updated version of the source dictionary \"\"\" for k, v in updates.items(): if isinstance(v, dict) and v: updated_dict = deep_update(source.get(k), v) if updated_dict: source[k] = updated_dict else: source[k] = v return source flatten_type_dict def flatten_type_dict ( type_dict ) Flattens a nested dictionary Args : type_dict: dictionary of types that are generic Returns : flat_dict: flatten dictionary to a single level ??? example \"View Source\" def flatten_type_dict(type_dict): \"\"\"Flattens a nested dictionary *Args*: type_dict: dictionary of types that are generic *Returns*: flat_dict: flatten dictionary to a single level \"\"\" flat_dict = {} for k, v in type_dict.items(): if isinstance(v, dict): return_dict = flatten_type_dict(v) flat_dict.update(return_dict) else: flat_dict[k] = v return flat_dict get_attr_fields def get_attr_fields ( input_classes ) Gets the attribute fields from all classes Args : input_classes: current list of input classes Returns : dictionary of all attrs attribute fields ??? example \"View Source\" def get_attr_fields(input_classes): \"\"\"Gets the attribute fields from all classes *Args*: input_classes: current list of input classes *Returns*: dictionary of all attrs attribute fields \"\"\" return { attr.__name__: [val.name for val in attr.__attrs_attrs__] for attr in input_classes } get_type_fields def get_type_fields ( input_classes ) Creates a dictionary of names and types Args : input_classes: list of input classes Returns : type_fields: dictionary of names and types ??? example \"View Source\" def get_type_fields(input_classes): \"\"\"Creates a dictionary of names and types *Args*: input_classes: list of input classes *Returns*: type_fields: dictionary of names and types \"\"\" # Parse out the types if generic type_fields = {} for attr in input_classes: input_attr = {} for val in attr.__attrs_attrs__: if \"type\" in val.metadata: input_attr.update({val.name: val.metadata[\"type\"]}) else: input_attr.update({val.name: None}) type_fields.update({attr.__name__: input_attr}) return type_fields","title":"Utils"},{"location":"reference/spock/backend/utils/#module-spockbackendutils","text":"Attr utility functions for Spock None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Attr utility functions for Spock\"\"\" def get_attr_fields(input_classes): \"\"\"Gets the attribute fields from all classes *Args*: input_classes: current list of input classes *Returns*: dictionary of all attrs attribute fields \"\"\" return { attr.__name__: [val.name for val in attr.__attrs_attrs__] for attr in input_classes } def get_type_fields(input_classes): \"\"\"Creates a dictionary of names and types *Args*: input_classes: list of input classes *Returns*: type_fields: dictionary of names and types \"\"\" # Parse out the types if generic type_fields = {} for attr in input_classes: input_attr = {} for val in attr.__attrs_attrs__: if \"type\" in val.metadata: input_attr.update({val.name: val.metadata[\"type\"]}) else: input_attr.update({val.name: None}) type_fields.update({attr.__name__: input_attr}) return type_fields def flatten_type_dict(type_dict): \"\"\"Flattens a nested dictionary *Args*: type_dict: dictionary of types that are generic *Returns*: flat_dict: flatten dictionary to a single level \"\"\" flat_dict = {} for k, v in type_dict.items(): if isinstance(v, dict): return_dict = flatten_type_dict(v) flat_dict.update(return_dict) else: flat_dict[k] = v return flat_dict def convert_to_tuples(input_dict, named_type_dict, class_names): \"\"\"Convert lists to tuples Payloads from markup come in as Lists and not Tuples. This function turns lists in to tuples for the payloads so the attr values are set correctly. Will call itself recursively if a dictionary is present for class specific values *Args*: input_dict: input dictionary named_type_dict: dictionary of names with generic types *Returns*: updated_dict: a dictionary with lists converted to tuples \"\"\" updated_dict = {} all_typed_dict = flatten_type_dict(named_type_dict) for k, v in input_dict.items(): if k != \"config\": if isinstance(v, dict): updated = convert_to_tuples(v, named_type_dict.get(k), class_names) if updated: updated_dict.update({k: updated}) elif isinstance(v, list) and k in class_names: for val in v: updated = convert_to_tuples( val, named_type_dict.get(k), class_names ) if updated: updated_dict.update({k: updated}) elif all_typed_dict[k] is not None: updated = _recursive_list_to_tuple(v, all_typed_dict[k], class_names) updated_dict.update({k: updated}) return updated_dict def deep_update(source, updates): \"\"\"Deeply updates a dictionary Iterates through a dictionary recursively to update individual values within a possibly nested dictionary of dictionaries *Args*: source: source dictionary updates: updates to the dictionary *Returns*: source: updated version of the source dictionary \"\"\" for k, v in updates.items(): if isinstance(v, dict) and v: updated_dict = deep_update(source.get(k), v) if updated_dict: source[k] = updated_dict else: source[k] = v return source def _recursive_list_to_tuple(value, typed, class_names): \"\"\"Recursively turn lists into tuples Recursively looks through a pair of value and type and sets any of the possibly nested type of value to tuple if tuple is the specified type *Args*: value: value to check and set typ if necessary typed: type of the generic alias to check against *Returns*: value: updated value with correct type casts \"\"\" # Check for __args__ as it signifies a generic and make sure it's not already been cast as a tuple # from a composed payload if ( hasattr(typed, \"__args__\") and not isinstance(value, tuple) and not (isinstance(value, str) and value in class_names) ): # Force those with origin tuple types to be of the defined length if (typed.__origin__.__name__.lower() == \"tuple\") and len(value) != len( typed.__args__ ): raise ValueError( f\"Tuple(s) use a fixed/defined length -- Length of the provided argument ({len(value)}) \" f\"does not match the length of the defined argument ({len(typed.__args__)})\" ) # need to recurse before casting as we can't set values in a tuple with idx # Since it's generic it should be iterable to recurse and check it's children for idx, val in enumerate(value): value[idx] = _recursive_list_to_tuple(val, typed.__args__[0], class_names) # First check if list and then swap to tuple if the origin is tuple if isinstance(value, list) and typed.__origin__.__name__.lower() == \"tuple\": value = tuple(value) else: return value return value","title":"Module spock.backend.utils"},{"location":"reference/spock/backend/utils/#functions","text":"","title":"Functions"},{"location":"reference/spock/backend/utils/#convert_to_tuples","text":"def convert_to_tuples ( input_dict , named_type_dict , class_names ) Convert lists to tuples Payloads from markup come in as Lists and not Tuples. This function turns lists in to tuples for the payloads so the attr values are set correctly. Will call itself recursively if a dictionary is present for class specific values Args : input_dict: input dictionary named_type_dict: dictionary of names with generic types Returns : updated_dict: a dictionary with lists converted to tuples ??? example \"View Source\" def convert_to_tuples(input_dict, named_type_dict, class_names): \"\"\"Convert lists to tuples Payloads from markup come in as Lists and not Tuples. This function turns lists in to tuples for the payloads so the attr values are set correctly. Will call itself recursively if a dictionary is present for class specific values *Args*: input_dict: input dictionary named_type_dict: dictionary of names with generic types *Returns*: updated_dict: a dictionary with lists converted to tuples \"\"\" updated_dict = {} all_typed_dict = flatten_type_dict(named_type_dict) for k, v in input_dict.items(): if k != \"config\": if isinstance(v, dict): updated = convert_to_tuples(v, named_type_dict.get(k), class_names) if updated: updated_dict.update({k: updated}) elif isinstance(v, list) and k in class_names: for val in v: updated = convert_to_tuples( val, named_type_dict.get(k), class_names ) if updated: updated_dict.update({k: updated}) elif all_typed_dict[k] is not None: updated = _recursive_list_to_tuple(v, all_typed_dict[k], class_names) updated_dict.update({k: updated}) return updated_dict","title":"convert_to_tuples"},{"location":"reference/spock/backend/utils/#deep_update","text":"def deep_update ( source , updates ) Deeply updates a dictionary Iterates through a dictionary recursively to update individual values within a possibly nested dictionary of dictionaries Args : source: source dictionary updates: updates to the dictionary Returns : source: updated version of the source dictionary ??? example \"View Source\" def deep_update(source, updates): \"\"\"Deeply updates a dictionary Iterates through a dictionary recursively to update individual values within a possibly nested dictionary of dictionaries *Args*: source: source dictionary updates: updates to the dictionary *Returns*: source: updated version of the source dictionary \"\"\" for k, v in updates.items(): if isinstance(v, dict) and v: updated_dict = deep_update(source.get(k), v) if updated_dict: source[k] = updated_dict else: source[k] = v return source","title":"deep_update"},{"location":"reference/spock/backend/utils/#flatten_type_dict","text":"def flatten_type_dict ( type_dict ) Flattens a nested dictionary Args : type_dict: dictionary of types that are generic Returns : flat_dict: flatten dictionary to a single level ??? example \"View Source\" def flatten_type_dict(type_dict): \"\"\"Flattens a nested dictionary *Args*: type_dict: dictionary of types that are generic *Returns*: flat_dict: flatten dictionary to a single level \"\"\" flat_dict = {} for k, v in type_dict.items(): if isinstance(v, dict): return_dict = flatten_type_dict(v) flat_dict.update(return_dict) else: flat_dict[k] = v return flat_dict","title":"flatten_type_dict"},{"location":"reference/spock/backend/utils/#get_attr_fields","text":"def get_attr_fields ( input_classes ) Gets the attribute fields from all classes Args : input_classes: current list of input classes Returns : dictionary of all attrs attribute fields ??? example \"View Source\" def get_attr_fields(input_classes): \"\"\"Gets the attribute fields from all classes *Args*: input_classes: current list of input classes *Returns*: dictionary of all attrs attribute fields \"\"\" return { attr.__name__: [val.name for val in attr.__attrs_attrs__] for attr in input_classes }","title":"get_attr_fields"},{"location":"reference/spock/backend/utils/#get_type_fields","text":"def get_type_fields ( input_classes ) Creates a dictionary of names and types Args : input_classes: list of input classes Returns : type_fields: dictionary of names and types ??? example \"View Source\" def get_type_fields(input_classes): \"\"\"Creates a dictionary of names and types *Args*: input_classes: list of input classes *Returns*: type_fields: dictionary of names and types \"\"\" # Parse out the types if generic type_fields = {} for attr in input_classes: input_attr = {} for val in attr.__attrs_attrs__: if \"type\" in val.metadata: input_attr.update({val.name: val.metadata[\"type\"]}) else: input_attr.update({val.name: None}) type_fields.update({attr.__name__: input_attr}) return type_fields","title":"get_type_fields"},{"location":"reference/spock/backend/wrappers/","text":"Module spock.backend.wrappers Handles Spock data type wrappers None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles Spock data type wrappers\"\"\" import argparse import yaml class Spockspace(argparse.Namespace): \"\"\"Inherits from Namespace to implement a pretty print on the obj Overwrites the __repr__ method with a pretty version of printing \"\"\" def __init__(self, **kwargs): super(Spockspace, self).__init__(**kwargs) def __repr__(self): # Remove aliases in YAML print yaml.Dumper.ignore_aliases = lambda *args: True return yaml.dump(self.__dict__, default_flow_style=False) Classes Spockspace class Spockspace ( ** kwargs ) ??? example \"View Source\" class Spockspace(argparse.Namespace): \"\"\"Inherits from Namespace to implement a pretty print on the obj Overwrites the __repr__ method with a pretty version of printing \"\"\" def __init__(self, **kwargs): super(Spockspace, self).__init__(**kwargs) def __repr__(self): # Remove aliases in YAML print yaml.Dumper.ignore_aliases = lambda *args: True return yaml.dump(self.__dict__, default_flow_style=False) Ancestors (in MRO) argparse.Namespace argparse._AttributeHolder","title":"Wrappers"},{"location":"reference/spock/backend/wrappers/#module-spockbackendwrappers","text":"Handles Spock data type wrappers None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles Spock data type wrappers\"\"\" import argparse import yaml class Spockspace(argparse.Namespace): \"\"\"Inherits from Namespace to implement a pretty print on the obj Overwrites the __repr__ method with a pretty version of printing \"\"\" def __init__(self, **kwargs): super(Spockspace, self).__init__(**kwargs) def __repr__(self): # Remove aliases in YAML print yaml.Dumper.ignore_aliases = lambda *args: True return yaml.dump(self.__dict__, default_flow_style=False)","title":"Module spock.backend.wrappers"},{"location":"reference/spock/backend/wrappers/#classes","text":"","title":"Classes"},{"location":"reference/spock/backend/wrappers/#spockspace","text":"class Spockspace ( ** kwargs ) ??? example \"View Source\" class Spockspace(argparse.Namespace): \"\"\"Inherits from Namespace to implement a pretty print on the obj Overwrites the __repr__ method with a pretty version of printing \"\"\" def __init__(self, **kwargs): super(Spockspace, self).__init__(**kwargs) def __repr__(self): # Remove aliases in YAML print yaml.Dumper.ignore_aliases = lambda *args: True return yaml.dump(self.__dict__, default_flow_style=False)","title":"Spockspace"},{"location":"reference/spock/backend/wrappers/#ancestors-in-mro","text":"argparse.Namespace argparse._AttributeHolder","title":"Ancestors (in MRO)"}]}