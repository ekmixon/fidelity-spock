"use strict";(self.webpackChunkdocs_v_2=self.webpackChunkdocs_v_2||[]).push([[1206],{3905:function(e,n,t){t.d(n,{Zo:function(){return l},kt:function(){return m}});var r=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function p(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=r.createContext({}),c=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},l=function(e){var n=c(e.components);return r.createElement(s.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},u=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,l=p(e,["components","mdxType","originalType","parentName"]),u=c(t),m=a,f=u["".concat(s,".").concat(m)]||u[m]||d[m]||i;return t?r.createElement(f,o(o({ref:n},l),{},{components:t})):r.createElement(f,o({ref:n},l))}));function m(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,o=new Array(i);o[0]=u;var p={};for(var s in n)hasOwnProperty.call(n,s)&&(p[s]=n[s]);p.originalType=e,p.mdxType="string"==typeof e?e:a,o[1]=p;for(var c=2;c<i;c++)o[c]=t[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}u.displayName="MDXCreateElement"},9248:function(e,n,t){t.r(n),t.d(n,{frontMatter:function(){return p},contentTitle:function(){return s},metadata:function(){return c},toc:function(){return l},default:function(){return u}});var r=t(7462),a=t(3366),i=(t(7294),t(3905)),o=["components"],p={},s="Parameter Groups",c={unversionedId:"advanced_features/Parameter-Groups",id:"advanced_features/Parameter-Groups",isDocsHomePage:!1,title:"Parameter Groups",description:"Since spock manages complex configurations via a class based solution we can define and decorate multiple classes",source:"@site/docs/advanced_features/Parameter-Groups.md",sourceDirName:"advanced_features",slug:"/advanced_features/Parameter-Groups",permalink:"/spock/advanced_features/Parameter-Groups",editUrl:"https://github.com/fidelity/spock/edit/master/website/docs/advanced_features/Parameter-Groups.md",tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Optional Parameters",permalink:"/spock/advanced_features/Optional-Parameters"},next:{title:"Inheritance",permalink:"/spock/advanced_features/Inheritance"}},l=[{value:"Building spock Parameter Groups",id:"building-spock-parameter-groups",children:[]},{value:"Adding More Code",id:"adding-more-code",children:[]}],d={toc:l};function u(e){var n=e.components,t=(0,a.Z)(e,o);return(0,i.kt)("wrapper",(0,r.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"parameter-groups"},"Parameter Groups"),(0,i.kt)("p",null,"Since ",(0,i.kt)("inlineCode",{parentName:"p"},"spock")," manages complex configurations via a class based solution we can define and decorate multiple classes\nwith ",(0,i.kt)("inlineCode",{parentName:"p"},"@spock"),". Each class gets created as a separate class object within the ",(0,i.kt)("inlineCode",{parentName:"p"},"spock")," namespace object."),(0,i.kt)("h3",{id:"building-spock-parameter-groups"},"Building spock Parameter Groups"),(0,i.kt)("p",null,"Let's go back to our example. Say we need to add a few more parameters to our code. We could just keep adding them to\nthe single defined class, but this would lead to a ",(0,i.kt)("em",{parentName:"p"},"'mega'")," class that has parameters for many different parts of your\ncode. Instead, we will define two new ",(0,i.kt)("inlineCode",{parentName:"p"},"spock")," classes for our new parameters and begin to organize them by\nfunctionality."),(0,i.kt)("p",null,"Editing our definition in: ",(0,i.kt)("inlineCode",{parentName:"p"},"tutorial.py")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from enum import Enum\nfrom spock import spock\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nclass Activation(Enum):\n    relu = 'relu'\n    gelu = 'gelu'\n    tanh = 'tanh'\n\n\nclass Optimizer(Enum):\n    sgd = 'SGD'\n    adam = 'Adam'\n\n\n@spock\nclass ModelConfig:\n    n_features: int\n    dropout: Optional[List[float]]\n    hidden_sizes: Tuple[int, int, int] = (32, 32, 32)\n    activation: Activation = 'relu'\n    optimizer: Optimizer\n\n\n@spock\nclass DataConfig:\n    batch_size: int = 2\n    n_samples: int = 8\n\n\n@spock\nclass OptimizerConfig:\n    lr: float = 0.01\n    n_epochs: int = 2\n    grad_clip: Optional[float]\n\n")),(0,i.kt)("p",null,"Now we have three separate ",(0,i.kt)("inlineCode",{parentName:"p"},"spock")," classes that we need to generate the namespace object from. Simply add the new\nclasses to ",(0,i.kt)("inlineCode",{parentName:"p"},"*args")," in the ",(0,i.kt)("inlineCode",{parentName:"p"},"SpockBuilder"),". Editing ",(0,i.kt)("inlineCode",{parentName:"p"},"tutorial.py"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from spock.builder import SpockBuilder\n\ndef main():\n    # A simple description\n    description = 'spock Advanced Tutorial'\n    # Build out the parser by passing in Spock config objects as *args after description\n    config = SpockBuilder(ModelConfig, DataConfig, OptimizerConfig, desc=description).generate()\n    # One can now access the Spock config object by class name with the returned namespace\n    # For instance...\n    print(config.ModelConfig)\n    print(config.OptimizerConfig)\n")),(0,i.kt)("p",null,"Editing our configuration file: ",(0,i.kt)("inlineCode",{parentName:"p"},"tutorial.yaml")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"################\n# tutorial.yaml\n################\n# Special Key\nsave_path: /tmp\n# ModelConfig\nn_features: 64\ndropout: [0.2, 0.1]\nhidden_sizes: [32, 32, 16]\nactivation: relu\noptimizer: SGD\n# DataConfig\nbatch_size: 2\nn_samples: 8\n# OptimizerConfig\nlr: 0.01\nn_epochs: 2\ngrad_clip: 5.0\n")),(0,i.kt)("h3",{id:"adding-more-code"},"Adding More Code"),(0,i.kt)("p",null,"Let's add a bit more functionality to our code that uses our new parameters by running a 'basic training loop' (this is\nkept very simple for illustrative purposes, hence the simple data slicing) on our basic neural network: ",(0,i.kt)("inlineCode",{parentName:"p"},"tutorial.py")," "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import torch\nfrom .basic_nn import BasicNet\n\n\ndef train(x_data, y_data, model, model_config, data_config, optimizer_config):\n    if model_config.optimizer == 'SGD':\n        optimizer = torch.optim.SGD(model.parameters(), lr=optimizer_config.lr)\n    elif model_config.optimizer == 'Adam':\n        optimizer = torch.optim.Adam(model.parameters(), lr=optimizer_config.lr)\n    else:\n        raise ValueError(f'Optimizer choice {optimizer_config.optimizer} not available')\n    n_steps_per_epoch = data_config.n_samples % data_config.batch_size\n    for epoch in range(optimizer_config.n_epochs):\n        for i in range(n_steps_per_epoch):\n            # Ugly data slicing for simplicity\n            x_batch = x_data[i*n_steps_per_epoch:(i+1)*n_steps_per_epoch,]\n            y_batch = y_data[i*n_steps_per_epoch:(i+1)*n_steps_per_epoch,]\n            optimizer.zero_grad()\n            output = model(x_batch)\n            loss = torch.nn.CrossEntropyLoss(output, y_batch)\n            loss.backward()\n            if optimizer_config.grad_clip:\n                torch.nn.utils.clip_grad_value(model.parameters(), optimizer_config.grad_clip)\n            optimizer.step()\n                \n\ndef main():\n    # A simple description\n    description = 'spock Advanced Tutorial'\n    # Build out the parser by passing in Spock config objects as *args after description\n    config = SpockBuilder(ModelConfig, DataConfig, OptimizerConfig, desc=description).generate()\n    # Instantiate our neural net using\n    basic_nn = BasicNet(model_config=config.ModelConfig)\n    # Make some random data (BxH): H has dim of features in\n    x_data = torch.rand(config.DataConfig.n_samples, config.ModelConfig.n_features)\n    y_data = torch.randint(0, 3, (config.DataConig.n_samples,))\n    # Run some training\n    train(x_data, y_data, basic_nn, config.ModelConfig, config.DataConfig, config.OptimizerConfig) \n")))}u.isMDXComponent=!0}}]);