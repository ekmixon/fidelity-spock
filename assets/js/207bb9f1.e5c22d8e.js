"use strict";(self.webpackChunkdocs_v_2=self.webpackChunkdocs_v_2||[]).push([[7521],{3905:function(e,n,t){t.d(n,{Zo:function(){return l},kt:function(){return m}});var i=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,i,r=function(e,n){if(null==e)return{};var t,i,r={},a=Object.keys(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var c=i.createContext({}),p=function(e){var n=i.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},l=function(e){var n=p(e.components);return i.createElement(c.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},f=i.forwardRef((function(e,n){var t=e.components,r=e.mdxType,a=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),f=p(t),m=r,u=f["".concat(c,".").concat(m)]||f[m]||d[m]||a;return t?i.createElement(u,o(o({ref:n},l),{},{components:t})):i.createElement(u,o({ref:n},l))}));function m(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var a=t.length,o=new Array(a);o[0]=f;var s={};for(var c in n)hasOwnProperty.call(n,c)&&(s[c]=n[c]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var p=2;p<a;p++)o[p]=t[p];return i.createElement.apply(null,o)}return i.createElement.apply(null,t)}f.displayName="MDXCreateElement"},7360:function(e,n,t){t.r(n),t.d(n,{frontMatter:function(){return s},contentTitle:function(){return c},metadata:function(){return p},toc:function(){return l},default:function(){return f}});var i=t(7462),r=t(3366),a=(t(7294),t(3905)),o=["components"],s={},c="Inheritance",p={unversionedId:"advanced_features/Inheritance",id:"advanced_features/Inheritance",isDocsHomePage:!1,title:"Inheritance",description:"spock supports class inheritance between different defined spock classes. This allows you to build complex",source:"@site/docs/advanced_features/Inheritance.md",sourceDirName:"advanced_features",slug:"/advanced_features/Inheritance",permalink:"/spock/advanced_features/Inheritance",editUrl:"https://github.com/fidelity/spock/edit/master/website/docs/advanced_features/Inheritance.md",tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Parameter Groups",permalink:"/spock/advanced_features/Parameter-Groups"},next:{title:"Advanced Types",permalink:"/spock/advanced_features/Advanced-Types"}},l=[{value:"Defining an Inherited spock Class",id:"defining-an-inherited-spock-class",children:[]},{value:"Using an Inherited spock Class",id:"using-an-inherited-spock-class",children:[]}],d={toc:l};function f(e){var n=e.components,t=(0,r.Z)(e,o);return(0,a.kt)("wrapper",(0,i.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"inheritance"},"Inheritance"),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"spock")," supports class inheritance between different defined ",(0,a.kt)("inlineCode",{parentName:"p"},"spock")," classes. This allows you to build complex\nconfigurations derived from a common base class or classes."),(0,a.kt)("h3",{id:"defining-an-inherited-spock-class"},"Defining an Inherited spock Class"),(0,a.kt)("p",null,"Back to our example. We have implemented two different optimizers to train our neural network. In its current state\nwe have overlooked the fact that the two different optimizers share a set of common parameters but each also has a\nset of specific parameters. Instead of defining redundant parameter definitions let's use ",(0,a.kt)("inlineCode",{parentName:"p"},"spock")," inheritance."),(0,a.kt)("p",null,"We create a new ",(0,a.kt)("inlineCode",{parentName:"p"},"spock")," class that inherits from another ",(0,a.kt)("inlineCode",{parentName:"p"},"spock")," class. This functions just like traditional inheritance\nwhere the child will inherit the parameter definitions from the parent class."),(0,a.kt)("p",null,"Editing our definition in: ",(0,a.kt)("inlineCode",{parentName:"p"},"tutorial.py")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from enum import Enum\nfrom spock.args import SavePath\nfrom spock.config import spock\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nclass Activation(Enum):\n    relu = 'relu'\n    gelu = 'gelu'\n    tanh = 'tanh'\n\n\nclass Optimizer(Enum):\n    sgd = 'SGD'\n    adam = 'Adam'\n\n\n@spock\nclass ModelConfig:\n    save_path: SavePath\n    n_features: int\n    dropout: Optional[List[float]]\n    hidden_sizes: Tuple[int, int, int] = (32, 32, 32)\n    activation: Activation = 'relu'\n    optimizer: Optimizer\n\n\n@spock\nclass DataConfig:\n    batch_size: int = 2\n    n_samples: int = 8\n\n\n@spock\nclass OptimizerConfig:\n    lr: float = 0.01\n    n_epochs: int = 2\n    grad_clip: Optional[float]\n\n\n@spock\nclass SGDConfig(OptimizerConfig):\n    weight_decay: float\n    momentum: float\n    nesterov: bool\n\n")),(0,a.kt)("p",null,"Editing our configuration file: ",(0,a.kt)("inlineCode",{parentName:"p"},"tutorial.yaml")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"################\n# tutorial.yaml\n################\n# Special Key\nsave_path: /tmp\n# ModelConfig\nn_features: 64\ndropout: [0.2, 0.1]\nhidden_sizes: [32, 32, 16]\nactivation: relu\noptimizer: SGD\n# DataConfig\nbatch_size: 2\nn_samples: 8\n# OptimizerConfig\nlr: 0.01\nn_epochs: 2\ngrad_clip: 5.0\n# SGD Config\nweight_decay: 1E-5\nmomentum: 0.9\nnesterov: true\n")),(0,a.kt)("h3",{id:"using-an-inherited-spock-class"},"Using an Inherited spock Class"),(0,a.kt)("p",null,"Let's use our inherited class to use the SGD optimizer with the defined parameter on our basic neural network:\n",(0,a.kt)("inlineCode",{parentName:"p"},"tutorial.py")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import torch\nfrom .basic_nn import BasicNet\n\n\ndef train(x_data, y_data, model, model_config, data_config, optimizer_config):\n    if model_config.optimizer == 'SGD':\n        optimizer = torch.optim.SGD(model.parameters(), lr=optimizer_config.lr, momentum=optimizer_config.momentum,\n                                    nesterov=optimizer_config.nesterov)\n    elif model_config.optimizer == 'Adam':\n        optimizer = torch.optim.Adam(model.parameters(), lr=optimizer_config.lr)\n    else:\n        raise ValueError(f'Optimizer choice {optimizer_config.optimizer} not available')\n    n_steps_per_epoch = data_config.n_samples % data_config.batch_size\n    for epoch in range(optimizer_config.n_epochs):\n        for i in range(n_steps_per_epoch):\n            # Ugly data slicing for simplicity\n            x_batch = x_data[i * n_steps_per_epoch:(i + 1) * n_steps_per_epoch, ]\n            y_batch = y_data[i * n_steps_per_epoch:(i + 1) * n_steps_per_epoch, ]\n            optimizer.zero_grad()\n            output = model(x_batch)\n            loss = torch.nn.CrossEntropyLoss(output, y_batch)\n            loss.backward()\n            if optimizer_config.grad_clip:\n                torch.nn.utils.clip_grad_value(model.parameters(), optimizer_config.grad_clip)\n            optimizer.step()\n        print(f'Finished Epoch {epoch+1}')\n\n\ndef main():\n    # A simple description\n    description = 'spock Advanced Tutorial'\n    # Build out the parser by passing in Spock config objects as *args after description\n    config = ConfigArgBuilder(ModelConfig, DataConfig, SGDConfig, desc=description).generate()\n    # Instantiate our neural net using\n    basic_nn = BasicNet(model_config=config.ModelConfig)\n    # Make some random data (BxH): H has dim of features in\n    x_data = torch.rand(config.DataConfig.n_samples, config.ModelConfig.n_features)\n    y_data = torch.randint(0, 3, (config.DataConfig.n_samples,))\n    # Run some training\n    train(x_data, y_data, basic_nn, config.ModelConfig, config.DataConfig, config.SGDConfig)\n")))}f.isMDXComponent=!0}}]);